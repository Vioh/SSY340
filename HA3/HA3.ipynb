{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checklist for submission\n",
    "\n",
    "It is extremely important to make sure that:\n",
    "\n",
    "1. Everything runs as expected (no bugs when running cells);\n",
    "2. The output from each cell corresponds to its code (don't change any cell's contents without rerunning it afterwards);\n",
    "3. All outputs are present (don't delete any of the outputs);\n",
    "4. Fill in all the places that say `# YOUR CODE HERE`, or \"**Your answer:** (fill in here)\".\n",
    "5. You should not need to create any new cells in the notebook, but feel free to do it if convenient for you.\n",
    "6. The notebook contains some hidden metadata which is important during our grading process. **Make sure not to corrupt any of this metadata!** The metadata may be corrupted if you perform an unsuccessful git merge / git pull. It may also be pruned completely if using Google Colab, so watch out for this. Searching for \"nbgrader\" when opening the notebook in a text editor should take you to the important metadata entries.\n",
    "7. Fill in your group number and the full names of the members in the cell below;\n",
    "8. Make sure that you are not running an old version of IPython (we provide you with a cell that checks this, make sure you can run it without errors).\n",
    "\n",
    "Failing to meet any of these requirements might lead to either a subtraction of POEs (at best) or a request for resubmission (at worst).\n",
    "\n",
    "We advise you the following steps before submission for ensuring that requirements 1, 2, and 3 are always met: **Restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All). This might require a bit of time, so plan ahead for this (and possibly use Google Cloud's GPU in HA1 and HA2 for this step). Finally press the \"Save and Checkout\" button before handing in, to make sure that all your changes are saved to this .ipynb file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Group number and member names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROUP = \"43\"\n",
    "NAME1 = \"Hai Dinh\"\n",
    "NAME2 = \"Edvin Agnas\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you can run the following cell without errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "assert IPython.version_info[0] >= 3, \"Your version of IPython is too old, please update it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9f4edef3c4e6df4dbb2a29dba78c09d8",
     "grade": false,
     "grade_id": "cell-2f332c3ca731afc6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Home Assignment 3\n",
    "This home assignment will focus on reinforcement learning and deep reinforcement learning. The first part will cover value-table reinforcement learning techniques, and the second part will include neural networks as function approximators, i.e. deep reinforcement learning. \n",
    "\n",
    "When handing in this assignment, make sure that you're handing in the correct version, and more importantly, *that you do no clear any output from your cells*. We'll use these outputs to aid us when grading your assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "cc4d88f2e070d5479382bf223b5c5d49",
     "grade": false,
     "grade_id": "cell-8122dcb8d8ca1c9e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Task 1: Gridworld\n",
    "\n",
    "In this task, you will implement Value Iteration to solve for the optimal policy, $\\pi^*$, and the corresponding state value function, $V^*$.\n",
    "\n",
    "The MDP you will work with in this assignment is illustrated in the figure below\n",
    "\n",
    "![title](./grid_world.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fbdc1c764382473316a629ae7682d1bb",
     "grade": false,
     "grade_id": "cell-b4e5d5337fbaa0e5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The agent starts in one of the squares shown in the above figure, and then proceeds to take actions. The available actions at any time step are: **North, West, South,** and **East**. If an action would make the agent bump into a wall, or one of the black (unreachable) states, it instead does nothing, leaving the agent at the same place it was before.\n",
    "\n",
    "The reward $\\mathcal{R}_s^a$ of being in state $s$ and performing actions $a$ is zero for all states, regardless of the action taken, with the exception of the green and the red squares. For the green square, the reward is always 1, and for the red square, always -1, regardless of the action.\n",
    "\n",
    "When the agent is either in the green or the red square, it will be transported to the terminal state in the next time step, regardless of the action taken. The terminal state is shown as the white square with the \"T\" inside.\n",
    "\n",
    "#### State representation\n",
    "The notations used to define the states are illustrated in the table below\n",
    "\n",
    "| $S_0$ | $S_1$ | $S_2$ | $S_3$ | $S_4$ |    |\n",
    "|-------|-------|-------|-------|-------|----|\n",
    "| $S_5$ | $S_6$ | $S_7$ | $S_8$ | $S_9$ |    |\n",
    "| $S_{10}$ | $S_{11}$ | $S_{12}$ | $S_{13}$ | $S_{14}$ | $S_{15}$|\n",
    "\n",
    "where $S_{10}$ corresponds to the initial state of the environment, $S_4$ and $S_9$ to the green and red states of the environment, and $S_{15}$ to the terminal state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8d0703fff00f4fb67a72b46968fe7253",
     "grade": false,
     "grade_id": "cell-c54a0f7162b1f260",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "\n",
    "### Task 1.a: Solve for $V^*(s)$ and $Q^*(s,a)$\n",
    "For this task all transition probabilities are assumed to be 1 (that is, trying to move in a certain direction will definitely move the agent in the chosen direction), and a discount factor of .9, i.e. $\\gamma=.9$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9894346da453882ad420d049511a5b8b",
     "grade": false,
     "grade_id": "cell-c7fa1d00113f314e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "* Solve for $V^*(S_{10})$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e998cc86ed986eeac11f54b0f6869a67",
     "grade": true,
     "grade_id": "cell-966bc6b1276b31f1",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** \n",
    "$$V^*(S_{10}) = 0.9^6 \\approx 0.531$$\n",
    "This can easily be shown mathematically with Bellman optimality equation that was given in the video lectures. The full proof requires us to walk through the equation step by step recursively. However, to give a short explanation, we notice that Bellman optimality equation basically tells us that the optimal value of a state is equal to the reward after taking the best action plus with the discounted value of the next state. Since there are 6 steps in the optimal path (either by 10-5-0-1-2-3-4 or by 10-11-12-13-8-3-4), and since we have a discount factor of 0.9 at each step, so that result should be $0.9^6$ as stated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c2d30fc8031fa1ff3f06019a9cf1ba27",
     "grade": false,
     "grade_id": "cell-4cc15316add9bd67",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "* Solve $Q^*(S_{10},a)$ for all actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ad5e1409a936a5c1ec6684a1edae79ee",
     "grade": true,
     "grade_id": "cell-0e5efad7ed72fdcb",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** \n",
    "Using the Bellman optimality equation, we know that $Q^*(S_{10}, a)$ is equal to the reward after taking the action $a$ plus with the discounted value of the next state. Since the rewards surrounding $S_{10}$ are always zero, we can ignore them. The discounted value of the next state can be worked out in the exact same way as in the previous question. So the answer for this question is:\n",
    "$$Q^*(S_{10}, N) = \\gamma V^*(S_5) = 0.9 \\times 0.9^5 = 0.9^6 \\approx 0.531$$\n",
    "$$Q^*(S_{10}, E) = \\gamma V^*(S_{11}) = 0.9 \\times 0.9^5 = 0.9^6 \\approx 0.531$$\n",
    "$$Q^*(S_{10}, S) = \\gamma V^*(S_{10}) = 0.9 \\times 0.9^6 = 0.9^7 \\approx 0.478$$\n",
    "$$Q^*(S_{10}, W) = \\gamma V^*(S_{10}) = 0.9 \\times 0.9^6 = 0.9^7 \\approx 0.478$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fcab701a30ff0fd17edaa87e562124ba",
     "grade": false,
     "grade_id": "cell-e426e3815f78930a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "\n",
    "\n",
    "### Task 1.b Write a mathematical expression relating $V^\\pi(s)$ to $Q^\\pi(s,a)$ and $\\pi(a|s)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "61bf64e2c38d41b0f0d024faba554e8f",
     "grade": true,
     "grade_id": "cell-343c3ea4883085e1",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** This is basically the general Bellman expectation equation introduced in the video lectures.\n",
    "$$V^\\pi (s) = \\sum_{a}{\\pi(a|s) \\, Q^\\pi(s,a)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "993f36fffd9ebfd3c89a3b63dd42685d",
     "grade": false,
     "grade_id": "cell-ab80df325256cf89",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "\n",
    "###  Task 1.c: Value Iteration\n",
    "For this task, the transitions are no longer deterministic. Instead, there is a 0.2 probability that the agent will try to travel in an orthogonal direction of the chosen action (0.1 probability for each of the two orthogonal directions). Note that the Markov decision process is still known and does not have to be learned from experience.\n",
    "\n",
    "Your task is to implement value iteration and solve for the\n",
    "* optimal greedy policy $\\pi^*(s)$ \n",
    "* $V^*(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "659925b7584671453aaee51f14ec4083",
     "grade": false,
     "grade_id": "cell-74497ad9b13e8362",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### The value iteration algorithm\n",
    "Value iteration is an iterative algorithm used to compute the optimal value function $V^*(s)$. Each iteration starts with a guess of what the value function is and then uses the Bellman equations to improve this guess iteratively. We can describe one iteration of the algorithm as\n",
    "\n",
    "$\n",
    "\\textbf{For} ~ s \\in {\\cal S}:\\qquad  \\\\\n",
    "\\quad V_{k+1}(s) = \\underset{a \\in {\\cal A}}{\\text{max}}~ \\left( \\mathcal{R}_s^a + \\gamma \\underset{{s'\\in \\mathcal{S}}}{\\sum} \\mathcal{P}_{ss'}^a \\cdot V_k(s') \\right)\n",
    "$\n",
    "\n",
    "where $\\mathcal{P}_{ss'}^a={\\mathrm Pr}[S'=s'\\big|S=s,A=a]$ is the probability to transition state $s$ to $s'$ given action $a$.\n",
    "\n",
    "\n",
    "#### The MDP Python class\n",
    "The Markov Decision Process you will work with is defined in `gridworld_mdp.py`. In the implementation, the actions are represented by integers as, North = 0, West = 1, South = 2, and East = 3.\n",
    "To interact with the MDP, you need to instantiate an object as: \n",
    "\n",
    "\n",
    "```python\n",
    "mdp = GridWorldMDP()\n",
    "```\n",
    "\n",
    "At your disposal there are a number of instance-functions implemented for you, and presented below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "636b8c5ea0bb7d3b4798564dfc7d580d",
     "grade": false,
     "grade_id": "cell-21e5d7b3d3083cd6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function get_states in module gridworld_mdp:\n",
      "\n",
      "get_states(self)\n",
      "    Returns complete set of states for the MDP\n",
      "    :return: numpy array of shape [num states,]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from gridworld_mdp import *\n",
    "import numpy as np\n",
    "\n",
    "help(GridWorldMDP.get_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "620f4674b0caadf475b4889bd8747b62",
     "grade": false,
     "grade_id": "cell-9706322eb34e16db",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function __init__ in module gridworld_mdp:\n",
      "\n",
      "__init__(self, trans_prob=0.8)\n",
      "    Initializes an instance of the GridWorldMDP class\n",
      "    :param trans_prob: transition probabilities (e.g. =1 for deterministic MDP)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The constructor\n",
    "help(GridWorldMDP.__init__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f9b14ba756ee79811ccba63e1e51f14a",
     "grade": false,
     "grade_id": "cell-38d3ab6fb24c1af8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function get_actions in module gridworld_mdp:\n",
      "\n",
      "get_actions(self)\n",
      "    Returns complete set of actions for the MDP\n",
      "    :return: numpy array of shape [num actions,]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(GridWorldMDP.get_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e0b98cbe90baa640578ed1991b4e3501",
     "grade": false,
     "grade_id": "cell-ecb00397472a5faa",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function state_transition_func in module gridworld_mdp:\n",
      "\n",
      "state_transition_func(self, s, a)\n",
      "    Returns the transition probabilities to all states given current state and action\n",
      "    :param state: current state as integer\n",
      "    :param action: selected action as integer\n",
      "    :return: state-transition probabilities, i.e.\n",
      "     [P[S_0| S=s, A_t=a], P[S_1| S=s, A=a], ..., P[S_14| S=s, A=a]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(GridWorldMDP.state_transition_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "40120f95b5767060a8ca55455cf5d485",
     "grade": false,
     "grade_id": "cell-aa8e1498649053a5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function reward_function in module gridworld_mdp:\n",
      "\n",
      "reward_function(self, s, a)\n",
      "    Returns the reward r(s,a)\n",
      "    :param state: current state as integer\n",
      "    :param action: selected action as integer\n",
      "    :return: r(s,a)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(GridWorldMDP.reward_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8defb43e783bcb9464571753a5c6e452",
     "grade": false,
     "grade_id": "cell-c1408cc9707dd7f8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "We also provide two helper functions for visualizing the value function and the policies you obtain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b39ad0b689edcb447e156b888fbc0515",
     "grade": false,
     "grade_id": "cell-b754590784e24eb1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Function for printing a policy pi\n",
    "def print_policy(pi):\n",
    "    print('Policy for non-terminal states: ')\n",
    "    indencies = np.arange(1, 16)\n",
    "    txt = '| '\n",
    "    hor_delimiter = '---------------------'\n",
    "    print(hor_delimiter)\n",
    "    for a, i in zip(pi, indencies):\n",
    "        txt += mdp.act_to_char_dict[a] + ' | '\n",
    "        if i % 5 == 0:\n",
    "            print(txt + '\\n' + hor_delimiter)\n",
    "            txt = '| '\n",
    "    print('                            ---')\n",
    "    print('Policy for terminal state: |', mdp.act_to_char_dict[pi[15]],'|')\n",
    "    print('                            ---')            \n",
    "\n",
    "# Function for printing a table with of the value function\n",
    "def print_value_table(values, num_iterations=None):            \n",
    "    if num_iterations:\n",
    "        print('Values for non-terminal states after: ', num_iterations, 'iterations \\n', np.reshape(values, [3, 5]), '\\n')\n",
    "        print('Value for terminal state:', terminal_value, '\\n')\n",
    "    else: \n",
    "        terminal_value = values[-1]\n",
    "        print('Values for non-terminal states: \\n', np.reshape(values[:-1], [3, 5]))\n",
    "        print('Value for terminal state:', terminal_value, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c04d9592932bc9c1e92df4949bcea301",
     "grade": false,
     "grade_id": "cell-87e02763b23fe1f8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now it's time for you to implement your own version of value iteration to solve for the greedy policy and $V^*(s)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "4ec5804f5a67633ffcbb04d6e0f2addf",
     "grade": true,
     "grade_id": "cell-d473b99fe1825067",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def value_iteration(gamma, mdp):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        V - state value table, numpy array of shape (16,)\n",
    "        pi - greedy policy table, numpy array of shape (16,)\n",
    "    \"\"\"\n",
    "    V = np.zeros([16]) # state value table\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    Q = np.zeros([16, 4]) # state action value table\n",
    "    pi = np.zeros([16])   # greedy policy table\n",
    "    threshold = 0.00001\n",
    "    Q_prev = None\n",
    "    \n",
    "    while Q_prev is None or np.any(np.abs(Q-Q_prev) > threshold):\n",
    "        Q_prev = np.copy(Q)\n",
    "        for s in mdp.get_states():\n",
    "            for a in mdp.get_actions():\n",
    "                P = mdp.state_transition_func(s, a)\n",
    "                R = mdp.reward_function(s, a)\n",
    "                Q[s, a] = R + gamma * np.sum(P * V)\n",
    "            V[s] = np.max(Q[s])\n",
    "        \n",
    "    pi = np.argmax(Q, axis=1)\n",
    "    return V, pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "41ca129edbe5353b31ec35933bf28cdb",
     "grade": false,
     "grade_id": "cell-99c149095318adac",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Run your implementation for the deterministic version of our MDP. As a sanity check, compare your analytical solutions with the output from your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8a8722a6c33a96991f1091e2418def51",
     "grade": false,
     "grade_id": "cell-bd495acfe33d405f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values for non-terminal states: \n",
      " [[ 0.6561    0.729     0.81      0.9       1.      ]\n",
      " [ 0.59049   0.        0.        0.81     -1.      ]\n",
      " [ 0.531441  0.59049   0.6561    0.729     0.6561  ]]\n",
      "Value for terminal state: 0.0 \n",
      "\n",
      "Policy for non-terminal states: \n",
      "---------------------\n",
      "| E | E | E | E | N | \n",
      "---------------------\n",
      "| N | N | N | N | N | \n",
      "---------------------\n",
      "| N | E | E | N | W | \n",
      "---------------------\n",
      "                            ---\n",
      "Policy for terminal state: | N |\n",
      "                            ---\n"
     ]
    }
   ],
   "source": [
    "mdp = GridWorldMDP(trans_prob=1.)\n",
    "v, pi = value_iteration(.9, mdp)\n",
    "print_value_table(v)\n",
    "print_policy(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "172eb8e58e4dbdcca83bb2bb8589032d",
     "grade": false,
     "grade_id": "cell-5a24214a0645d4b4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Once your implementation passed the sanity check, run it for the stochastic case, where the probability of an action succeding is 0.8, and 0.2 of moving the agent in an orthogonal direction to the intended. Use $\\gamma = .99$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "60e210ba654df60fcc54a7d6eda59aae",
     "grade": false,
     "grade_id": "cell-c6d0282ee295bb85",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values for non-terminal states: \n",
      " [[ 0.93861961  0.95193383  0.96395322  0.97612436  1.        ]\n",
      " [ 0.92691612  0.          0.          0.88371776 -1.        ]\n",
      " [ 0.91395183  0.90255591  0.89130209  0.88057637  0.79971442]]\n",
      "Value for terminal state: 0.0 \n",
      "\n",
      "Policy for non-terminal states: \n",
      "---------------------\n",
      "| E | E | E | E | N | \n",
      "---------------------\n",
      "| N | N | N | W | N | \n",
      "---------------------\n",
      "| N | W | W | W | S | \n",
      "---------------------\n",
      "                            ---\n",
      "Policy for terminal state: | N |\n",
      "                            ---\n"
     ]
    }
   ],
   "source": [
    "# Run for stochastic MDP, gamma = .99\n",
    "mdp = GridWorldMDP()\n",
    "v, pi = value_iteration(.99, mdp)\n",
    "print_value_table(v)\n",
    "print_policy(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6d055e91280e2309b99724e4b6d6b1c6",
     "grade": false,
     "grade_id": "cell-b80f5f5b9d1398a6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Does the policy that the algorithm found looks reasonable? For instance, what's the policy for state $S_8$? Is that a good idea? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c078f48627ee20611cff693693deb293",
     "grade": true,
     "grade_id": "cell-daff5655fe78f131",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** Yes, the found policy looks reasonable. At state $S_8$, the policy is to go west, which ensures that there is 0% chance for the agent to step into the red state. Taking any other actions will be risky. For most cells in the bottom row, the policy is go west such that the agent can reach the green state by going through the top row. This is also reasonable, because state $S_8$ is located right next to the red state, so it's more dangerous this way. And even if the agent is smart enough to go west at state $S_8$ such that there is no chance of going to the red state, there is still 80% chance that the agent will be stuck at the same state in the next time step. Hence, we think it is a very good idea for the agent to try to go west at the bottom row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5c94489bd5a5a7608e4fa04705176796",
     "grade": false,
     "grade_id": "cell-d4840da19cbbb63a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Test your implementation using this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e5071817f08d1145df1c5095dab2e7dc",
     "grade": false,
     "grade_id": "cell-f89a5e7709d41efc",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed: state-value test, for gamma=.99\n",
      "Passed: policy test, for gamma=.99\n"
     ]
    }
   ],
   "source": [
    "test_value_iteration(v, pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7a2a2376284478e5ff87e8fc45681df1",
     "grade": false,
     "grade_id": "cell-32b52b966ea12de5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Run value iteration for the same scenario as above, but now with $\\gamma=.9$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "45da583d4680e326a85cde59f0373b3e",
     "grade": false,
     "grade_id": "cell-3f797c0f704c2394",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values for non-terminal states: \n",
      " [[ 0.5663144   0.65360208  0.74438015  0.84776628  1.        ]\n",
      " [ 0.4972516   0.          0.          0.57185903 -1.        ]\n",
      " [ 0.43084407  0.37830179  0.41624465  0.47405641  0.2761765 ]]\n",
      "Value for terminal state: 0.0 \n",
      "\n",
      "Policy for non-terminal states: \n",
      "---------------------\n",
      "| E | E | E | E | N | \n",
      "---------------------\n",
      "| N | N | N | N | N | \n",
      "---------------------\n",
      "| N | W | E | N | W | \n",
      "---------------------\n",
      "                            ---\n",
      "Policy for terminal state: | N |\n",
      "                            ---\n"
     ]
    }
   ],
   "source": [
    "# Run for stochastic MDP, gamma = .9\n",
    "mdp = GridWorldMDP()\n",
    "v, pi = value_iteration(.9, mdp)\n",
    "print_value_table(v)\n",
    "print_policy(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "90e75aaf90f849962cc2170695e557c0",
     "grade": false,
     "grade_id": "cell-9192d61af754d47b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Do you notice any difference between the greedy policy for the two different discount factors. If so, what's the difference, and why do you think this happened?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "5bd2832c73ff6f59dd2dfe977ff3fd50",
     "grade": true,
     "grade_id": "cell-1a675e7574dce1d5",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** Informally, we can think of the discount factor $\\gamma$ as the \"ability to see the future\", since it controls how much the agent chooses to value the \"delayed reward\" over the \"immediate reward\". With the higher discount factor, the future gain diminishes slower with time, which means that the optimal policy will favor the longer path as long as it is safer to get the goal along this path. So suppose that the agent starts at the middle cell of the bottom row (state $S_{12}$). With $\\gamma = 0.99$, the agent is more careful and will choose to go west to reach the goal through the top row, despite the fact that this is the longer path. On the other hand, with $\\gamma = 0.9$, the agent takes more risks and will attempt to reach the goal by going east. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "79f657022c51018529b8d26b0ddd63c8",
     "grade": false,
     "grade_id": "cell-01feb7e04644407c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Task 2: Q-learning\n",
    "\n",
    "In the previous task, you solved for $V^*(s)$ and the greedy policy $\\pi^*(s)$, with the entire model of the MDP being available to you. This is however not very practical since for most problems we are trying to solve, the model is not known, and estimating the model is quite often a very tedious process which often also requires a lot of simplifications. \n",
    "\n",
    "#### Q-learning algorithm\n",
    "$\n",
    "\\text{Initialize}~Q(s,a), ~ \\forall~ s \\in {\\cal S},~ a~\\in {\\cal A} \\\\\n",
    "\\textbf{Repeat}~\\text{(for each episode):}\\\\\n",
    "\\quad \\text{Initialize}~s\\\\\n",
    "\\qquad \\textbf{Repeat}~\\text{(for each step in episode):}\\\\\n",
    "\\qquad\\quad \\text{Chose $a$ from $s$ using poliy derived from $Q$ (e.g., $\\epsilon$-greedy)}\\\\\n",
    "\\qquad\\quad \\text{Take action a, observe r, s'}\\\\\n",
    "\\qquad\\quad Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left(r + \\gamma~\\underset{a}{\\text{max}}~Q(s',a) - Q(s,a) \\right) \\\\\n",
    "\\qquad\\quad s \\leftarrow s' \\\\\n",
    "\\qquad \\text{Until s is terminal}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f704f54a37111f555333212087572ac6",
     "grade": false,
     "grade_id": "cell-c974d24244fe78ca",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Task 2.1 Model-free control\n",
    "Why is it that Q-learning does not require a model of the MDP to solve for it?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b369cc3d0a009ff83d971bc83c58fe69",
     "grade": true,
     "grade_id": "cell-448842959cfe9780",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** There are 2 reasons for this. The first reason is that Q-learning actually learns directly from episodes of experience. So even though the agent doesn't know about the reward function, nor does it know anything about the state-transition probablities, the agent can still observe the reward and the state transitions while it is performing the episodes. \n",
    "\n",
    "The second reason is that Q-learning uses the action-value function $Q(s,a)$ to derive the policy instead of the state-value function $V(s)$. This is very important because acting $\\varepsilon$-greedily with respect to $V(s)$ requires the full knowledge of both the reward function and the state transition probability. This problem wouldn't arise if we instead keep a table that stores $Q(s,a)$, since deriving the $\\varepsilon$-greedy policy requires only the knowledge of the action-values (as shown in the next task 2.2). Of course, building up the table of $Q(s,a)$ itself requires both the reward and the action-values on the next state, but these can be observed from the environment while the agent is performing the episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9570fd8489dd5eec632778ab56e56c8b",
     "grade": false,
     "grade_id": "cell-bbe7f99d1cc4e6af",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Task 2.2  Implement an $\\epsilon$-greedy policy\n",
    "The goal of the Q-learning algorithm is to find the optimal policy $\\pi^*$, by estimating the state action value function under the optimal policy, i.e. $Q^*(s, a)$. From $Q^*(s,a)$, the agent can follow $\\pi^*$, by choosing the action with that yields the largest expected value for each state, i.e. $\\underset{a}{\\text{argmax}}~Q^*(s, a)$.\n",
    "\n",
    "However, when training a Q-learning model, the agent typically follows another policy to explore the environment. In reinforcement learning this is known as off-policy learning. \n",
    "\n",
    "Your task is to implement a widely popular exploration policy, known as  the $\\epsilon$-greedy policy, in the cell below.\n",
    "\n",
    "An $\\epsilon$-Greedy policy should:\n",
    "* with probability $\\epsilon$ take an uniformly-random action.\n",
    "* otherwise choose the best action according to the estimated state action values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ec694eb01517a447e2d27c05d3e69b9a",
     "grade": true,
     "grade_id": "cell-48c826a87791fb56",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def eps_greedy_policy(q_values, eps):\n",
    "    '''\n",
    "    Creates an epsilon-greedy policy\n",
    "    :param q_values: set of Q-values of shape (num actions,)\n",
    "    :param eps: probability of taking a uniform random action \n",
    "    :return: policy of shape (num actions,)\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    policy = np.ones(q_values.shape) * (eps/len(q_values))\n",
    "    policy[np.argmax(q_values)] += (1 - eps)\n",
    "    \n",
    "    return policy    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "583492c65cb090c181c66b42c2a51eff",
     "grade": false,
     "grade_id": "cell-6d33489b428b1179",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Run the cell below to test your implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8803a2f8fb24390fc517a97579e04242",
     "grade": false,
     "grade_id": "cell-80bd577e278ec0b0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed, good job!\n"
     ]
    }
   ],
   "source": [
    "mdp = GridWorldMDP()\n",
    "\n",
    "# Test shape of output\n",
    "actions = mdp.get_actions()\n",
    "for eps in (0, 1):\n",
    "    foo = np.zeros([len(actions)])\n",
    "    foo[0] = 1.\n",
    "    eps_greedy = eps_greedy_policy(foo, eps)\n",
    "    assert foo.shape == eps_greedy.shape, \"wrong shape of output\"\n",
    "actions = [i for i in range(10)]\n",
    "for eps in (0, 1):\n",
    "    foo = np.zeros([len(actions)])\n",
    "    foo[0] = 1.\n",
    "    eps_greedy = eps_greedy_policy(foo, eps)\n",
    "    assert foo.shape == eps_greedy.shape, \"wrong shape of output\"\n",
    "\n",
    "# Test for greedy actions\n",
    "for a in actions:\n",
    "    foo = np.zeros([len(actions)])\n",
    "    foo[a] = 1.\n",
    "    eps_greedy = eps_greedy_policy(foo, 0)\n",
    "    assert np.array_equal(foo, eps_greedy), \"policy is not greedy\"\n",
    "\n",
    "# Test for uniform distribution, when eps=1\n",
    "eps_greedy = eps_greedy_policy(foo, 1)\n",
    "assert all(p==eps_greedy[0] for p in eps_greedy) and np.sum(eps_greedy)==1, \\\n",
    "\"policy does not return a uniform distribution for eps=1\"\n",
    "\n",
    "print('Test passed, good job!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "21d88565b3663fe971606656dd045804",
     "grade": false,
     "grade_id": "cell-1dccaeebe5a41325",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Task 2.2: Implement the Q-learning algorithm\n",
    "\n",
    "Now it's time to actually implement the Q-learning algorithm. Unlike the Value iteration where there is no direct interactions with the environment, the Q-learning algorithm builds up its estimations by interacting and exploring the environment. \n",
    "\n",
    "To enable the agent to explore the environment a set of helper functions are provided:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "02bb92b833a3dc4e535cda13a4a1fae4",
     "grade": false,
     "grade_id": "cell-881edd2be439489e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function reset in module gridworld_mdp:\n",
      "\n",
      "reset(self)\n",
      "    Resets the environment and the agent is positioned in the initial state in the bottom left corner.\n",
      "    :return: state, reward, terminal\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(GridWorldMDP.reset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "589834a8662125b7792560c134990d91",
     "grade": false,
     "grade_id": "cell-061e7670ebd7b35c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function step in module gridworld_mdp:\n",
      "\n",
      "step(self, action)\n",
      "    Takes one step in the environment using the selected action\n",
      "    :param action: action to execute, integer\n",
      "    :return: state, reward, terminal\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(GridWorldMDP.step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8b92ae99e0b252016ed7040a0f706cec",
     "grade": false,
     "grade_id": "cell-15fa6bbf763cdc6f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Implement your version of Q-learning in the cell below. \n",
    "\n",
    "**Hint:** It might be useful to study the pseudocode provided above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "465ba05d25e3bc7b0121f8a025b38831",
     "grade": true,
     "grade_id": "cell-3912d729d9527acd",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def q_learning(eps, gamma):\n",
    "    Q = np.zeros([16, 4]) # state action value table\n",
    "    pi = np.zeros([16]) # greedy policy table\n",
    "    alpha = .01\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    for _ in range(10000):\n",
    "        mdp.reset()\n",
    "        sCurr = 10\n",
    "        while True:\n",
    "            action = np.random.choice(Q.shape[1], p=eps_greedy_policy(Q[sCurr], eps))\n",
    "            sNext, reward, terminal = mdp.step(action)\n",
    "            Q[sCurr, action] += alpha * (reward + gamma * np.max(Q[sNext]) - Q[sCurr, action])\n",
    "            sCurr = sNext\n",
    "            if terminal: break\n",
    "    pi = np.argmax(Q, axis=1)\n",
    "    return pi, Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2db55b6a6838a75f15ef87f979ead425",
     "grade": false,
     "grade_id": "cell-b48032d234ecb11d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Run Q-learning with  $\\epsilon = 1$ for the MDP with $\\gamma=0.99$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bebc79f9656b76b9e936221eb633cc4b",
     "grade": false,
     "grade_id": "cell-0464324eb2e2bf9c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy for non-terminal states: \n",
      "---------------------\n",
      "| E | E | E | E | S | \n",
      "---------------------\n",
      "| N | N | N | W | E | \n",
      "---------------------\n",
      "| N | W | W | W | S | \n",
      "---------------------\n",
      "                            ---\n",
      "Policy for terminal state: | N |\n",
      "                            ---\n"
     ]
    }
   ],
   "source": [
    "pi, Q = q_learning(1, .99)\n",
    "print_policy(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9bc5026e570acf5162847f04d4b2daf3",
     "grade": false,
     "grade_id": "cell-a424df8abe557f2e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Test your implementation by running the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fe0420c08764bdf6ddec81e380ccf729",
     "grade": false,
     "grade_id": "cell-e2832d3538099d67",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed: policy test, for gamma=.99\n"
     ]
    }
   ],
   "source": [
    "test_q_learning(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "49f7e5d4884836a1c3c968b767d51f40",
     "grade": false,
     "grade_id": "cell-d3623c3f5c170bd4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Run Q-learning with $\\epsilon=0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8913eaf75e0f0d1c388d682d5767e0e6",
     "grade": false,
     "grade_id": "cell-1c095409c30320d7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy for non-terminal states: \n",
      "---------------------\n",
      "| N | N | N | N | N | \n",
      "---------------------\n",
      "| N | N | N | N | W | \n",
      "---------------------\n",
      "| N | N | N | N | W | \n",
      "---------------------\n",
      "                            ---\n",
      "Policy for terminal state: | N |\n",
      "                            ---\n"
     ]
    }
   ],
   "source": [
    "pi, Q = q_learning(0, .99)\n",
    "print_policy(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f8211c5750767a6a9ad1290cffb73aa7",
     "grade": false,
     "grade_id": "cell-d3383d13bae73e68",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "You ran your implementation with $\\epsilon$ set to both 0 and 1. What are the results, and your conclusions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "f4556e9c53e5bfbac90a17601f48d1c1",
     "grade": true,
     "grade_id": "cell-54eb158e84c99275",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** According to the $\\varepsilon$-greedy policy, when $\\varepsilon = 0$, we obtain a fully-greedy policy (i.e. always take the action that gives the maximum action-value). In this case, we get exact same result as in the value iteration algorithm, and this is reasonable, because value iteration actually follows the greedy policy.\n",
    "\n",
    "On the other hand, when $\\varepsilon = 1$, then we are following a uniform-random policy (i.e. all actions are equally likely to be chosen).\n",
    "\n",
    "TODO: EXPLAIN MORE ABOUT THE SECOND CASE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f272655e0a5e12f3fcfe70c514467e1f",
     "grade": false,
     "grade_id": "cell-ae2a001335118014",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Task 3: Deep Double Q-learning (DDQN)\n",
    "For this task, you will implement a DDQN (double deep Q-learning network) to solve one of the problems of the OpenAI gym. Before we get into details about these type of networks, let's first review the simpler, DQN (deep Q-learning network) version. \n",
    "\n",
    "#### Deep Q Networks\n",
    "As we saw in the video lectures, using a neural network as a state action value approximator is a great idea. However, if one tries to use this approach with Q-learning, it's very likely that the optimization will be very unstable. To remediate this, two main ideas are used. First, we use experience replay, in order to decorrelate the experience samples we obtain when exploring the environment. Second, we use two networks instead of one, in order to fix the optimization targets. That is, for a given minibatch sampled from the replay buffer, we'll optimize the weights of only one of the networks (commonly denoted as the \"online\" network), using the gradients w.r.t a loss function. This loss function is computed as the mean squared error between the current action values, computed according to the **online** network, and the temporal difference (TD) targets, computed using the other, **fixed network** (which we'll refer to as the \"target\" network).\n",
    "\n",
    "That is, the loss function is \n",
    "\n",
    "$$ L(\\theta) = \\frac{1}{N}\\sum_{i=1}^N \\left(Q(s_i,a_i; \\theta\\right) - Y_i)^2~,$$\n",
    "\n",
    "where $N$ is the number of samples in your minibatch, $Q(s,a;\\theta)$ is the state action value estimate, according to the online network (with parameters $\\theta$), and $Y_t$ is the TD target, computed as\n",
    "\n",
    "$$ Y_i = r_i +  \\gamma ~\\underset{a}{\\text{max}}~Q(s_i', a; \\theta^-)~, $$\n",
    "\n",
    "where $Q(s', a;\\theta')$ is the action value estimate, according to the fixed network (with parameters $\\theta^-$).\n",
    "\n",
    "Finally, so that the offline parameters are also updated, we periodically change the roles of the networks, fixing the online one, and training the other.\n",
    "\n",
    "#### Double Deep Q Networks\n",
    "\n",
    "The idea explained above works well in practice, but later it was discovered that this approach is very prone to overestimating the state action values. The main reason for this is that the max operator, used to select the greedy action when computing the TD target, uses the same values both to select and to evaluate an action (this tends to prefer overestimated actions). In order to prevent this, we can decouple the selection from the evaluation, which is the idea that created DDQN. More concretely, the TD target for a DDQN is now \n",
    "\n",
    "$$ Y_i = r_i + \\gamma Q(s_i', \\underset{a}{\\text{argmax}}Q(s_i',a;\\theta); \\theta^-)~. $$\n",
    "\n",
    "Hence, we're using the **online** network to select which action is best, but we use the **fixed** network to evaluate the state action value for that chosen action in the next state. This is what makes DDQN not overestimate (as much) the state action values, which in turn helps us to train faster and obtain better policies.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ffc8f76962e410ad44919896b9d0de31",
     "grade": false,
     "grade_id": "cell-b37fc5ebe369b45a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Environment\n",
    "\n",
    "The problem you will solve for this task is the inverted pendulum problem. \n",
    "On [Open AIs environment documentation](https://gym.openai.com/envs/CartPole-v0) , the following description is provided:\n",
    "\n",
    "*A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every time step that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.*\n",
    "\n",
    "![title](./cartpole.jpg) \n",
    "\n",
    "#### Implementation\n",
    "We'll solve this task using a DDQN. Most of the code is provided for you, in the file **ddqn_model.py**. This file contains the implementation of a neural network, which is described in the table below (feel free to experiment with different architectures).\n",
    "\n",
    "|Layer 1: units, activation | Layer 2: units, activation | Layer 3: units, activation | Cost function |\n",
    "|---------------------------|----------------------------|----------------------------|---------------|\n",
    "| 100, ReLu                 | 60, ReLu                   | number of actions, linear | MSE           |\n",
    "\n",
    "The only missing part of the code is the function that computes the TD targets for each minibatch of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "55a5b74265fad2a849d7bcf7b7d09d45",
     "grade": false,
     "grade_id": "cell-ae647de789982b2e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Task 3.1:  Calculate TD-target\n",
    "\n",
    "For this task, you will calculate the temporal difference target used for the loss in the double Q-learning algorithm. Your implementation should follow precisely the equation defined above for the TD target of DDQNs, with one exception: when s' is terminal, the TD target for it should simply be $ Y_i = r_i$. Why is this necessary?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "40e03a98c7372496040da5499e1fc29a",
     "grade": true,
     "grade_id": "cell-d28bf13d3d581368",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** The TD target for DDQN is calculated using the best estimated action-value of the next state. But if $s'$ is terminal, then there is no next state after that. That's why it is neccessary to set $Y_i = r_i$ in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f6a429aaede47e041414f7fac4093390",
     "grade": false,
     "grade_id": "cell-26a12456d7c70776",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Implement your function in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "5b73cff549519ebc4c082c01928539b0",
     "grade": true,
     "grade_id": "cell-e73bca0bd9d5574a",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def calculate_td_targets(q1_batch, q2_batch, r_batch, t_batch, gamma=.99):\n",
    "    '''\n",
    "    Calculates the TD-target used for the loss\n",
    "    : param q1_batch: Batch of Q(s', a) from online network, shape (N, num actions)\n",
    "    : param q2_batch: Batch of Q(s', a) from target network, shape (N, num actions)\n",
    "    : param r_batch: Batch of rewards, shape (N, 1)\n",
    "    : param t_batch: Batch of booleans indicating if state, s' is terminal, shape (N, 1)\n",
    "    : return: TD-target, shape (N, 1)\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    N = q1_batch.shape[0]\n",
    "    best_actions = np.argmax(q1_batch, axis=1).reshape(N,1) # Nx1\n",
    "    Q = np.array([q2_batch[s,a] for s,a in enumerate(best_actions)]) # Nx1 \n",
    "    Y = r_batch + gamma * np.logical_not(t_batch) * Q\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b22afaa707ff45fb62badaadf2d632e7",
     "grade": false,
     "grade_id": "cell-401656fa71e3f227",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Test your implementation by trying to solve the reinforcement learning problem for the Cartpole environment. The following cell defines the `train_loop_ddqn` function, which will be called ahead.\n",
    "\n",
    "**Note:** If you have issues with the env.render() command below on your system, you may simply comment it out. However, you would be missing out on a visualization of the training episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import dependencies\n",
    "import numpy as np\n",
    "import gym\n",
    "from keras.utils.np_utils import to_categorical as one_hot\n",
    "from collections import namedtuple\n",
    "from dqn_model import DoubleQLearningModel, ExperienceReplay\n",
    "\n",
    "def train_loop_ddqn(model, env, num_episodes, batch_size=64, gamma=.94):        \n",
    "    Transition = namedtuple(\"Transition\", [\"s\", \"a\", \"r\", \"next_s\", \"t\"])\n",
    "    eps = 1.\n",
    "    eps_end = .1 \n",
    "    eps_decay = .001\n",
    "    R_buffer = []\n",
    "    R_avg = []\n",
    "    for i in range(num_episodes):\n",
    "        state = env.reset() #reset to initial state\n",
    "        state = np.expand_dims(state, axis=0)/2\n",
    "        terminal = False # reset terminal flag\n",
    "        ep_reward = 0\n",
    "        q_buffer = []\n",
    "        steps = 0\n",
    "        while not terminal:\n",
    "            env.render() # comment this line out if you don't want to / cannot render the environment on your system\n",
    "            steps += 1\n",
    "            q_values = model.get_q_values(state)\n",
    "            q_buffer.append(q_values)\n",
    "            policy = eps_greedy_policy(q_values.squeeze(), eps) \n",
    "            action = np.random.choice(num_actions, p=policy) # sample action from epsilon-greedy policy\n",
    "            new_state, reward, terminal, _ = env.step(action) # take one step in the evironment\n",
    "            new_state = np.expand_dims(new_state, axis=0)/2\n",
    "            \n",
    "            # only use the terminal flag for ending the episode and not for training\n",
    "            # if the flag is set due to that the maximum amount of steps is reached \n",
    "            t_to_buffer = terminal if not steps == 200 else False\n",
    "            \n",
    "            # store data to replay buffer\n",
    "            replay_buffer.add(Transition(s=state, a=action, r=reward, next_s=new_state, t=t_to_buffer))\n",
    "            state = new_state\n",
    "            ep_reward += reward\n",
    "            \n",
    "            # if buffer contains more than 1000 samples, perform one training step\n",
    "            if replay_buffer.buffer_length > 1000:\n",
    "                s, a, r, s_, t = replay_buffer.sample_minibatch(batch_size) # sample a minibatch of transitions\n",
    "                q_1, q_2 = model.get_q_values_for_both_models(np.squeeze(s_))\n",
    "                td_target = calculate_td_targets(q_1, q_2, r, t, gamma)\n",
    "                model.update(s, td_target, a)    \n",
    "                \n",
    "        eps = max(eps - eps_decay, eps_end) # decrease epsilon        \n",
    "        R_buffer.append(ep_reward)\n",
    "        \n",
    "        # running average of episodic rewards\n",
    "        R_avg.append(.05 * R_buffer[i] + .95 * R_avg[i-1]) if i > 0 else R_avg.append(R_buffer[i])\n",
    "        print('Episode: ', i, 'Reward:', ep_reward, 'Epsilon', eps, 'mean q', np.mean(np.array(q_buffer)))\n",
    "        \n",
    "        # if running average > 195, the task is considerd solved\n",
    "        if R_avg[-1] > 195:\n",
    "            return R_buffer, R_avg\n",
    "    return R_buffer, R_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "51f90902daf95d61d17b1d2c32877abf",
     "grade": false,
     "grade_id": "cell-75c84628ce999711",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "and the next cell performs the actual training. \n",
    "\n",
    "A Working implementation should start to improve after 500 episodes. An episodic reward of around 200 is likely to be achieved after 800 episodes for a batchsize of 128, and 1000 episodes for a batchsize of 64. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Episode:  0 Reward: 27.0 Epsilon 0.999 mean q -8.64127e-08\n",
      "Episode:  1 Reward: 25.0 Epsilon 0.998 mean q -1.1727431e-07\n",
      "Episode:  2 Reward: 17.0 Epsilon 0.997 mean q -1.0450472e-07\n",
      "Episode:  3 Reward: 35.0 Epsilon 0.996 mean q -5.2617054e-08\n",
      "Episode:  4 Reward: 19.0 Epsilon 0.995 mean q -1.3981344e-07\n",
      "Episode:  5 Reward: 18.0 Epsilon 0.994 mean q -1.6108243e-07\n",
      "Episode:  6 Reward: 14.0 Epsilon 0.993 mean q -1.3958899e-07\n",
      "Episode:  7 Reward: 46.0 Epsilon 0.992 mean q -1.7608312e-07\n",
      "Episode:  8 Reward: 33.0 Epsilon 0.991 mean q -1.6700344e-07\n",
      "Episode:  9 Reward: 14.0 Epsilon 0.99 mean q -1.998225e-07\n",
      "Episode:  10 Reward: 10.0 Epsilon 0.989 mean q -1.5119761e-07\n",
      "Episode:  11 Reward: 12.0 Epsilon 0.988 mean q -2.9096006e-07\n",
      "Episode:  12 Reward: 10.0 Epsilon 0.987 mean q -3.3470775e-07\n",
      "Episode:  13 Reward: 34.0 Epsilon 0.986 mean q -1.5284881e-07\n",
      "Episode:  14 Reward: 28.0 Epsilon 0.985 mean q -1.9983756e-07\n",
      "Episode:  15 Reward: 15.0 Epsilon 0.984 mean q -2.7968292e-07\n",
      "Episode:  16 Reward: 23.0 Epsilon 0.983 mean q -1.8062292e-07\n",
      "Episode:  17 Reward: 20.0 Epsilon 0.982 mean q -1.2548294e-07\n",
      "Episode:  18 Reward: 26.0 Epsilon 0.981 mean q -2.0403405e-07\n",
      "Episode:  19 Reward: 14.0 Epsilon 0.98 mean q -1.6498215e-07\n",
      "Episode:  20 Reward: 46.0 Epsilon 0.979 mean q -1.4808765e-07\n",
      "Episode:  21 Reward: 20.0 Epsilon 0.978 mean q -1.3086225e-07\n",
      "Episode:  22 Reward: 31.0 Epsilon 0.977 mean q -1.6378024e-07\n",
      "Episode:  23 Reward: 30.0 Epsilon 0.976 mean q -8.877784e-08\n",
      "Episode:  24 Reward: 16.0 Epsilon 0.975 mean q -2.0354074e-07\n",
      "Episode:  25 Reward: 29.0 Epsilon 0.974 mean q -1.2771275e-07\n",
      "Episode:  26 Reward: 14.0 Epsilon 0.973 mean q -2.922325e-07\n",
      "Episode:  27 Reward: 13.0 Epsilon 0.972 mean q -2.9425348e-07\n",
      "Episode:  28 Reward: 10.0 Epsilon 0.971 mean q -3.7149158e-07\n",
      "Episode:  29 Reward: 18.0 Epsilon 0.97 mean q -2.2415318e-07\n",
      "Episode:  30 Reward: 26.0 Epsilon 0.969 mean q -1.3253286e-07\n",
      "Episode:  31 Reward: 9.0 Epsilon 0.968 mean q -2.0863644e-07\n",
      "Episode:  32 Reward: 10.0 Epsilon 0.967 mean q -3.237099e-07\n",
      "Episode:  33 Reward: 34.0 Epsilon 0.966 mean q -1.9210306e-07\n",
      "Episode:  34 Reward: 16.0 Epsilon 0.965 mean q -1.6314361e-07\n",
      "Episode:  35 Reward: 17.0 Epsilon 0.964 mean q -1.2123327e-07\n",
      "Episode:  36 Reward: 33.0 Epsilon 0.963 mean q -7.5139035e-08\n",
      "Episode:  37 Reward: 66.0 Epsilon 0.962 mean q -6.5250234e-08\n",
      "Episode:  38 Reward: 47.0 Epsilon 0.961 mean q -3.390028e-08\n",
      "Episode:  39 Reward: 35.0 Epsilon 0.96 mean q -8.491281e-08\n",
      "Episode:  40 Reward: 30.0 Epsilon 0.959 mean q -1.807795e-07\n",
      "Episode:  41 Reward: 10.0 Epsilon 0.958 mean q -3.7480808e-07\n",
      "Episode:  42 Reward: 21.0 Epsilon 0.957 mean q 0.0037809855\n",
      "Episode:  43 Reward: 53.0 Epsilon 0.956 mean q 0.010947802\n",
      "Episode:  44 Reward: 13.0 Epsilon 0.955 mean q 0.043376938\n",
      "Episode:  45 Reward: 45.0 Epsilon 0.954 mean q 0.033515524\n",
      "Episode:  46 Reward: 28.0 Epsilon 0.953 mean q 0.07130588\n",
      "Episode:  47 Reward: 18.0 Epsilon 0.952 mean q 0.08680571\n",
      "Episode:  48 Reward: 19.0 Epsilon 0.951 mean q 0.09479672\n",
      "Episode:  49 Reward: 20.0 Epsilon 0.95 mean q 0.09959461\n",
      "Episode:  50 Reward: 22.0 Epsilon 0.949 mean q 0.10795186\n",
      "Episode:  51 Reward: 15.0 Epsilon 0.948 mean q 0.14049341\n",
      "Episode:  52 Reward: 111.0 Epsilon 0.947 mean q 0.3805078\n",
      "Episode:  53 Reward: 12.0 Epsilon 0.946 mean q 0.34111378\n",
      "Episode:  54 Reward: 15.0 Epsilon 0.945 mean q 0.34668365\n",
      "Episode:  55 Reward: 11.0 Epsilon 0.944 mean q 0.41043714\n",
      "Episode:  56 Reward: 11.0 Epsilon 0.943 mean q 0.54474145\n",
      "Episode:  57 Reward: 12.0 Epsilon 0.942 mean q 0.46273014\n",
      "Episode:  58 Reward: 15.0 Epsilon 0.941 mean q 0.49503395\n",
      "Episode:  59 Reward: 17.0 Epsilon 0.94 mean q 0.4808211\n",
      "Episode:  60 Reward: 24.0 Epsilon 0.939 mean q 0.46710148\n",
      "Episode:  61 Reward: 19.0 Epsilon 0.938 mean q 0.46465653\n",
      "Episode:  62 Reward: 11.0 Epsilon 0.9369999999999999 mean q 0.7397812\n",
      "Episode:  63 Reward: 30.0 Epsilon 0.9359999999999999 mean q 0.7092605\n",
      "Episode:  64 Reward: 9.0 Epsilon 0.9349999999999999 mean q 0.8403173\n",
      "Episode:  65 Reward: 26.0 Epsilon 0.9339999999999999 mean q 0.5931081\n",
      "Episode:  66 Reward: 14.0 Epsilon 0.9329999999999999 mean q 0.785754\n",
      "Episode:  67 Reward: 15.0 Epsilon 0.9319999999999999 mean q 0.86266786\n",
      "Episode:  68 Reward: 15.0 Epsilon 0.9309999999999999 mean q 0.91265005\n",
      "Episode:  69 Reward: 10.0 Epsilon 0.9299999999999999 mean q 1.4450761\n",
      "Episode:  70 Reward: 14.0 Epsilon 0.9289999999999999 mean q 0.8387437\n",
      "Episode:  71 Reward: 31.0 Epsilon 0.9279999999999999 mean q 0.7153833\n",
      "Episode:  72 Reward: 32.0 Epsilon 0.9269999999999999 mean q 1.0425932\n",
      "Episode:  73 Reward: 14.0 Epsilon 0.9259999999999999 mean q 1.3925642\n",
      "Episode:  74 Reward: 37.0 Epsilon 0.9249999999999999 mean q 0.8662022\n",
      "Episode:  75 Reward: 10.0 Epsilon 0.9239999999999999 mean q 1.5446886\n",
      "Episode:  76 Reward: 36.0 Epsilon 0.9229999999999999 mean q 1.0118098\n",
      "Episode:  77 Reward: 13.0 Epsilon 0.9219999999999999 mean q 1.8740058\n",
      "Episode:  78 Reward: 15.0 Epsilon 0.9209999999999999 mean q 1.3620262\n",
      "Episode:  79 Reward: 16.0 Epsilon 0.9199999999999999 mean q 1.898102\n",
      "Episode:  80 Reward: 23.0 Epsilon 0.9189999999999999 mean q 1.4601877\n",
      "Episode:  81 Reward: 21.0 Epsilon 0.9179999999999999 mean q 1.518652\n",
      "Episode:  82 Reward: 15.0 Epsilon 0.9169999999999999 mean q 2.044913\n",
      "Episode:  83 Reward: 13.0 Epsilon 0.9159999999999999 mean q 2.2525723\n",
      "Episode:  84 Reward: 13.0 Epsilon 0.9149999999999999 mean q 2.0807264\n",
      "Episode:  85 Reward: 17.0 Epsilon 0.9139999999999999 mean q 2.36804\n",
      "Episode:  86 Reward: 14.0 Epsilon 0.9129999999999999 mean q 1.7953861\n",
      "Episode:  87 Reward: 14.0 Epsilon 0.9119999999999999 mean q 2.208114\n",
      "Episode:  88 Reward: 9.0 Epsilon 0.9109999999999999 mean q 2.6679292\n",
      "Episode:  89 Reward: 20.0 Epsilon 0.9099999999999999 mean q 2.06885\n",
      "Episode:  90 Reward: 17.0 Epsilon 0.9089999999999999 mean q 2.2379665\n",
      "Episode:  91 Reward: 13.0 Epsilon 0.9079999999999999 mean q 2.315735\n",
      "Episode:  92 Reward: 28.0 Epsilon 0.9069999999999999 mean q 1.6952177\n",
      "Episode:  93 Reward: 28.0 Epsilon 0.9059999999999999 mean q 2.015594\n",
      "Episode:  94 Reward: 21.0 Epsilon 0.9049999999999999 mean q 2.35712\n",
      "Episode:  95 Reward: 13.0 Epsilon 0.9039999999999999 mean q 2.963357\n",
      "Episode:  96 Reward: 19.0 Epsilon 0.9029999999999999 mean q 2.749323\n",
      "Episode:  97 Reward: 14.0 Epsilon 0.9019999999999999 mean q 3.3816395\n",
      "Episode:  98 Reward: 12.0 Epsilon 0.9009999999999999 mean q 3.819352\n",
      "Episode:  99 Reward: 10.0 Epsilon 0.8999999999999999 mean q 3.8902652\n",
      "Episode:  100 Reward: 39.0 Epsilon 0.8989999999999999 mean q 3.2546713\n",
      "Episode:  101 Reward: 13.0 Epsilon 0.8979999999999999 mean q 3.2691052\n",
      "Episode:  102 Reward: 12.0 Epsilon 0.8969999999999999 mean q 3.9342325\n",
      "Episode:  103 Reward: 9.0 Epsilon 0.8959999999999999 mean q 4.1105957\n",
      "Episode:  104 Reward: 26.0 Epsilon 0.8949999999999999 mean q 3.3593366\n",
      "Episode:  105 Reward: 34.0 Epsilon 0.8939999999999999 mean q 2.5148304\n",
      "Episode:  106 Reward: 37.0 Epsilon 0.8929999999999999 mean q 3.0182462\n",
      "Episode:  107 Reward: 18.0 Epsilon 0.8919999999999999 mean q 3.803111\n",
      "Episode:  108 Reward: 17.0 Epsilon 0.8909999999999999 mean q 3.3841443\n",
      "Episode:  109 Reward: 29.0 Epsilon 0.8899999999999999 mean q 3.68054\n",
      "Episode:  110 Reward: 16.0 Epsilon 0.8889999999999999 mean q 4.475849\n",
      "Episode:  111 Reward: 20.0 Epsilon 0.8879999999999999 mean q 3.6342797\n",
      "Episode:  112 Reward: 12.0 Epsilon 0.8869999999999999 mean q 6.3202376\n",
      "Episode:  113 Reward: 18.0 Epsilon 0.8859999999999999 mean q 3.8275375\n",
      "Episode:  114 Reward: 21.0 Epsilon 0.8849999999999999 mean q 3.8019824\n",
      "Episode:  115 Reward: 15.0 Epsilon 0.8839999999999999 mean q 4.74043\n",
      "Episode:  116 Reward: 15.0 Epsilon 0.8829999999999999 mean q 5.4992175\n",
      "Episode:  117 Reward: 16.0 Epsilon 0.8819999999999999 mean q 4.6912985\n",
      "Episode:  118 Reward: 46.0 Epsilon 0.8809999999999999 mean q 3.2654998\n",
      "Episode:  119 Reward: 42.0 Epsilon 0.8799999999999999 mean q 4.3750935\n",
      "Episode:  120 Reward: 12.0 Epsilon 0.8789999999999999 mean q 6.6793976\n",
      "Episode:  121 Reward: 24.0 Epsilon 0.8779999999999999 mean q 4.5499635\n",
      "Episode:  122 Reward: 20.0 Epsilon 0.8769999999999999 mean q 4.598155\n",
      "Episode:  123 Reward: 31.0 Epsilon 0.8759999999999999 mean q 4.683513\n",
      "Episode:  124 Reward: 44.0 Epsilon 0.8749999999999999 mean q 4.883876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  125 Reward: 23.0 Epsilon 0.8739999999999999 mean q 4.894648\n",
      "Episode:  126 Reward: 15.0 Epsilon 0.8729999999999999 mean q 6.0435715\n",
      "Episode:  127 Reward: 12.0 Epsilon 0.8719999999999999 mean q 6.9594955\n",
      "Episode:  128 Reward: 13.0 Epsilon 0.8709999999999999 mean q 7.3852606\n",
      "Episode:  129 Reward: 10.0 Epsilon 0.8699999999999999 mean q 8.447391\n",
      "Episode:  130 Reward: 30.0 Epsilon 0.8689999999999999 mean q 5.682903\n",
      "Episode:  131 Reward: 19.0 Epsilon 0.8679999999999999 mean q 5.246709\n",
      "Episode:  132 Reward: 33.0 Epsilon 0.8669999999999999 mean q 5.811224\n",
      "Episode:  133 Reward: 17.0 Epsilon 0.8659999999999999 mean q 5.7667184\n",
      "Episode:  134 Reward: 16.0 Epsilon 0.8649999999999999 mean q 7.782732\n",
      "Episode:  135 Reward: 13.0 Epsilon 0.8639999999999999 mean q 6.983534\n",
      "Episode:  136 Reward: 21.0 Epsilon 0.8629999999999999 mean q 7.0138826\n",
      "Episode:  137 Reward: 13.0 Epsilon 0.8619999999999999 mean q 6.4729633\n",
      "Episode:  138 Reward: 42.0 Epsilon 0.8609999999999999 mean q 5.9305015\n",
      "Episode:  139 Reward: 13.0 Epsilon 0.8599999999999999 mean q 9.023292\n",
      "Episode:  140 Reward: 14.0 Epsilon 0.8589999999999999 mean q 7.096066\n",
      "Episode:  141 Reward: 11.0 Epsilon 0.8579999999999999 mean q 6.782585\n",
      "Episode:  142 Reward: 21.0 Epsilon 0.8569999999999999 mean q 5.5833883\n",
      "Episode:  143 Reward: 35.0 Epsilon 0.8559999999999999 mean q 6.764245\n",
      "Episode:  144 Reward: 23.0 Epsilon 0.8549999999999999 mean q 7.2101045\n",
      "Episode:  145 Reward: 21.0 Epsilon 0.8539999999999999 mean q 8.543432\n",
      "Episode:  146 Reward: 50.0 Epsilon 0.8529999999999999 mean q 7.782152\n",
      "Episode:  147 Reward: 24.0 Epsilon 0.8519999999999999 mean q 8.372291\n",
      "Episode:  148 Reward: 14.0 Epsilon 0.8509999999999999 mean q 8.636209\n",
      "Episode:  149 Reward: 14.0 Epsilon 0.8499999999999999 mean q 8.479185\n",
      "Episode:  150 Reward: 14.0 Epsilon 0.8489999999999999 mean q 10.242337\n",
      "Episode:  151 Reward: 17.0 Epsilon 0.8479999999999999 mean q 9.557431\n",
      "Episode:  152 Reward: 12.0 Epsilon 0.8469999999999999 mean q 9.64881\n",
      "Episode:  153 Reward: 13.0 Epsilon 0.8459999999999999 mean q 10.442796\n",
      "Episode:  154 Reward: 14.0 Epsilon 0.8449999999999999 mean q 10.145543\n",
      "Episode:  155 Reward: 19.0 Epsilon 0.8439999999999999 mean q 7.1007466\n",
      "Episode:  156 Reward: 18.0 Epsilon 0.8429999999999999 mean q 9.480754\n",
      "Episode:  157 Reward: 25.0 Epsilon 0.8419999999999999 mean q 8.766517\n",
      "Episode:  158 Reward: 18.0 Epsilon 0.8409999999999999 mean q 9.319325\n",
      "Episode:  159 Reward: 14.0 Epsilon 0.8399999999999999 mean q 10.085729\n",
      "Episode:  160 Reward: 12.0 Epsilon 0.8389999999999999 mean q 10.424052\n",
      "Episode:  161 Reward: 26.0 Epsilon 0.8379999999999999 mean q 8.332651\n",
      "Episode:  162 Reward: 14.0 Epsilon 0.8369999999999999 mean q 7.4810743\n",
      "Episode:  163 Reward: 18.0 Epsilon 0.8359999999999999 mean q 9.703399\n",
      "Episode:  164 Reward: 8.0 Epsilon 0.8349999999999999 mean q 12.743248\n",
      "Episode:  165 Reward: 10.0 Epsilon 0.8339999999999999 mean q 14.407507\n",
      "Episode:  166 Reward: 12.0 Epsilon 0.8329999999999999 mean q 11.730072\n",
      "Episode:  167 Reward: 17.0 Epsilon 0.8319999999999999 mean q 9.760638\n",
      "Episode:  168 Reward: 12.0 Epsilon 0.8309999999999998 mean q 11.330468\n",
      "Episode:  169 Reward: 19.0 Epsilon 0.8299999999999998 mean q 8.768706\n",
      "Episode:  170 Reward: 13.0 Epsilon 0.8289999999999998 mean q 12.101212\n",
      "Episode:  171 Reward: 10.0 Epsilon 0.8279999999999998 mean q 14.465036\n",
      "Episode:  172 Reward: 24.0 Epsilon 0.8269999999999998 mean q 10.330171\n",
      "Episode:  173 Reward: 26.0 Epsilon 0.8259999999999998 mean q 10.835799\n",
      "Episode:  174 Reward: 13.0 Epsilon 0.8249999999999998 mean q 8.807722\n",
      "Episode:  175 Reward: 16.0 Epsilon 0.8239999999999998 mean q 11.86344\n",
      "Episode:  176 Reward: 27.0 Epsilon 0.8229999999999998 mean q 11.40388\n",
      "Episode:  177 Reward: 43.0 Epsilon 0.8219999999999998 mean q 9.67009\n",
      "Episode:  178 Reward: 14.0 Epsilon 0.8209999999999998 mean q 12.768807\n",
      "Episode:  179 Reward: 19.0 Epsilon 0.8199999999999998 mean q 12.594099\n",
      "Episode:  180 Reward: 25.0 Epsilon 0.8189999999999998 mean q 8.542466\n",
      "Episode:  181 Reward: 16.0 Epsilon 0.8179999999999998 mean q 11.132917\n",
      "Episode:  182 Reward: 12.0 Epsilon 0.8169999999999998 mean q 10.318828\n",
      "Episode:  183 Reward: 23.0 Epsilon 0.8159999999999998 mean q 11.628988\n",
      "Episode:  184 Reward: 18.0 Epsilon 0.8149999999999998 mean q 12.873276\n",
      "Episode:  185 Reward: 36.0 Epsilon 0.8139999999999998 mean q 10.90409\n",
      "Episode:  186 Reward: 11.0 Epsilon 0.8129999999999998 mean q 14.580239\n",
      "Episode:  187 Reward: 10.0 Epsilon 0.8119999999999998 mean q 15.566943\n",
      "Episode:  188 Reward: 23.0 Epsilon 0.8109999999999998 mean q 12.991495\n",
      "Episode:  189 Reward: 11.0 Epsilon 0.8099999999999998 mean q 14.009277\n",
      "Episode:  190 Reward: 31.0 Epsilon 0.8089999999999998 mean q 11.445642\n",
      "Episode:  191 Reward: 42.0 Epsilon 0.8079999999999998 mean q 11.546803\n",
      "Episode:  192 Reward: 16.0 Epsilon 0.8069999999999998 mean q 13.064217\n",
      "Episode:  193 Reward: 16.0 Epsilon 0.8059999999999998 mean q 14.296256\n",
      "Episode:  194 Reward: 19.0 Epsilon 0.8049999999999998 mean q 14.022247\n",
      "Episode:  195 Reward: 24.0 Epsilon 0.8039999999999998 mean q 12.664719\n",
      "Episode:  196 Reward: 15.0 Epsilon 0.8029999999999998 mean q 10.10735\n",
      "Episode:  197 Reward: 52.0 Epsilon 0.8019999999999998 mean q 10.839586\n",
      "Episode:  198 Reward: 26.0 Epsilon 0.8009999999999998 mean q 13.653884\n",
      "Episode:  199 Reward: 20.0 Epsilon 0.7999999999999998 mean q 13.556445\n",
      "Episode:  200 Reward: 11.0 Epsilon 0.7989999999999998 mean q 12.987548\n",
      "Episode:  201 Reward: 13.0 Epsilon 0.7979999999999998 mean q 17.549635\n",
      "Episode:  202 Reward: 58.0 Epsilon 0.7969999999999998 mean q 12.725464\n",
      "Episode:  203 Reward: 12.0 Epsilon 0.7959999999999998 mean q 18.354383\n",
      "Episode:  204 Reward: 41.0 Epsilon 0.7949999999999998 mean q 11.024849\n",
      "Episode:  205 Reward: 13.0 Epsilon 0.7939999999999998 mean q 16.728905\n",
      "Episode:  206 Reward: 17.0 Epsilon 0.7929999999999998 mean q 15.595646\n",
      "Episode:  207 Reward: 19.0 Epsilon 0.7919999999999998 mean q 15.023733\n",
      "Episode:  208 Reward: 19.0 Epsilon 0.7909999999999998 mean q 15.18293\n",
      "Episode:  209 Reward: 11.0 Epsilon 0.7899999999999998 mean q 18.560081\n",
      "Episode:  210 Reward: 17.0 Epsilon 0.7889999999999998 mean q 11.007399\n",
      "Episode:  211 Reward: 13.0 Epsilon 0.7879999999999998 mean q 17.597715\n",
      "Episode:  212 Reward: 31.0 Epsilon 0.7869999999999998 mean q 11.706297\n",
      "Episode:  213 Reward: 26.0 Epsilon 0.7859999999999998 mean q 14.915132\n",
      "Episode:  214 Reward: 14.0 Epsilon 0.7849999999999998 mean q 18.400509\n",
      "Episode:  215 Reward: 43.0 Epsilon 0.7839999999999998 mean q 14.434928\n",
      "Episode:  216 Reward: 24.0 Epsilon 0.7829999999999998 mean q 15.398735\n",
      "Episode:  217 Reward: 12.0 Epsilon 0.7819999999999998 mean q 18.155321\n",
      "Episode:  218 Reward: 13.0 Epsilon 0.7809999999999998 mean q 16.895542\n",
      "Episode:  219 Reward: 15.0 Epsilon 0.7799999999999998 mean q 16.401558\n",
      "Episode:  220 Reward: 27.0 Epsilon 0.7789999999999998 mean q 15.92734\n",
      "Episode:  221 Reward: 10.0 Epsilon 0.7779999999999998 mean q 12.492096\n",
      "Episode:  222 Reward: 15.0 Epsilon 0.7769999999999998 mean q 18.291445\n",
      "Episode:  223 Reward: 21.0 Epsilon 0.7759999999999998 mean q 16.30224\n",
      "Episode:  224 Reward: 47.0 Epsilon 0.7749999999999998 mean q 15.326573\n",
      "Episode:  225 Reward: 29.0 Epsilon 0.7739999999999998 mean q 15.560618\n",
      "Episode:  226 Reward: 13.0 Epsilon 0.7729999999999998 mean q 16.979591\n",
      "Episode:  227 Reward: 10.0 Epsilon 0.7719999999999998 mean q 19.577196\n",
      "Episode:  228 Reward: 19.0 Epsilon 0.7709999999999998 mean q 16.651068\n",
      "Episode:  229 Reward: 16.0 Epsilon 0.7699999999999998 mean q 17.058285\n",
      "Episode:  230 Reward: 14.0 Epsilon 0.7689999999999998 mean q 18.619343\n",
      "Episode:  231 Reward: 36.0 Epsilon 0.7679999999999998 mean q 13.391686\n",
      "Episode:  232 Reward: 24.0 Epsilon 0.7669999999999998 mean q 13.398247\n",
      "Episode:  233 Reward: 24.0 Epsilon 0.7659999999999998 mean q 16.958458\n",
      "Episode:  234 Reward: 11.0 Epsilon 0.7649999999999998 mean q 19.070686\n",
      "Episode:  235 Reward: 13.0 Epsilon 0.7639999999999998 mean q 13.983152\n",
      "Episode:  236 Reward: 10.0 Epsilon 0.7629999999999998 mean q 20.18403\n",
      "Episode:  237 Reward: 32.0 Epsilon 0.7619999999999998 mean q 17.402298\n",
      "Episode:  238 Reward: 13.0 Epsilon 0.7609999999999998 mean q 14.411209\n",
      "Episode:  239 Reward: 26.0 Epsilon 0.7599999999999998 mean q 17.061256\n",
      "Episode:  240 Reward: 27.0 Epsilon 0.7589999999999998 mean q 16.93276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  241 Reward: 25.0 Epsilon 0.7579999999999998 mean q 16.715828\n",
      "Episode:  242 Reward: 10.0 Epsilon 0.7569999999999998 mean q 20.491611\n",
      "Episode:  243 Reward: 18.0 Epsilon 0.7559999999999998 mean q 17.924454\n",
      "Episode:  244 Reward: 13.0 Epsilon 0.7549999999999998 mean q 18.435055\n",
      "Episode:  245 Reward: 15.0 Epsilon 0.7539999999999998 mean q 18.497993\n",
      "Episode:  246 Reward: 10.0 Epsilon 0.7529999999999998 mean q 18.883602\n",
      "Episode:  247 Reward: 17.0 Epsilon 0.7519999999999998 mean q 18.09465\n",
      "Episode:  248 Reward: 12.0 Epsilon 0.7509999999999998 mean q 19.401873\n",
      "Episode:  249 Reward: 24.0 Epsilon 0.7499999999999998 mean q 17.536608\n",
      "Episode:  250 Reward: 18.0 Epsilon 0.7489999999999998 mean q 17.473183\n",
      "Episode:  251 Reward: 15.0 Epsilon 0.7479999999999998 mean q 17.662884\n",
      "Episode:  252 Reward: 23.0 Epsilon 0.7469999999999998 mean q 17.359085\n",
      "Episode:  253 Reward: 16.0 Epsilon 0.7459999999999998 mean q 14.205808\n",
      "Episode:  254 Reward: 15.0 Epsilon 0.7449999999999998 mean q 19.073248\n",
      "Episode:  255 Reward: 18.0 Epsilon 0.7439999999999998 mean q 17.534231\n",
      "Episode:  256 Reward: 16.0 Epsilon 0.7429999999999998 mean q 18.60534\n",
      "Episode:  257 Reward: 21.0 Epsilon 0.7419999999999998 mean q 17.651861\n",
      "Episode:  258 Reward: 18.0 Epsilon 0.7409999999999998 mean q 18.615807\n",
      "Episode:  259 Reward: 37.0 Epsilon 0.7399999999999998 mean q 16.4739\n",
      "Episode:  260 Reward: 10.0 Epsilon 0.7389999999999998 mean q 19.236866\n",
      "Episode:  261 Reward: 10.0 Epsilon 0.7379999999999998 mean q 19.872456\n",
      "Episode:  262 Reward: 48.0 Epsilon 0.7369999999999998 mean q 16.206276\n",
      "Episode:  263 Reward: 14.0 Epsilon 0.7359999999999998 mean q 18.038197\n",
      "Episode:  264 Reward: 13.0 Epsilon 0.7349999999999998 mean q 19.446672\n",
      "Episode:  265 Reward: 12.0 Epsilon 0.7339999999999998 mean q 20.279856\n",
      "Episode:  266 Reward: 40.0 Epsilon 0.7329999999999998 mean q 16.634802\n",
      "Episode:  267 Reward: 9.0 Epsilon 0.7319999999999998 mean q 19.535292\n",
      "Episode:  268 Reward: 19.0 Epsilon 0.7309999999999998 mean q 18.203173\n",
      "Episode:  269 Reward: 19.0 Epsilon 0.7299999999999998 mean q 17.662256\n",
      "Episode:  270 Reward: 16.0 Epsilon 0.7289999999999998 mean q 17.633583\n",
      "Episode:  271 Reward: 10.0 Epsilon 0.7279999999999998 mean q 18.514143\n",
      "Episode:  272 Reward: 31.0 Epsilon 0.7269999999999998 mean q 17.29685\n",
      "Episode:  273 Reward: 14.0 Epsilon 0.7259999999999998 mean q 17.760805\n",
      "Episode:  274 Reward: 14.0 Epsilon 0.7249999999999998 mean q 17.53319\n",
      "Episode:  275 Reward: 12.0 Epsilon 0.7239999999999998 mean q 14.4588175\n",
      "Episode:  276 Reward: 20.0 Epsilon 0.7229999999999998 mean q 17.977348\n",
      "Episode:  277 Reward: 12.0 Epsilon 0.7219999999999998 mean q 17.878506\n",
      "Episode:  278 Reward: 11.0 Epsilon 0.7209999999999998 mean q 19.76105\n",
      "Episode:  279 Reward: 21.0 Epsilon 0.7199999999999998 mean q 17.232515\n",
      "Episode:  280 Reward: 11.0 Epsilon 0.7189999999999998 mean q 18.236551\n",
      "Episode:  281 Reward: 11.0 Epsilon 0.7179999999999997 mean q 15.143546\n",
      "Episode:  282 Reward: 15.0 Epsilon 0.7169999999999997 mean q 17.889645\n",
      "Episode:  283 Reward: 30.0 Epsilon 0.7159999999999997 mean q 16.21116\n",
      "Episode:  284 Reward: 11.0 Epsilon 0.7149999999999997 mean q 18.228827\n",
      "Episode:  285 Reward: 19.0 Epsilon 0.7139999999999997 mean q 17.320683\n",
      "Episode:  286 Reward: 15.0 Epsilon 0.7129999999999997 mean q 17.671196\n",
      "Episode:  287 Reward: 16.0 Epsilon 0.7119999999999997 mean q 17.240944\n",
      "Episode:  288 Reward: 14.0 Epsilon 0.7109999999999997 mean q 18.34055\n",
      "Episode:  289 Reward: 12.0 Epsilon 0.7099999999999997 mean q 17.17084\n",
      "Episode:  290 Reward: 16.0 Epsilon 0.7089999999999997 mean q 17.077303\n",
      "Episode:  291 Reward: 14.0 Epsilon 0.7079999999999997 mean q 17.193382\n",
      "Episode:  292 Reward: 12.0 Epsilon 0.7069999999999997 mean q 17.700731\n",
      "Episode:  293 Reward: 10.0 Epsilon 0.7059999999999997 mean q 17.99727\n",
      "Episode:  294 Reward: 19.0 Epsilon 0.7049999999999997 mean q 16.733648\n",
      "Episode:  295 Reward: 15.0 Epsilon 0.7039999999999997 mean q 16.192574\n",
      "Episode:  296 Reward: 26.0 Epsilon 0.7029999999999997 mean q 15.787577\n",
      "Episode:  297 Reward: 15.0 Epsilon 0.7019999999999997 mean q 16.595783\n",
      "Episode:  298 Reward: 18.0 Epsilon 0.7009999999999997 mean q 15.8895\n",
      "Episode:  299 Reward: 16.0 Epsilon 0.6999999999999997 mean q 16.575191\n",
      "Episode:  300 Reward: 11.0 Epsilon 0.6989999999999997 mean q 16.649132\n",
      "Episode:  301 Reward: 13.0 Epsilon 0.6979999999999997 mean q 16.49273\n",
      "Episode:  302 Reward: 19.0 Epsilon 0.6969999999999997 mean q 15.632744\n",
      "Episode:  303 Reward: 11.0 Epsilon 0.6959999999999997 mean q 16.252419\n",
      "Episode:  304 Reward: 13.0 Epsilon 0.6949999999999997 mean q 16.712934\n",
      "Episode:  305 Reward: 26.0 Epsilon 0.6939999999999997 mean q 15.493844\n",
      "Episode:  306 Reward: 16.0 Epsilon 0.6929999999999997 mean q 15.945378\n",
      "Episode:  307 Reward: 31.0 Epsilon 0.6919999999999997 mean q 15.200712\n",
      "Episode:  308 Reward: 13.0 Epsilon 0.6909999999999997 mean q 16.402327\n",
      "Episode:  309 Reward: 21.0 Epsilon 0.6899999999999997 mean q 14.993965\n",
      "Episode:  310 Reward: 18.0 Epsilon 0.6889999999999997 mean q 14.140938\n",
      "Episode:  311 Reward: 30.0 Epsilon 0.6879999999999997 mean q 14.68571\n",
      "Episode:  312 Reward: 35.0 Epsilon 0.6869999999999997 mean q 14.279975\n",
      "Episode:  313 Reward: 9.0 Epsilon 0.6859999999999997 mean q 15.892824\n",
      "Episode:  314 Reward: 11.0 Epsilon 0.6849999999999997 mean q 13.521179\n",
      "Episode:  315 Reward: 16.0 Epsilon 0.6839999999999997 mean q 15.122759\n",
      "Episode:  316 Reward: 15.0 Epsilon 0.6829999999999997 mean q 14.475023\n",
      "Episode:  317 Reward: 27.0 Epsilon 0.6819999999999997 mean q 14.204543\n",
      "Episode:  318 Reward: 9.0 Epsilon 0.6809999999999997 mean q 14.912208\n",
      "Episode:  319 Reward: 18.0 Epsilon 0.6799999999999997 mean q 14.903115\n",
      "Episode:  320 Reward: 16.0 Epsilon 0.6789999999999997 mean q 14.712777\n",
      "Episode:  321 Reward: 22.0 Epsilon 0.6779999999999997 mean q 13.888305\n",
      "Episode:  322 Reward: 18.0 Epsilon 0.6769999999999997 mean q 13.694142\n",
      "Episode:  323 Reward: 29.0 Epsilon 0.6759999999999997 mean q 14.040064\n",
      "Episode:  324 Reward: 18.0 Epsilon 0.6749999999999997 mean q 13.746634\n",
      "Episode:  325 Reward: 10.0 Epsilon 0.6739999999999997 mean q 13.320486\n",
      "Episode:  326 Reward: 34.0 Epsilon 0.6729999999999997 mean q 13.9275465\n",
      "Episode:  327 Reward: 13.0 Epsilon 0.6719999999999997 mean q 13.506683\n",
      "Episode:  328 Reward: 15.0 Epsilon 0.6709999999999997 mean q 13.37699\n",
      "Episode:  329 Reward: 12.0 Epsilon 0.6699999999999997 mean q 12.902774\n",
      "Episode:  330 Reward: 18.0 Epsilon 0.6689999999999997 mean q 13.001733\n",
      "Episode:  331 Reward: 13.0 Epsilon 0.6679999999999997 mean q 13.763169\n",
      "Episode:  332 Reward: 10.0 Epsilon 0.6669999999999997 mean q 13.177622\n",
      "Episode:  333 Reward: 13.0 Epsilon 0.6659999999999997 mean q 13.110449\n",
      "Episode:  334 Reward: 10.0 Epsilon 0.6649999999999997 mean q 13.0541935\n",
      "Episode:  335 Reward: 13.0 Epsilon 0.6639999999999997 mean q 12.630378\n",
      "Episode:  336 Reward: 11.0 Epsilon 0.6629999999999997 mean q 12.990662\n",
      "Episode:  337 Reward: 12.0 Epsilon 0.6619999999999997 mean q 12.808665\n",
      "Episode:  338 Reward: 12.0 Epsilon 0.6609999999999997 mean q 12.599866\n",
      "Episode:  339 Reward: 12.0 Epsilon 0.6599999999999997 mean q 12.702081\n",
      "Episode:  340 Reward: 10.0 Epsilon 0.6589999999999997 mean q 13.0857115\n",
      "Episode:  341 Reward: 10.0 Epsilon 0.6579999999999997 mean q 12.870595\n",
      "Episode:  342 Reward: 15.0 Epsilon 0.6569999999999997 mean q 12.565957\n",
      "Episode:  343 Reward: 11.0 Epsilon 0.6559999999999997 mean q 12.665404\n",
      "Episode:  344 Reward: 17.0 Epsilon 0.6549999999999997 mean q 12.298611\n",
      "Episode:  345 Reward: 19.0 Epsilon 0.6539999999999997 mean q 12.069409\n",
      "Episode:  346 Reward: 8.0 Epsilon 0.6529999999999997 mean q 12.14185\n",
      "Episode:  347 Reward: 12.0 Epsilon 0.6519999999999997 mean q 12.853963\n",
      "Episode:  348 Reward: 10.0 Epsilon 0.6509999999999997 mean q 13.386032\n",
      "Episode:  349 Reward: 14.0 Epsilon 0.6499999999999997 mean q 12.235186\n",
      "Episode:  350 Reward: 14.0 Epsilon 0.6489999999999997 mean q 12.554814\n",
      "Episode:  351 Reward: 12.0 Epsilon 0.6479999999999997 mean q 13.08476\n",
      "Episode:  352 Reward: 11.0 Epsilon 0.6469999999999997 mean q 12.388064\n",
      "Episode:  353 Reward: 16.0 Epsilon 0.6459999999999997 mean q 12.387714\n",
      "Episode:  354 Reward: 12.0 Epsilon 0.6449999999999997 mean q 11.61456\n",
      "Episode:  355 Reward: 27.0 Epsilon 0.6439999999999997 mean q 13.375934\n",
      "Episode:  356 Reward: 19.0 Epsilon 0.6429999999999997 mean q 12.437543\n",
      "Episode:  357 Reward: 10.0 Epsilon 0.6419999999999997 mean q 12.498346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  358 Reward: 22.0 Epsilon 0.6409999999999997 mean q 12.267279\n",
      "Episode:  359 Reward: 19.0 Epsilon 0.6399999999999997 mean q 12.49559\n",
      "Episode:  360 Reward: 11.0 Epsilon 0.6389999999999997 mean q 12.572181\n",
      "Episode:  361 Reward: 18.0 Epsilon 0.6379999999999997 mean q 12.084649\n",
      "Episode:  362 Reward: 10.0 Epsilon 0.6369999999999997 mean q 11.058232\n",
      "Episode:  363 Reward: 13.0 Epsilon 0.6359999999999997 mean q 12.434223\n",
      "Episode:  364 Reward: 20.0 Epsilon 0.6349999999999997 mean q 12.479194\n",
      "Episode:  365 Reward: 8.0 Epsilon 0.6339999999999997 mean q 12.876963\n",
      "Episode:  366 Reward: 15.0 Epsilon 0.6329999999999997 mean q 10.971024\n",
      "Episode:  367 Reward: 15.0 Epsilon 0.6319999999999997 mean q 12.553222\n",
      "Episode:  368 Reward: 11.0 Epsilon 0.6309999999999997 mean q 12.876193\n",
      "Episode:  369 Reward: 17.0 Epsilon 0.6299999999999997 mean q 12.345984\n",
      "Episode:  370 Reward: 13.0 Epsilon 0.6289999999999997 mean q 12.608968\n",
      "Episode:  371 Reward: 13.0 Epsilon 0.6279999999999997 mean q 10.956219\n",
      "Episode:  372 Reward: 21.0 Epsilon 0.6269999999999997 mean q 12.490739\n",
      "Episode:  373 Reward: 31.0 Epsilon 0.6259999999999997 mean q 11.742911\n",
      "Episode:  374 Reward: 23.0 Epsilon 0.6249999999999997 mean q 12.122732\n",
      "Episode:  375 Reward: 20.0 Epsilon 0.6239999999999997 mean q 12.878189\n",
      "Episode:  376 Reward: 9.0 Epsilon 0.6229999999999997 mean q 13.964718\n",
      "Episode:  377 Reward: 13.0 Epsilon 0.6219999999999997 mean q 12.998208\n",
      "Episode:  378 Reward: 11.0 Epsilon 0.6209999999999997 mean q 12.994935\n",
      "Episode:  379 Reward: 13.0 Epsilon 0.6199999999999997 mean q 13.835122\n",
      "Episode:  380 Reward: 19.0 Epsilon 0.6189999999999997 mean q 13.045398\n",
      "Episode:  381 Reward: 10.0 Epsilon 0.6179999999999997 mean q 13.934537\n",
      "Episode:  382 Reward: 19.0 Epsilon 0.6169999999999997 mean q 13.540868\n",
      "Episode:  383 Reward: 14.0 Epsilon 0.6159999999999997 mean q 13.687279\n",
      "Episode:  384 Reward: 16.0 Epsilon 0.6149999999999997 mean q 13.030252\n",
      "Episode:  385 Reward: 22.0 Epsilon 0.6139999999999997 mean q 13.062552\n",
      "Episode:  386 Reward: 15.0 Epsilon 0.6129999999999997 mean q 13.043038\n",
      "Episode:  387 Reward: 16.0 Epsilon 0.6119999999999997 mean q 13.880029\n",
      "Episode:  388 Reward: 13.0 Epsilon 0.6109999999999997 mean q 13.754835\n",
      "Episode:  389 Reward: 10.0 Epsilon 0.6099999999999997 mean q 14.444626\n",
      "Episode:  390 Reward: 22.0 Epsilon 0.6089999999999997 mean q 13.416123\n",
      "Episode:  391 Reward: 17.0 Epsilon 0.6079999999999997 mean q 13.663678\n",
      "Episode:  392 Reward: 15.0 Epsilon 0.6069999999999997 mean q 13.556844\n",
      "Episode:  393 Reward: 12.0 Epsilon 0.6059999999999997 mean q 14.119256\n",
      "Episode:  394 Reward: 15.0 Epsilon 0.6049999999999996 mean q 13.177244\n",
      "Episode:  395 Reward: 31.0 Epsilon 0.6039999999999996 mean q 13.419328\n",
      "Episode:  396 Reward: 15.0 Epsilon 0.6029999999999996 mean q 13.134542\n",
      "Episode:  397 Reward: 10.0 Epsilon 0.6019999999999996 mean q 14.210672\n",
      "Episode:  398 Reward: 12.0 Epsilon 0.6009999999999996 mean q 14.008485\n",
      "Episode:  399 Reward: 10.0 Epsilon 0.5999999999999996 mean q 14.5662\n",
      "Episode:  400 Reward: 13.0 Epsilon 0.5989999999999996 mean q 13.619005\n",
      "Episode:  401 Reward: 10.0 Epsilon 0.5979999999999996 mean q 13.785425\n",
      "Episode:  402 Reward: 15.0 Epsilon 0.5969999999999996 mean q 13.730607\n",
      "Episode:  403 Reward: 11.0 Epsilon 0.5959999999999996 mean q 13.63644\n",
      "Episode:  404 Reward: 9.0 Epsilon 0.5949999999999996 mean q 13.871296\n",
      "Episode:  405 Reward: 17.0 Epsilon 0.5939999999999996 mean q 13.725608\n",
      "Episode:  406 Reward: 14.0 Epsilon 0.5929999999999996 mean q 14.037393\n",
      "Episode:  407 Reward: 33.0 Epsilon 0.5919999999999996 mean q 13.276478\n",
      "Episode:  408 Reward: 13.0 Epsilon 0.5909999999999996 mean q 13.851393\n",
      "Episode:  409 Reward: 14.0 Epsilon 0.5899999999999996 mean q 13.357824\n",
      "Episode:  410 Reward: 13.0 Epsilon 0.5889999999999996 mean q 13.839664\n",
      "Episode:  411 Reward: 21.0 Epsilon 0.5879999999999996 mean q 12.224193\n",
      "Episode:  412 Reward: 20.0 Epsilon 0.5869999999999996 mean q 13.798033\n",
      "Episode:  413 Reward: 32.0 Epsilon 0.5859999999999996 mean q 13.613888\n",
      "Episode:  414 Reward: 9.0 Epsilon 0.5849999999999996 mean q 13.915661\n",
      "Episode:  415 Reward: 8.0 Epsilon 0.5839999999999996 mean q 14.193356\n",
      "Episode:  416 Reward: 9.0 Epsilon 0.5829999999999996 mean q 14.191656\n",
      "Episode:  417 Reward: 22.0 Epsilon 0.5819999999999996 mean q 13.6953\n",
      "Episode:  418 Reward: 37.0 Epsilon 0.5809999999999996 mean q 12.8485155\n",
      "Episode:  419 Reward: 17.0 Epsilon 0.5799999999999996 mean q 13.338595\n",
      "Episode:  420 Reward: 50.0 Epsilon 0.5789999999999996 mean q 13.2629385\n",
      "Episode:  421 Reward: 18.0 Epsilon 0.5779999999999996 mean q 13.527791\n",
      "Episode:  422 Reward: 26.0 Epsilon 0.5769999999999996 mean q 13.411367\n",
      "Episode:  423 Reward: 13.0 Epsilon 0.5759999999999996 mean q 13.4267435\n",
      "Episode:  424 Reward: 10.0 Epsilon 0.5749999999999996 mean q 13.301356\n",
      "Episode:  425 Reward: 12.0 Epsilon 0.5739999999999996 mean q 13.368825\n",
      "Episode:  426 Reward: 10.0 Epsilon 0.5729999999999996 mean q 13.218666\n",
      "Episode:  427 Reward: 12.0 Epsilon 0.5719999999999996 mean q 13.31869\n",
      "Episode:  428 Reward: 11.0 Epsilon 0.5709999999999996 mean q 13.19931\n",
      "Episode:  429 Reward: 12.0 Epsilon 0.5699999999999996 mean q 13.240074\n",
      "Episode:  430 Reward: 14.0 Epsilon 0.5689999999999996 mean q 13.218383\n",
      "Episode:  431 Reward: 24.0 Epsilon 0.5679999999999996 mean q 11.871898\n",
      "Episode:  432 Reward: 12.0 Epsilon 0.5669999999999996 mean q 12.8450365\n",
      "Episode:  433 Reward: 19.0 Epsilon 0.5659999999999996 mean q 12.94138\n",
      "Episode:  434 Reward: 13.0 Epsilon 0.5649999999999996 mean q 12.90576\n",
      "Episode:  435 Reward: 13.0 Epsilon 0.5639999999999996 mean q 12.415585\n",
      "Episode:  436 Reward: 9.0 Epsilon 0.5629999999999996 mean q 12.488967\n",
      "Episode:  437 Reward: 22.0 Epsilon 0.5619999999999996 mean q 12.470486\n",
      "Episode:  438 Reward: 14.0 Epsilon 0.5609999999999996 mean q 12.49517\n",
      "Episode:  439 Reward: 13.0 Epsilon 0.5599999999999996 mean q 12.632415\n",
      "Episode:  440 Reward: 13.0 Epsilon 0.5589999999999996 mean q 12.775809\n",
      "Episode:  441 Reward: 11.0 Epsilon 0.5579999999999996 mean q 12.626856\n",
      "Episode:  442 Reward: 16.0 Epsilon 0.5569999999999996 mean q 12.378383\n",
      "Episode:  443 Reward: 9.0 Epsilon 0.5559999999999996 mean q 12.476978\n",
      "Episode:  444 Reward: 30.0 Epsilon 0.5549999999999996 mean q 12.387589\n",
      "Episode:  445 Reward: 22.0 Epsilon 0.5539999999999996 mean q 12.442681\n",
      "Episode:  446 Reward: 9.0 Epsilon 0.5529999999999996 mean q 12.100628\n",
      "Episode:  447 Reward: 10.0 Epsilon 0.5519999999999996 mean q 12.004602\n",
      "Episode:  448 Reward: 12.0 Epsilon 0.5509999999999996 mean q 11.931979\n",
      "Episode:  449 Reward: 25.0 Epsilon 0.5499999999999996 mean q 11.614524\n",
      "Episode:  450 Reward: 19.0 Epsilon 0.5489999999999996 mean q 12.039371\n",
      "Episode:  451 Reward: 13.0 Epsilon 0.5479999999999996 mean q 11.758146\n",
      "Episode:  452 Reward: 16.0 Epsilon 0.5469999999999996 mean q 11.856707\n",
      "Episode:  453 Reward: 9.0 Epsilon 0.5459999999999996 mean q 11.579648\n",
      "Episode:  454 Reward: 30.0 Epsilon 0.5449999999999996 mean q 11.996367\n",
      "Episode:  455 Reward: 11.0 Epsilon 0.5439999999999996 mean q 11.289028\n",
      "Episode:  456 Reward: 8.0 Epsilon 0.5429999999999996 mean q 11.072176\n",
      "Episode:  457 Reward: 13.0 Epsilon 0.5419999999999996 mean q 11.200916\n",
      "Episode:  458 Reward: 21.0 Epsilon 0.5409999999999996 mean q 11.4818535\n",
      "Episode:  459 Reward: 20.0 Epsilon 0.5399999999999996 mean q 11.485971\n",
      "Episode:  460 Reward: 10.0 Epsilon 0.5389999999999996 mean q 10.7119\n",
      "Episode:  461 Reward: 11.0 Epsilon 0.5379999999999996 mean q 11.013919\n",
      "Episode:  462 Reward: 9.0 Epsilon 0.5369999999999996 mean q 10.8414955\n",
      "Episode:  463 Reward: 12.0 Epsilon 0.5359999999999996 mean q 10.782514\n",
      "Episode:  464 Reward: 13.0 Epsilon 0.5349999999999996 mean q 10.960821\n",
      "Episode:  465 Reward: 37.0 Epsilon 0.5339999999999996 mean q 11.42755\n",
      "Episode:  466 Reward: 14.0 Epsilon 0.5329999999999996 mean q 10.682207\n",
      "Episode:  467 Reward: 11.0 Epsilon 0.5319999999999996 mean q 10.4492655\n",
      "Episode:  468 Reward: 12.0 Epsilon 0.5309999999999996 mean q 10.275189\n",
      "Episode:  469 Reward: 9.0 Epsilon 0.5299999999999996 mean q 9.850285\n",
      "Episode:  470 Reward: 21.0 Epsilon 0.5289999999999996 mean q 10.619176\n",
      "Episode:  471 Reward: 11.0 Epsilon 0.5279999999999996 mean q 9.630261\n",
      "Episode:  472 Reward: 14.0 Epsilon 0.5269999999999996 mean q 9.845312\n",
      "Episode:  473 Reward: 16.0 Epsilon 0.5259999999999996 mean q 10.53833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  474 Reward: 15.0 Epsilon 0.5249999999999996 mean q 9.827477\n",
      "Episode:  475 Reward: 14.0 Epsilon 0.5239999999999996 mean q 9.763335\n",
      "Episode:  476 Reward: 25.0 Epsilon 0.5229999999999996 mean q 10.039881\n",
      "Episode:  477 Reward: 24.0 Epsilon 0.5219999999999996 mean q 9.812455\n",
      "Episode:  478 Reward: 27.0 Epsilon 0.5209999999999996 mean q 9.8574295\n",
      "Episode:  479 Reward: 54.0 Epsilon 0.5199999999999996 mean q 10.357519\n",
      "Episode:  480 Reward: 24.0 Epsilon 0.5189999999999996 mean q 9.823059\n",
      "Episode:  481 Reward: 19.0 Epsilon 0.5179999999999996 mean q 9.703635\n",
      "Episode:  482 Reward: 20.0 Epsilon 0.5169999999999996 mean q 9.464957\n",
      "Episode:  483 Reward: 27.0 Epsilon 0.5159999999999996 mean q 9.681524\n",
      "Episode:  484 Reward: 49.0 Epsilon 0.5149999999999996 mean q 9.34176\n",
      "Episode:  485 Reward: 23.0 Epsilon 0.5139999999999996 mean q 9.651411\n",
      "Episode:  486 Reward: 46.0 Epsilon 0.5129999999999996 mean q 9.807928\n",
      "Episode:  487 Reward: 20.0 Epsilon 0.5119999999999996 mean q 9.283911\n",
      "Episode:  488 Reward: 44.0 Epsilon 0.5109999999999996 mean q 9.772904\n",
      "Episode:  489 Reward: 104.0 Epsilon 0.5099999999999996 mean q 10.559378\n",
      "Episode:  490 Reward: 46.0 Epsilon 0.5089999999999996 mean q 10.963746\n",
      "Episode:  491 Reward: 54.0 Epsilon 0.5079999999999996 mean q 10.044097\n",
      "Episode:  492 Reward: 61.0 Epsilon 0.5069999999999996 mean q 10.799705\n",
      "Episode:  493 Reward: 70.0 Epsilon 0.5059999999999996 mean q 11.004708\n",
      "Episode:  494 Reward: 51.0 Epsilon 0.5049999999999996 mean q 10.57744\n",
      "Episode:  495 Reward: 79.0 Epsilon 0.5039999999999996 mean q 10.434161\n",
      "Episode:  496 Reward: 42.0 Epsilon 0.5029999999999996 mean q 11.569081\n",
      "Episode:  497 Reward: 73.0 Epsilon 0.5019999999999996 mean q 11.840891\n",
      "Episode:  498 Reward: 46.0 Epsilon 0.5009999999999996 mean q 11.800985\n",
      "Episode:  499 Reward: 100.0 Epsilon 0.49999999999999956 mean q 11.651848\n",
      "Episode:  500 Reward: 61.0 Epsilon 0.49899999999999956 mean q 11.190574\n",
      "Episode:  501 Reward: 30.0 Epsilon 0.49799999999999955 mean q 11.514256\n",
      "Episode:  502 Reward: 62.0 Epsilon 0.49699999999999955 mean q 11.652194\n",
      "Episode:  503 Reward: 32.0 Epsilon 0.49599999999999955 mean q 11.571802\n",
      "Episode:  504 Reward: 55.0 Epsilon 0.49499999999999955 mean q 11.695603\n",
      "Episode:  505 Reward: 49.0 Epsilon 0.49399999999999955 mean q 11.691573\n",
      "Episode:  506 Reward: 65.0 Epsilon 0.49299999999999955 mean q 12.563788\n",
      "Episode:  507 Reward: 41.0 Epsilon 0.49199999999999955 mean q 11.871933\n",
      "Episode:  508 Reward: 44.0 Epsilon 0.49099999999999955 mean q 11.997811\n",
      "Episode:  509 Reward: 50.0 Epsilon 0.48999999999999955 mean q 12.476292\n",
      "Episode:  510 Reward: 40.0 Epsilon 0.48899999999999955 mean q 12.098978\n",
      "Episode:  511 Reward: 48.0 Epsilon 0.48799999999999955 mean q 13.796247\n",
      "Episode:  512 Reward: 27.0 Epsilon 0.48699999999999954 mean q 12.229394\n",
      "Episode:  513 Reward: 54.0 Epsilon 0.48599999999999954 mean q 14.112568\n",
      "Episode:  514 Reward: 30.0 Epsilon 0.48499999999999954 mean q 12.427856\n",
      "Episode:  515 Reward: 32.0 Epsilon 0.48399999999999954 mean q 12.503279\n",
      "Episode:  516 Reward: 56.0 Epsilon 0.48299999999999954 mean q 12.978689\n",
      "Episode:  517 Reward: 42.0 Epsilon 0.48199999999999954 mean q 12.801939\n",
      "Episode:  518 Reward: 43.0 Epsilon 0.48099999999999954 mean q 13.082215\n",
      "Episode:  519 Reward: 49.0 Epsilon 0.47999999999999954 mean q 12.476179\n",
      "Episode:  520 Reward: 143.0 Epsilon 0.47899999999999954 mean q 13.856778\n",
      "Episode:  521 Reward: 40.0 Epsilon 0.47799999999999954 mean q 12.981076\n",
      "Episode:  522 Reward: 67.0 Epsilon 0.47699999999999954 mean q 13.347382\n",
      "Episode:  523 Reward: 33.0 Epsilon 0.47599999999999953 mean q 13.615459\n",
      "Episode:  524 Reward: 12.0 Epsilon 0.47499999999999953 mean q 11.994775\n",
      "Episode:  525 Reward: 54.0 Epsilon 0.47399999999999953 mean q 14.023273\n",
      "Episode:  526 Reward: 39.0 Epsilon 0.47299999999999953 mean q 13.426091\n",
      "Episode:  527 Reward: 36.0 Epsilon 0.47199999999999953 mean q 13.644874\n",
      "Episode:  528 Reward: 63.0 Epsilon 0.47099999999999953 mean q 14.060807\n",
      "Episode:  529 Reward: 53.0 Epsilon 0.46999999999999953 mean q 13.664887\n",
      "Episode:  530 Reward: 41.0 Epsilon 0.46899999999999953 mean q 13.391207\n",
      "Episode:  531 Reward: 73.0 Epsilon 0.4679999999999995 mean q 15.952654\n",
      "Episode:  532 Reward: 30.0 Epsilon 0.4669999999999995 mean q 13.525511\n",
      "Episode:  533 Reward: 41.0 Epsilon 0.4659999999999995 mean q 14.701285\n",
      "Episode:  534 Reward: 43.0 Epsilon 0.4649999999999995 mean q 13.583112\n",
      "Episode:  535 Reward: 71.0 Epsilon 0.4639999999999995 mean q 14.1049595\n",
      "Episode:  536 Reward: 95.0 Epsilon 0.4629999999999995 mean q 16.37383\n",
      "Episode:  537 Reward: 135.0 Epsilon 0.4619999999999995 mean q 14.712485\n",
      "Episode:  538 Reward: 62.0 Epsilon 0.4609999999999995 mean q 14.314362\n",
      "Episode:  539 Reward: 45.0 Epsilon 0.4599999999999995 mean q 13.894728\n",
      "Episode:  540 Reward: 43.0 Epsilon 0.4589999999999995 mean q 14.559962\n",
      "Episode:  541 Reward: 36.0 Epsilon 0.4579999999999995 mean q 14.606273\n",
      "Episode:  542 Reward: 87.0 Epsilon 0.4569999999999995 mean q 14.793687\n",
      "Episode:  543 Reward: 47.0 Epsilon 0.4559999999999995 mean q 14.617188\n",
      "Episode:  544 Reward: 40.0 Epsilon 0.4549999999999995 mean q 14.835012\n",
      "Episode:  545 Reward: 50.0 Epsilon 0.4539999999999995 mean q 13.862283\n",
      "Episode:  546 Reward: 19.0 Epsilon 0.4529999999999995 mean q 13.602168\n",
      "Episode:  547 Reward: 20.0 Epsilon 0.4519999999999995 mean q 14.088847\n",
      "Episode:  548 Reward: 71.0 Epsilon 0.4509999999999995 mean q 14.442766\n",
      "Episode:  549 Reward: 91.0 Epsilon 0.4499999999999995 mean q 14.595573\n",
      "Episode:  550 Reward: 49.0 Epsilon 0.4489999999999995 mean q 14.562084\n",
      "Episode:  551 Reward: 91.0 Epsilon 0.4479999999999995 mean q 16.594593\n",
      "Episode:  552 Reward: 70.0 Epsilon 0.4469999999999995 mean q 17.038837\n",
      "Episode:  553 Reward: 41.0 Epsilon 0.4459999999999995 mean q 15.0462055\n",
      "Episode:  554 Reward: 75.0 Epsilon 0.4449999999999995 mean q 14.690432\n",
      "Episode:  555 Reward: 98.0 Epsilon 0.4439999999999995 mean q 17.269302\n",
      "Episode:  556 Reward: 50.0 Epsilon 0.4429999999999995 mean q 14.818088\n",
      "Episode:  557 Reward: 49.0 Epsilon 0.4419999999999995 mean q 14.399659\n",
      "Episode:  558 Reward: 55.0 Epsilon 0.4409999999999995 mean q 14.308269\n",
      "Episode:  559 Reward: 70.0 Epsilon 0.4399999999999995 mean q 14.45851\n",
      "Episode:  560 Reward: 21.0 Epsilon 0.4389999999999995 mean q 14.96241\n",
      "Episode:  561 Reward: 85.0 Epsilon 0.4379999999999995 mean q 15.134223\n",
      "Episode:  562 Reward: 74.0 Epsilon 0.4369999999999995 mean q 16.807775\n",
      "Episode:  563 Reward: 129.0 Epsilon 0.4359999999999995 mean q 16.750496\n",
      "Episode:  564 Reward: 88.0 Epsilon 0.4349999999999995 mean q 17.430847\n",
      "Episode:  565 Reward: 79.0 Epsilon 0.4339999999999995 mean q 17.881615\n",
      "Episode:  566 Reward: 65.0 Epsilon 0.4329999999999995 mean q 14.772302\n",
      "Episode:  567 Reward: 60.0 Epsilon 0.4319999999999995 mean q 14.741085\n",
      "Episode:  568 Reward: 96.0 Epsilon 0.4309999999999995 mean q 14.79095\n",
      "Episode:  569 Reward: 36.0 Epsilon 0.4299999999999995 mean q 16.280529\n",
      "Episode:  570 Reward: 41.0 Epsilon 0.4289999999999995 mean q 16.894005\n",
      "Episode:  571 Reward: 74.0 Epsilon 0.4279999999999995 mean q 16.758684\n",
      "Episode:  572 Reward: 40.0 Epsilon 0.4269999999999995 mean q 14.987005\n",
      "Episode:  573 Reward: 57.0 Epsilon 0.4259999999999995 mean q 14.5556555\n",
      "Episode:  574 Reward: 53.0 Epsilon 0.4249999999999995 mean q 16.701889\n",
      "Episode:  575 Reward: 99.0 Epsilon 0.4239999999999995 mean q 15.144326\n",
      "Episode:  576 Reward: 64.0 Epsilon 0.4229999999999995 mean q 14.901861\n",
      "Episode:  577 Reward: 97.0 Epsilon 0.4219999999999995 mean q 14.397584\n",
      "Episode:  578 Reward: 36.0 Epsilon 0.4209999999999995 mean q 16.685247\n",
      "Episode:  579 Reward: 33.0 Epsilon 0.4199999999999995 mean q 14.5465355\n",
      "Episode:  580 Reward: 93.0 Epsilon 0.4189999999999995 mean q 16.6935\n",
      "Episode:  581 Reward: 118.0 Epsilon 0.4179999999999995 mean q 16.448298\n",
      "Episode:  582 Reward: 79.0 Epsilon 0.4169999999999995 mean q 15.1927185\n",
      "Episode:  583 Reward: 59.0 Epsilon 0.4159999999999995 mean q 14.779203\n",
      "Episode:  584 Reward: 45.0 Epsilon 0.4149999999999995 mean q 15.155607\n",
      "Episode:  585 Reward: 88.0 Epsilon 0.4139999999999995 mean q 14.828061\n",
      "Episode:  586 Reward: 134.0 Epsilon 0.4129999999999995 mean q 15.506191\n",
      "Episode:  587 Reward: 80.0 Epsilon 0.4119999999999995 mean q 15.406311\n",
      "Episode:  588 Reward: 122.0 Epsilon 0.4109999999999995 mean q 15.999604\n",
      "Episode:  589 Reward: 57.0 Epsilon 0.4099999999999995 mean q 17.778818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  590 Reward: 44.0 Epsilon 0.4089999999999995 mean q 15.088616\n",
      "Episode:  591 Reward: 140.0 Epsilon 0.4079999999999995 mean q 16.80106\n",
      "Episode:  592 Reward: 60.0 Epsilon 0.4069999999999995 mean q 14.865376\n",
      "Episode:  593 Reward: 48.0 Epsilon 0.4059999999999995 mean q 14.40612\n",
      "Episode:  594 Reward: 68.0 Epsilon 0.40499999999999947 mean q 15.436211\n",
      "Episode:  595 Reward: 65.0 Epsilon 0.40399999999999947 mean q 15.1934595\n",
      "Episode:  596 Reward: 66.0 Epsilon 0.40299999999999947 mean q 14.877716\n",
      "Episode:  597 Reward: 140.0 Epsilon 0.40199999999999947 mean q 15.06889\n",
      "Episode:  598 Reward: 117.0 Epsilon 0.40099999999999947 mean q 17.633242\n",
      "Episode:  599 Reward: 55.0 Epsilon 0.39999999999999947 mean q 14.460719\n",
      "Episode:  600 Reward: 59.0 Epsilon 0.39899999999999947 mean q 16.942257\n",
      "Episode:  601 Reward: 93.0 Epsilon 0.39799999999999947 mean q 17.326132\n",
      "Episode:  602 Reward: 102.0 Epsilon 0.39699999999999946 mean q 18.458323\n",
      "Episode:  603 Reward: 55.0 Epsilon 0.39599999999999946 mean q 14.428996\n",
      "Episode:  604 Reward: 80.0 Epsilon 0.39499999999999946 mean q 17.585878\n",
      "Episode:  605 Reward: 86.0 Epsilon 0.39399999999999946 mean q 17.093327\n",
      "Episode:  606 Reward: 103.0 Epsilon 0.39299999999999946 mean q 17.593384\n",
      "Episode:  607 Reward: 103.0 Epsilon 0.39199999999999946 mean q 15.63038\n",
      "Episode:  608 Reward: 63.0 Epsilon 0.39099999999999946 mean q 17.941282\n",
      "Episode:  609 Reward: 91.0 Epsilon 0.38999999999999946 mean q 15.67677\n",
      "Episode:  610 Reward: 91.0 Epsilon 0.38899999999999946 mean q 17.526564\n",
      "Episode:  611 Reward: 125.0 Epsilon 0.38799999999999946 mean q 17.48217\n",
      "Episode:  612 Reward: 132.0 Epsilon 0.38699999999999946 mean q 15.308907\n",
      "Episode:  613 Reward: 58.0 Epsilon 0.38599999999999945 mean q 17.769737\n",
      "Episode:  614 Reward: 99.0 Epsilon 0.38499999999999945 mean q 15.513756\n",
      "Episode:  615 Reward: 37.0 Epsilon 0.38399999999999945 mean q 15.66656\n",
      "Episode:  616 Reward: 79.0 Epsilon 0.38299999999999945 mean q 16.005615\n",
      "Episode:  617 Reward: 34.0 Epsilon 0.38199999999999945 mean q 16.271866\n",
      "Episode:  618 Reward: 88.0 Epsilon 0.38099999999999945 mean q 17.559174\n",
      "Episode:  619 Reward: 72.0 Epsilon 0.37999999999999945 mean q 15.670702\n",
      "Episode:  620 Reward: 109.0 Epsilon 0.37899999999999945 mean q 16.996632\n",
      "Episode:  621 Reward: 48.0 Epsilon 0.37799999999999945 mean q 16.164574\n",
      "Episode:  622 Reward: 72.0 Epsilon 0.37699999999999945 mean q 17.37677\n",
      "Episode:  623 Reward: 110.0 Epsilon 0.37599999999999945 mean q 15.058678\n",
      "Episode:  624 Reward: 95.0 Epsilon 0.37499999999999944 mean q 14.862798\n",
      "Episode:  625 Reward: 122.0 Epsilon 0.37399999999999944 mean q 17.305616\n",
      "Episode:  626 Reward: 51.0 Epsilon 0.37299999999999944 mean q 16.196705\n",
      "Episode:  627 Reward: 54.0 Epsilon 0.37199999999999944 mean q 15.646932\n",
      "Episode:  628 Reward: 35.0 Epsilon 0.37099999999999944 mean q 14.365111\n",
      "Episode:  629 Reward: 104.0 Epsilon 0.36999999999999944 mean q 15.231421\n",
      "Episode:  630 Reward: 137.0 Epsilon 0.36899999999999944 mean q 15.339965\n",
      "Episode:  631 Reward: 38.0 Epsilon 0.36799999999999944 mean q 14.817216\n",
      "Episode:  632 Reward: 168.0 Epsilon 0.36699999999999944 mean q 17.263367\n",
      "Episode:  633 Reward: 111.0 Epsilon 0.36599999999999944 mean q 16.993078\n",
      "Episode:  634 Reward: 86.0 Epsilon 0.36499999999999944 mean q 17.696169\n",
      "Episode:  635 Reward: 93.0 Epsilon 0.36399999999999944 mean q 18.44545\n",
      "Episode:  636 Reward: 57.0 Epsilon 0.36299999999999943 mean q 17.833183\n",
      "Episode:  637 Reward: 200.0 Epsilon 0.36199999999999943 mean q 16.119022\n",
      "Episode:  638 Reward: 148.0 Epsilon 0.36099999999999943 mean q 15.378547\n",
      "Episode:  639 Reward: 36.0 Epsilon 0.35999999999999943 mean q 15.622996\n",
      "Episode:  640 Reward: 149.0 Epsilon 0.35899999999999943 mean q 15.490497\n",
      "Episode:  641 Reward: 92.0 Epsilon 0.35799999999999943 mean q 17.644533\n",
      "Episode:  642 Reward: 47.0 Epsilon 0.35699999999999943 mean q 17.140362\n",
      "Episode:  643 Reward: 113.0 Epsilon 0.35599999999999943 mean q 15.592499\n",
      "Episode:  644 Reward: 102.0 Epsilon 0.3549999999999994 mean q 16.23775\n",
      "Episode:  645 Reward: 200.0 Epsilon 0.3539999999999994 mean q 16.960318\n",
      "Episode:  646 Reward: 200.0 Epsilon 0.3529999999999994 mean q 16.462648\n",
      "Episode:  647 Reward: 108.0 Epsilon 0.3519999999999994 mean q 15.923489\n",
      "Episode:  648 Reward: 125.0 Epsilon 0.3509999999999994 mean q 16.544888\n",
      "Episode:  649 Reward: 82.0 Epsilon 0.3499999999999994 mean q 16.538557\n",
      "Episode:  650 Reward: 138.0 Epsilon 0.3489999999999994 mean q 16.863289\n",
      "Episode:  651 Reward: 159.0 Epsilon 0.3479999999999994 mean q 16.048492\n",
      "Episode:  652 Reward: 200.0 Epsilon 0.3469999999999994 mean q 16.017448\n",
      "Episode:  653 Reward: 106.0 Epsilon 0.3459999999999994 mean q 15.757768\n",
      "Episode:  654 Reward: 150.0 Epsilon 0.3449999999999994 mean q 17.84214\n",
      "Episode:  655 Reward: 200.0 Epsilon 0.3439999999999994 mean q 15.977063\n",
      "Episode:  656 Reward: 200.0 Epsilon 0.3429999999999994 mean q 17.260967\n",
      "Episode:  657 Reward: 150.0 Epsilon 0.3419999999999994 mean q 16.562239\n",
      "Episode:  658 Reward: 156.0 Epsilon 0.3409999999999994 mean q 16.442509\n",
      "Episode:  659 Reward: 126.0 Epsilon 0.3399999999999994 mean q 15.784403\n",
      "Episode:  660 Reward: 141.0 Epsilon 0.3389999999999994 mean q 16.488934\n",
      "Episode:  661 Reward: 147.0 Epsilon 0.3379999999999994 mean q 15.509961\n",
      "Episode:  662 Reward: 189.0 Epsilon 0.3369999999999994 mean q 17.40915\n",
      "Episode:  663 Reward: 136.0 Epsilon 0.3359999999999994 mean q 16.575089\n",
      "Episode:  664 Reward: 196.0 Epsilon 0.3349999999999994 mean q 16.84338\n",
      "Episode:  665 Reward: 200.0 Epsilon 0.3339999999999994 mean q 18.990143\n",
      "Episode:  666 Reward: 180.0 Epsilon 0.3329999999999994 mean q 15.991573\n",
      "Episode:  667 Reward: 200.0 Epsilon 0.3319999999999994 mean q 18.489006\n",
      "Episode:  668 Reward: 200.0 Epsilon 0.3309999999999994 mean q 17.56222\n",
      "Episode:  669 Reward: 152.0 Epsilon 0.3299999999999994 mean q 15.650073\n",
      "Episode:  670 Reward: 200.0 Epsilon 0.3289999999999994 mean q 16.844458\n",
      "Episode:  671 Reward: 200.0 Epsilon 0.3279999999999994 mean q 16.564547\n",
      "Episode:  672 Reward: 147.0 Epsilon 0.3269999999999994 mean q 16.576548\n",
      "Episode:  673 Reward: 65.0 Epsilon 0.3259999999999994 mean q 17.174065\n",
      "Episode:  674 Reward: 188.0 Epsilon 0.3249999999999994 mean q 17.214464\n",
      "Episode:  675 Reward: 106.0 Epsilon 0.3239999999999994 mean q 16.939325\n",
      "Episode:  676 Reward: 200.0 Epsilon 0.3229999999999994 mean q 17.885565\n",
      "Episode:  677 Reward: 200.0 Epsilon 0.3219999999999994 mean q 15.402209\n",
      "Episode:  678 Reward: 200.0 Epsilon 0.3209999999999994 mean q 17.821836\n",
      "Episode:  679 Reward: 177.0 Epsilon 0.3199999999999994 mean q 14.884555\n",
      "Episode:  680 Reward: 200.0 Epsilon 0.3189999999999994 mean q 17.426538\n",
      "Episode:  681 Reward: 200.0 Epsilon 0.3179999999999994 mean q 18.27878\n",
      "Episode:  682 Reward: 200.0 Epsilon 0.3169999999999994 mean q 16.102379\n",
      "Episode:  683 Reward: 200.0 Epsilon 0.3159999999999994 mean q 15.253568\n",
      "Episode:  684 Reward: 200.0 Epsilon 0.3149999999999994 mean q 17.764997\n",
      "Episode:  685 Reward: 200.0 Epsilon 0.3139999999999994 mean q 15.222398\n",
      "Episode:  686 Reward: 200.0 Epsilon 0.3129999999999994 mean q 17.740728\n",
      "Episode:  687 Reward: 200.0 Epsilon 0.3119999999999994 mean q 17.726805\n",
      "Episode:  688 Reward: 200.0 Epsilon 0.3109999999999994 mean q 17.829893\n",
      "Episode:  689 Reward: 200.0 Epsilon 0.3099999999999994 mean q 16.329462\n",
      "Episode:  690 Reward: 200.0 Epsilon 0.3089999999999994 mean q 18.380247\n",
      "Episode:  691 Reward: 200.0 Epsilon 0.3079999999999994 mean q 17.749548\n",
      "Episode:  692 Reward: 200.0 Epsilon 0.3069999999999994 mean q 16.205786\n",
      "Episode:  693 Reward: 200.0 Epsilon 0.3059999999999994 mean q 18.720901\n",
      "Episode:  694 Reward: 200.0 Epsilon 0.3049999999999994 mean q 16.285538\n",
      "Episode:  695 Reward: 200.0 Epsilon 0.3039999999999994 mean q 17.177784\n",
      "Episode:  696 Reward: 172.0 Epsilon 0.3029999999999994 mean q 17.352144\n",
      "Episode:  697 Reward: 200.0 Epsilon 0.3019999999999994 mean q 15.688242\n",
      "Episode:  698 Reward: 200.0 Epsilon 0.3009999999999994 mean q 18.021744\n",
      "Episode:  699 Reward: 200.0 Epsilon 0.2999999999999994 mean q 18.606125\n",
      "Episode:  700 Reward: 200.0 Epsilon 0.2989999999999994 mean q 17.53202\n",
      "Episode:  701 Reward: 200.0 Epsilon 0.2979999999999994 mean q 18.716394\n",
      "Episode:  702 Reward: 200.0 Epsilon 0.2969999999999994 mean q 16.365232\n",
      "Episode:  703 Reward: 200.0 Epsilon 0.2959999999999994 mean q 18.029245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  704 Reward: 200.0 Epsilon 0.2949999999999994 mean q 18.710314\n",
      "Episode:  705 Reward: 200.0 Epsilon 0.2939999999999994 mean q 18.700607\n",
      "Episode:  706 Reward: 200.0 Epsilon 0.29299999999999937 mean q 16.642838\n",
      "Episode:  707 Reward: 200.0 Epsilon 0.29199999999999937 mean q 17.408192\n",
      "Episode:  708 Reward: 53.0 Epsilon 0.29099999999999937 mean q 17.668211\n",
      "Episode:  709 Reward: 200.0 Epsilon 0.28999999999999937 mean q 18.374018\n",
      "Episode:  710 Reward: 200.0 Epsilon 0.28899999999999937 mean q 18.778692\n",
      "Episode:  711 Reward: 200.0 Epsilon 0.28799999999999937 mean q 18.53022\n",
      "Episode:  712 Reward: 200.0 Epsilon 0.28699999999999937 mean q 18.814089\n",
      "Episode:  713 Reward: 200.0 Epsilon 0.28599999999999937 mean q 17.877216\n",
      "Episode:  714 Reward: 200.0 Epsilon 0.28499999999999936 mean q 19.021935\n",
      "Episode:  715 Reward: 200.0 Epsilon 0.28399999999999936 mean q 19.594763\n",
      "Episode:  716 Reward: 200.0 Epsilon 0.28299999999999936 mean q 18.800154\n",
      "Episode:  717 Reward: 200.0 Epsilon 0.28199999999999936 mean q 18.807138\n",
      "Episode:  718 Reward: 200.0 Epsilon 0.28099999999999936 mean q 19.060163\n",
      "Episode:  719 Reward: 200.0 Epsilon 0.27999999999999936 mean q 18.7774\n",
      "Episode:  720 Reward: 200.0 Epsilon 0.27899999999999936 mean q 18.956842\n",
      "Episode:  721 Reward: 200.0 Epsilon 0.27799999999999936 mean q 18.346573\n",
      "Episode:  722 Reward: 200.0 Epsilon 0.27699999999999936 mean q 19.03808\n",
      "Episode:  723 Reward: 200.0 Epsilon 0.27599999999999936 mean q 18.879845\n",
      "Episode:  724 Reward: 200.0 Epsilon 0.27499999999999936 mean q 19.253414\n",
      "Episode:  725 Reward: 200.0 Epsilon 0.27399999999999936 mean q 19.27658\n",
      "Episode:  726 Reward: 200.0 Epsilon 0.27299999999999935 mean q 18.088896\n",
      "Episode:  727 Reward: 200.0 Epsilon 0.27199999999999935 mean q 19.137754\n",
      "Episode:  728 Reward: 200.0 Epsilon 0.27099999999999935 mean q 18.94236\n",
      "Episode:  729 Reward: 200.0 Epsilon 0.26999999999999935 mean q 19.059046\n",
      "Episode:  730 Reward: 200.0 Epsilon 0.26899999999999935 mean q 19.159935\n",
      "Episode:  731 Reward: 200.0 Epsilon 0.26799999999999935 mean q 18.503775\n",
      "Episode:  732 Reward: 200.0 Epsilon 0.26699999999999935 mean q 19.10141\n",
      "Episode:  733 Reward: 200.0 Epsilon 0.26599999999999935 mean q 19.14373\n",
      "Episode:  734 Reward: 200.0 Epsilon 0.26499999999999935 mean q 19.00556\n"
     ]
    }
   ],
   "source": [
    "# Create the environment\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "# Initializations\n",
    "num_actions = env.action_space.n\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "\n",
    "# Our Neural Netork model used to estimate the Q-values\n",
    "model = DoubleQLearningModel(state_dim=obs_dim, action_dim=num_actions, learning_rate=1e-4)\n",
    "\n",
    "# Create replay buffer, where experience in form of tuples <s,a,r,s',t>, gathered from the environment is stored \n",
    "# for training\n",
    "replay_buffer = ExperienceReplay(state_size=obs_dim)\n",
    "\n",
    "# Train\n",
    "num_episodes = 1200 \n",
    "batch_size = 128 \n",
    "R, R_avg = train_loop_ddqn(model, env, num_episodes, batch_size) #num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ccfca43f389c569bcbffc990f47b4193",
     "grade": false,
     "grade_id": "cell-4757be1a3ec18b56",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# close window (if you used env.render())\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6c25e0e7133fd3583dcac959c1daede9",
     "grade": false,
     "grade_id": "cell-8f1ad36de733ed92",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "According to the code above, and the code in the provided .py file, answer the following questions:\n",
    "    \n",
    "What is the state for this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "2f28740b9ccf7bf2fa7f166c9fbde003",
     "grade": true,
     "grade_id": "cell-0a780f1afdcd6b1a",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "**Your answer:**\n",
    "There are 4 states found in [Gym](https://github.com/openai/gym) stated below:\n",
    "\n",
    "\n",
    "| Num | Observation          | Min      | Max     |\n",
    "|-----|----------------------|----------|---------|\n",
    "| 0   | Cart Position        | -2.4     | 2.4     |\n",
    "| 1   | Cart Velocity        | -Inf     | Inf     |\n",
    "| 2   | Pole Angle           | ~ -41.8 | ~ 41.8 |\n",
    "| 3   | Pole Velocity At Tip | -Inf     | Inf     |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d24bef4443fea27ffd50ac2afa03e864",
     "grade": false,
     "grade_id": "cell-50a080269bf6f296",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "When do we switch the networks (i.e. when does the online network become the fixed one, and vice-versa)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ec6fb2c4cdc4df93cd1669b0e4696410",
     "grade": true,
     "grade_id": "cell-099530ded38d7038",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** After at least 1000 samples there is a 50% changes of switching the networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f182ff445a4e8d3848403742faa6a1cd",
     "grade": false,
     "grade_id": "cell-0836fc1b783d1158",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Run the cell below to visualize your final policy in an episode from this environment.\n",
    "\n",
    "**Note:** In order to visualize, the env.render() command needs to work out on your system (see comment a few cells above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4ff19e83fbc6ba870e1b638fc7801f37",
     "grade": false,
     "grade_id": "cell-1e8a9b49909882ac",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "num_episodes = 1\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "for i in range(num_episodes):\n",
    "        state = env.reset() #reset to initial state\n",
    "        state = np.expand_dims(state, axis=0)/2\n",
    "        terminal = False # reset terminal flag\n",
    "        while not terminal:\n",
    "            env.render()\n",
    "            time.sleep(.05)\n",
    "            q_values = model.get_q_values(state)\n",
    "            policy = eps_greedy_policy(q_values.squeeze(), .1) # greedy policy\n",
    "            action = np.random.choice(num_actions, p=policy)\n",
    "            state, reward, terminal, _ = env.step(action) # take one step in the evironment\n",
    "            state = np.expand_dims(state, axis=0)/2\n",
    "# close window\n",
    "env.close();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "91c95cdee8f1715e789b8bdf2a4e2ff8",
     "grade": false,
     "grade_id": "cell-0bb5d237ca6839d6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Plot the episodic rewards obtained throughout the optimization, together with a moving average of it (since the episodic reward is usually very noisy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a49cda53e12c1b8a976338c0f8bff7b9",
     "grade": false,
     "grade_id": "cell-a3c72b1dbffd2db4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rewards = plt.plot(R, alpha=.4, label='R')\n",
    "avg_rewards = plt.plot(R_avg,label='avg R')\n",
    "plt.legend(bbox_to_anchor=(1.01, 1), loc=2, borderaxespad=0.)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylim(0, 210)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "99ff38a2a4ff5f958140c9ee6019db87",
     "grade": false,
     "grade_id": "cell-293ec5dfa636ff48",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Congratulations, you have now successfully implemented the DDQN algorithm. You are encouraged to explore different problems. There are a lot of different environments ready for you to implement your algorithms in. A few of these resources are:\n",
    "* [OpenAI gym](https://github.com/openai/gym)\n",
    "* [OpenAI Universe](https://github.com/openai/universe)\n",
    "* [DeepMind Lab](https://deepmind.com/blog/open-sourcing-deepmind-lab/)\n",
    "\n",
    "The model you implemented in this lab can be extended to solve harder problems. A good starting-point is to try to solve the Acrobot-problem, by loading the environment as \n",
    "\n",
    "**gym.make(\"Acrobot-v1\")**.\n",
    "\n",
    "The problem might require some modifications to how you decay $\\epsilon$, but otherwise, the code you have written within this lab should be sufficient. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6be3464a29fcec53d4c2434c8af73acd",
     "grade": false,
     "grade_id": "cell-671cfb5a590863e9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Task 3.2 Atari games\n",
    "\n",
    "A common benchmark for reinforcement learning algorithms is the old Atari games. For the Atari games, each observation consists of one screenshot of the current state of the game. Other than adding convolutional layers to your neural network, there is one more issue regarding the new input that needs to be solved. Name at least two solutions to the problem, and why it won't work without these changes. \n",
    "\n",
    "Hint:\n",
    "- Imagine the game of pong. What is important for the algorithm to predict? What is the input to the algorithm? Is it possible to predict what we want from the input given?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "8c57f74e2e49c6ee244370df1b46e5a2",
     "grade": true,
     "grade_id": "cell-55e109dd6169612b",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** Since each observation consists of only one screenshot of the current state, it is not possible for the agent to figure out what the velocity (speed/angle) and acceleration of the ball. The agent is thus unable to decide on a reasonable action based on the one screenshot alone. \n",
    "\n",
    "The first solution to solve this is to give the algorithm the current screenshot as well as the screenshot from the previous timestep. This will give the algorithm the possibility to calculate the velocity. The screenshots can simply be concatenated or merged together. By adding a third image so that the algorithm can see 3 consecutive screenshots, the algorithm can then also estimate the acceleration. \n",
    "\n",
    "The second solution is to also give the algorithm an optical flow filtered image in addition to the current screenshot. The optical flow filter contains information about the changes in the current screenshot from the previous screenshot, thus allowing the algorithm to use these information as well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
