{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from __future__ import print_function, division\n",
    "from random import randrange\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.datasets import fashion_mnist, cifar10, mnist\n",
    "from keras.layers import Input, Dense, Flatten, Dropout, Reshape, Concatenate, MaxPooling2D\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D, UpSampling2D, Conv2D, Conv2DTranspose\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam, SGD\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\"> EMNIST </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data \n",
    "# Note that both the uppercase and lowercase variants of a single letter belong to the same class\n",
    "data = pd.read_csv(\"data/emnist-letters.csv\", header=None)\n",
    "ys_real = data.iloc[:,0].values - 1\n",
    "Xs_real = data.iloc[:,1:].values\n",
    "\n",
    "# Some constants and parameters\n",
    "N_SAMPLES = data.shape[0]\n",
    "N_CLASSES = len(np.unique(ys_real))\n",
    "IMAGE_SIZE = 28\n",
    "\n",
    "# Normalize pixels' values between -1 and 1, and reshape the images\n",
    "Xs_real = -1 + 2 * (Xs_real / np.max(Xs_real))\n",
    "Xs_real = Xs_real.reshape(N_SAMPLES, IMAGE_SIZE, IMAGE_SIZE)\n",
    "for i in range(N_SAMPLES):\n",
    "    Xs_real[i] = Xs_real[i].T\n",
    "    \n",
    "# Create a map from labels to their corresponding English letters\n",
    "letters = [chr(i) for i in range(65, 91)]\n",
    "letters_map = {index: letter for (index, letter) in enumerate(letters)}\n",
    "print(letters_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the data such that images for the same letters are consecutive in order\n",
    "# This might improve the results of the training a bit\n",
    "sorted_order = np.argsort(ys_real)\n",
    "ys_real = ys_real[sorted_order]\n",
    "Xs_real = Xs_real[sorted_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a random image from the data and show \n",
    "index = randrange(N_SAMPLES)\n",
    "image = Xs_real[index]\n",
    "letter = letters_map[ys_real[index]]\n",
    "print(\"Sample %d, Letter %s\" % (index, letter))\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_DIMENSION = 100\n",
    "y_input = Input(shape=(N_CLASSES,))\n",
    "Z_input = Input(shape=(Z_DIMENSION,))\n",
    "X_input = Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generator(Z_input, y_input):\n",
    "    \"\"\"\n",
    "    @param Z_dim: input tensor for the noise vector Z.\n",
    "    @param y_dim: input tensor for the conditional vector y, with dimensionality equal to number of classes\n",
    "                  (since y is the one-hot-encoded vector representing which class we want to generate).\n",
    "    \"\"\"\n",
    "    concatenated_input = Concatenate()([Z_input, y_input])\n",
    "    layer = Dense(1024)(concatenated_input)\n",
    "    layer = BatchNormalization()(layer)\n",
    "    layer = Activation(\"relu\")(layer)\n",
    "    \n",
    "    layer = Dense(128*7*7)(layer)\n",
    "    layer = BatchNormalization()(layer)\n",
    "    layer = Activation(\"relu\")(layer)\n",
    "    layer = Reshape((7,7,128))(layer)\n",
    "    \n",
    "    layer = UpSampling2D(size=2)(layer)\n",
    "    layer = Conv2DTranspose(128, kernel_size=3, strides=1, padding=\"same\")(layer)\n",
    "    layer = BatchNormalization()(layer)\n",
    "    layer = Activation(\"relu\")(layer)\n",
    "    \n",
    "    layer = UpSampling2D(size=2)(layer)\n",
    "    layer = Conv2DTranspose(1, kernel_size=5, strides=1, padding=\"same\")(layer)\n",
    "    layer = Activation(\"tanh\")(layer)\n",
    "    model = Model(inputs=[Z_input, y_input], outputs=layer)\n",
    "    return model\n",
    "\n",
    "G = get_generator(Z_input, y_input)\n",
    "G.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discriminator(X_input, y_input):\n",
    "    \"\"\"\n",
    "    @param X_input: input tensor for the actual image X.\n",
    "    @param y_input: input tensor for the conditional vector y, with dimensionality equal to number of classes\n",
    "                    (since y is the one-hot-encoded vector representing which class we want to generate).\n",
    "    \"\"\"\n",
    "    layer = Conv2D(128, kernel_size=3, strides=2, padding=\"same\")(X_input)\n",
    "    layer = BatchNormalization()(layer)\n",
    "    layer = Activation(\"relu\")(layer)\n",
    "    \n",
    "    layer = Conv2D(128, kernel_size=5, strides=2, padding=\"same\")(X_input)\n",
    "    layer = BatchNormalization()(layer)\n",
    "    layer = Activation(\"relu\")(layer)\n",
    "    \n",
    "    layer = Flatten()(layer)\n",
    "    layer = Concatenate()([layer, y_input])\n",
    "    \n",
    "    layer = Dense(256)(layer)\n",
    "    layer = BatchNormalization()(layer)\n",
    "    layer = Dropout(0.4)(layer)\n",
    "    layer = Activation(\"relu\")(layer)\n",
    "    \n",
    "    layer = Dense(128)(layer)\n",
    "    layer = BatchNormalization()(layer)\n",
    "    layer = Dropout(0.4)(layer)\n",
    "    layer = Activation(\"relu\")(layer)\n",
    "    \n",
    "    layer = Dense(1)(layer)\n",
    "    layer = Activation(\"sigmoid\")(layer)\n",
    "    model = Model(inputs=[X_input, y_input], outputs=layer)\n",
    "    return model\n",
    "\n",
    "opt = SGD(lr=0.01, momentum=0.9, decay=1e-5)\n",
    "D = get_discriminator(X_input, y_input)\n",
    "D.compile(loss=\"binary_crossentropy\", metrics=[\"accuracy\"], optimizer=opt)\n",
    "D.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cgan():\n",
    "    D.trainable = False # freeze discriminator so that only generator is trained\n",
    "    X_tensor = G([Z_input, y_input])\n",
    "    output = D([X_tensor, y_input])\n",
    "    model = Model(inputs=[Z_input, y_input], outputs=output)\n",
    "    return model\n",
    "\n",
    "opt = Adam(lr=0.01, decay=1e-5)\n",
    "cgan = get_cgan()\n",
    "cgan.compile(loss=\"binary_crossentropy\", metrics=[\"accuracy\"], optimizer=opt)\n",
    "cgan.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utililities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ROWS = 5\n",
    "N_COLS = 10\n",
    "N_EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "N_BATCHES = math.ceil(N_SAMPLES/BATCH_SIZE)\n",
    "IMAGES_PATH = \"images/\"\n",
    "MODELS_PATH = \"models/\"\n",
    "if not os.path.exists(IMAGES_PATH): os.mkdir(IMAGES_PATH)\n",
    "if not os.path.exists(MODELS_PATH): os.mkdir(MODELS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_encoding(ys):\n",
    "    onehot = [to_categorical(y, N_CLASSES) for y in ys]\n",
    "    return np.array(onehot)\n",
    "\n",
    "def generate_random_Z_noise(numSamples):\n",
    "    return np.random.normal(0.0, 1.0, size=(numSamples, Z_DIMENSION))\n",
    "\n",
    "def generate_random_conditions(numSamples):\n",
    "    return np.random.choice(N_CLASSES, numSamples)\n",
    "\n",
    "def generate_labels(image_type, flipped_ratio):\n",
    "    if image_type == \"real\":\n",
    "        labels = np.random.uniform(0.9, 1.0, size=(BATCH_SIZE, 1))\n",
    "    elif image_type == \"fake\":\n",
    "        labels = np.random.uniform(0.0, 0.1, size=(BATCH_SIZE, 1))\n",
    "    flipped_indices = np.random.choice(BATCH_SIZE, size=flipped_ratio*BATCH_SIZE)\n",
    "    labels[flipped_indices] = 1 - labels[flipped_indices]\n",
    "    return labels\n",
    "\n",
    "def show_images(epoch, images, conditions):\n",
    "    targets = [letters_map[c] for c in conditions]\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "    plt.subplots_adjust(bottom=0, top=1)\n",
    "    \n",
    "    for index in range(N_ROWS*N_COLS):\n",
    "        plt.subplot(N_ROWS, N_COLS, index+1)\n",
    "        image = images[index].reshape(IMAGE_SIZE, IMAGE_SIZE)\n",
    "        plt.imshow(image, cmap=\"gray\")\n",
    "        plt.title(targets[index])\n",
    "        plt.axis(\"off\")\n",
    "    plt.savefig(IMAGES_PATH + \"epoch\" + str(epoch) + \".png\")\n",
    "    plt.show()\n",
    "\n",
    "def show_progress(epoch):\n",
    "    numSamples = N_ROWS*N_COLS\n",
    "    Z_noise = generate_random_noise(numSamples)\n",
    "    conditions = generate_random_conditions(numSamples)\n",
    "    onehot_conditions = onehot_encoding(conditions)\n",
    "    images = G.predict([Z_noise, onehot_conditions])\n",
    "    show_images(epoch, images, conditions)\n",
    "    \n",
    "def save_progress(epoch):\n",
    "    G.save(MODELS_PATH + \"G_epoch\" + str(epoch) + \".h5\")\n",
    "    D.save(MODELS_PATH + \"D_epoch\" + str(epoch) + \".h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(N_EPOCHS):\n",
    "    D_total_loss = 0\n",
    "    G_total_loss = 0\n",
    "\n",
    "    for batch in range(N_BATCHES):\n",
    "        startIndex = (-BATCH_SIZE) if (batch == N_BATCHES-1) else (batch*BATCH_SIZE)\n",
    "        stopIndex = (-1) if (batch == N_BATCHES-1) else (startIndex+BATCH_SIZE)\n",
    "        \n",
    "        # get real data\n",
    "        X_real = Xs_real[startIndex:stopIndex] # images\n",
    "        y_real = ys_real[startIndex:stopIndex] # conditions\n",
    "        \n",
    "        # generate fake data using the same conditional vectors\n",
    "        Z_fake = generate_random_noise(BATCH_SIZE)\n",
    "        y_fake = y_real\n",
    "        X_fake = G.predict([Z_fake, y_fake])\n",
    "        \n",
    "        # train discriminator on real data\n",
    "        soft_labels_real = generate_labels(\"real\", 0.05)\n",
    "        D_loss_real = D.train_on_batch([X_real, y_real], soft_labels_real)\n",
    "        \n",
    "        # train discriminator on fake data\n",
    "        soft_labels_fake = generate_labels(\"fake\", 0.05)\n",
    "        D_loss_fake = D.train_on_batch([X_fake, y_fake], soft_labels_fake)\n",
    "        \n",
    "        # train generator with hard labels for real data\n",
    "        Z_fake = generate_random_noise(BATCH_SIZE)\n",
    "        hard_labels_real = generate_labels(\"real\", 0)\n",
    "        G_loss = cgan.train_on_batch([Z_fake, y_fake], hard_labels_real)\n",
    "        \n",
    "        # handle the loss\n",
    "        D_total_loss += (D_loss_real + D_loss_fake) / 2\n",
    "        G_total_loss += G_loss\n",
    "    \n",
    "    print(\"Epoch: {}, G Loss: {}, D Loss: {}\".format(epoch, G_total_loss/N_BATCHES, D_total_loss/N_BATCHES))\n",
    "    show_progress(epoch)\n",
    "    save_progress(epoch)"
   ]
  },
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DML",
   "language": "python",
   "name": "dml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
