{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checklist for submission\n",
    "\n",
    "It is extremely important to make sure that:\n",
    "\n",
    "1. Everything runs as expected (no bugs when running cells);\n",
    "2. The output from each cell corresponds to its code (don't change any cell's contents without rerunning it afterwards);\n",
    "3. All outputs are present (don't delete any of the outputs);\n",
    "4. Fill in all the places that say `YOUR CODE HERE`, or \"**Your answer:** (fill in here)\".\n",
    "5. You **ONLY** change the parts of the code we asked you to, nowhere else (change only the coding parts saying `# YOUR CODE HERE`, nothing else);\n",
    "6. Don't add any new cells to this notebook;\n",
    "7. Fill in your group number and the full names of the members in the cell below;\n",
    "8. Make sure that you are not running an old version of IPython (we provide you with a cell that checks this, make sure you can run it without errors).\n",
    "\n",
    "Failing to meet any of these requirements might lead to either a subtraction of POEs (at best) or a request for resubmission (at worst).\n",
    "\n",
    "We advise you the following steps before submission for ensuring that requirements 1, 2, and 3 are always met: **Restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All). This might require a bit of time, so plan ahead for this (and possibly use Google Cloud's GPU in HA1 and HA2 for this step). Finally press the \"Save and Checkout\" button before handing in, to make sure that all your changes are saved to this .ipynb file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Group number and member names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROUP = \"43\"\n",
    "NAME1 = \"Edvin Agnas\"\n",
    "NAME2 = \"Hai Dinh\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you can run the following cell without errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "assert IPython.version_info[0] >= 3, \"Your version of IPython is too old, please update it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f19467ab4786e36e3793ec0a949a3872",
     "grade": false,
     "grade_id": "cell-5f64122350a250b4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# HA2 - Recurrent Neural Networks\n",
    "\n",
    "Welcome to the second group home assignment!  \n",
    "The purpose of this assigment is to give you practical knowledge in how to implement recurrence in a neural network.  \n",
    "\n",
    "Every day you are exposed to sequences of data. For example text, video streams, audio, financial time series and medical sensors. Recurrence is therefore an important topic in the field of Machine Learning because it has the potential to solve real-world problems including the types of data above.  \n",
    "\n",
    "In this assignment, you will learn to:\n",
    " - Preprocess text data sets\n",
    " - Implement a vanilla RNN cell using only numpy\n",
    " - Text generation\n",
    " - Build a recurrent neural network with LSTM using Keras  \n",
    " - Neural machine translation\n",
    "\n",
    "**NOTE:** Task 1 (Shallow vanilla RNN) and Task 2 (Neural machine translation), are independent from each other. Task 2 asks you to train a NMT, which takes a while (specially without a GPU), so it might be efficient to start with task 2 and leave it running in the background while you solve task 1.\n",
    "\n",
    "Table of Contents:  \n",
    " [1 Shallow vanilla RNN](#1)  \n",
    "   [1.1 Preprocessing](#1.1)  \n",
    "     [1.1.1 Loading dataset](#1.1.1)  \n",
    "     [1.1.2 One-hot representations](#1.1.2)  \n",
    "   [1.2 RNN cell class](#1.2)  \n",
    "   [1.3 Training the RNN](#1.3)  \n",
    " [2 Neural machine translation](#2)  \n",
    "   [2.1 Pre-processing](#2.1)  \n",
    "     [2.1.1 Loading and inspecting dataset](#2.1.1)  \n",
    "     [2.1.2 Cleaning the dataset](#2.1.2)  \n",
    "     [2.1.3 Restricting sentence length and shuffling the data set](#2.1.3)  \n",
    "     [2.1.4 Word-to-index and index-to-word conversions](#2.1.4)  \n",
    "     [2.1.5 Padding](#2.1.5)  \n",
    "     [2.1.6 One-hot labels](#2.1.6)  \n",
    "   [2.2 Implementing a sequence-to-sequence model](#2.2)  \n",
    "     [2.2.1 Defining the architecture](#2.2.1)  \n",
    "     [2.2.2 Training the model](#2.2.2)  \n",
    "     [2.2.3 Evalutation](#2.2.3)  \n",
    "     [2.2.4 Testing](#2.2.4)  \n",
    "     \n",
    "**NOTE**: The tests available are not exhaustive, meaning that if you pass a test you have avoided the most common mistakes, but it is still not guaranteed that you solution is 100% correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ba23d62a14b6319db7d7e4056c2a3631",
     "grade": false,
     "grade_id": "cell-e3df9ba622ac9b73",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 1 Shallow vanilla RNN <a class=\"anchor\" id=\"1\"></a>\n",
    "In the first part of this assignment, you will implement a recurrent neural network from scratch without using any framework. You will train this network to predict the next character of a text, which will result in a network that can generate new sentences.  \n",
    "\n",
    "Start by importing dependencies below  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "afba6c87700a3a984c4e3dccfa61bfa1",
     "grade": false,
     "grade_id": "cell-625d522e7af1be75",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.tests.ha2Tests import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d1e46aaed7aaad0c16768220a8439c71",
     "grade": false,
     "grade_id": "cell-161d4d51d8efd9a0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 1.1 Preprocessing <a class=\"anchor\" id=\"1.1\"></a>\n",
    "#### 1.1.1 Loading data set <a class=\"anchor\" id=\"1.1.1\"></a>\n",
    "The text corpus to train your RNN on is going to be the book The metamorphosis by Franz Kafka. Run the cell below to load the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "88ebbbf552d6a670317d098cce99d154",
     "grade": false,
     "grade_id": "cell-eddd3178e98f6819",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 118560 chars, 62 unique\n"
     ]
    }
   ],
   "source": [
    "data = open('./utils/kafka.txt', 'r', encoding=\"utf-8\").read()\n",
    "\n",
    "chars = list(set(data)) \n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d chars, %d unique' % (data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9f80e54d3658a982d5bd894a20d4d82f",
     "grade": false,
     "grade_id": "cell-7e993f61d8a98a13",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### 1.1.2 One-hot representations <a class=\"anchor\" id=\"1.1.2\"></a>\n",
    "`data` is now a string, containing the contents of the book, with 80 unique characters. As usual, converting the labels (that is also the data in our case) into one-hot vectors is a good way for creating unbiased labels.  \n",
    "\n",
    "Below is a variable `one_hot_vectors` defined that maps every character in `chars` to a unique one-hot vector  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "609241821c3eb4ce1b929a9c727f355c",
     "grade": false,
     "grade_id": "cell-547e8c9d5874c36e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "one_hot_vectors = np.eye(len(chars))\n",
    "char_to_onehot = { val: one_hot_vectors[key,:].reshape(1,-1) for key, val in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "aa4da6c7dea9d40f5272f05f2c1d6f9f",
     "grade": false,
     "grade_id": "cell-05d8c9b256c98f8e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 1.2 RNN cell class <a class=\"anchor\" id=\"1.2\"></a>\n",
    "\n",
    "Now you will implement the RNN class. For your convenience, this task is split into 4 subtasks, one for each method of the class. These methods will be later linked to a class definition (if you're curious, scroll down to 1.2.5).\n",
    "\n",
    "Instead of implementing a neural network of arbitrary length like in **IHA1**, you are only going to implement a single RNN cell. At each time-step, the RNN cell will have one input, $\\mathbf{x}_t$ , one hidden layer with a hidden state $\\mathbf{h}_t$, and one output $\\mathbf{y}_t$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7ed0bfaac4000d97728b03afb10071a8",
     "grade": false,
     "grade_id": "cell-3a5034fe7f3b2df3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "![Simple RNN cell](utils/images/RNN-cell.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "42e1be4c7ad74323a713e6cbc61a3533",
     "grade": false,
     "grade_id": "cell-719152523ee5b5b6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The output at time-step $t$, referred to as $\\mathbf{y}_t$, is computed according to the following equations. Equation 1 and 2 together define the next hidden state $\\mathbf{h}_t$, and Equation 3 defines the unnormalized probability output $\\mathbf{z}_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ca5169401c992cb557a7e99f45e9405a",
     "grade": false,
     "grade_id": "cell-433ba61c3d3149cb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "$$\n",
    "\\mathbf{z}_t =  \\mathbf{x}_t \\mathbf{W}_{xh} + \\mathbf{h}_{t-1} \\mathbf{W}_{hh} + \\mathbf{b}_h, \\tag{1}\n",
    "$$\n",
    "$$\n",
    "\\mathbf{h}_t = \\tanh (\\mathbf{z}_t), \\tag{2}\n",
    "$$\n",
    "$$\n",
    "\\mathbf{y}_t =  \\mathbf{h}_t \\mathbf{W}_{hy} + \\mathbf{b}_y, \\tag{3}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{W}_{xh}$, $\\mathbf{W}_{hh}$, $\\mathbf{W}_{hy}$, $\\mathbf{b}_{h}$, and $\\mathbf{b}_{y}$ are the parameters of the RNN cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6db71b1252e96bb33a732c034090ba57",
     "grade": false,
     "grade_id": "cell-651ce67679087d00",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Calculating $\\mathbf{h}_{30}$ means that you need to need to have calculated every earlier hidden state $\\{\\mathbf{h}_t | t \\in \\{0, 1, ..., 29\\}\\}$. This can easier be understood by always imagining an unrolled RNN cell:\n",
    "![Simple RNN cell unrolled](utils/images/RNN-cell-unrolled.png)\n",
    "  \n",
    "\n",
    "Since it is a multiclass classification problem, the softmax activation function is going to be used to normalize the output of the RNN cell $\\mathbf{y}_t$, defined as\n",
    "\n",
    "$$\n",
    "\\mathbf{p}_t = softmax(\\mathbf{y}_t) = \\frac{e^{\\mathbf{y}_t - max(\\mathbf{y_t})}}{ \\sum^j e^{y_{j_t} - max(\\mathbf{y_t})}}. \\tag{4}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ba386cb1102d60da82adaa9b0deec8af",
     "grade": false,
     "grade_id": "cell-d98b57a52debcfcf",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### 1.2.1 The initalization method\n",
    "\n",
    "The first step in creating the RNN class is to create its `__init__` method, where we create all the necessary attributes and initialize them. Complete the `init_routine` function below. Later this will be linked to the `__init__` method in the `SimpleRNN` class, so you're now developing the constructor of that class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "0ab240474adc0f910903ac2a9321bb6b",
     "grade": true,
     "grade_id": "cell-c54cfa041a653943",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def init_routine(self, input_dim, hidden_dim, output_dim):\n",
    "    \"\"\"\n",
    "    Initialize the weights of the RNN and define a cache with the starting value of the hidden state\n",
    "\n",
    "    Arguments:\n",
    "    self - an object of the class SimpleRNN\n",
    "    input_dim - an integer representing the number of inputs\n",
    "    hidden_dim - an integer representing the length of the hidden state vector\n",
    "    output_dim - an integer representing the number of outputs\n",
    "\n",
    "    Attributes:\n",
    "    input_dim - attribute initialized with the value of the argument `input_dim`\n",
    "    cache - a dictionary holding the values of `h_start`, `xs` and `hs` from the previous forward propagation call.\n",
    "            h_start - the value to initialize the hidden state with when `forward_prop is called. The value of\n",
    "                      `h_start` should be initialized to a `numpy.ndarray` vector of zeros \n",
    "            xs - the list of `xs` used in the last `forward_prop` call. Later used in `backward_prop`. Can be\n",
    "                 initialized to `None`\n",
    "            hs - the list of `hs` calculated in the last `forward_prop` call. Later used in `backward_prop`. Can\n",
    "                 be initialized to `None`\n",
    "    W_hh - weight matrix of shape (hidden_dim, hidden_dim) and type `numpy.ndarray`. \n",
    "           Initialized randomly from a normal distribution of mean zero and stddev 0.01\n",
    "    W_xh - weight matrix of shape (input_dim, hidden_dim) and type `numpy.ndarray`. \n",
    "           Initialized randomly from a normal distribution of mean zero and stddev 0.01\n",
    "    W_hy - weight matrix of shape (hidden_dim, output_dim) and type `numpy.ndarray`. \n",
    "           Initialized randomly from a normal distribution of mean zero and stddev 0.01\n",
    "    b_h -  bias of shape (1, hidden_dim) and type `numpy.ndarray`. Initialized to all zeros\n",
    "    b_y -  bias of shape (1, output_dim) and type `numpy.ndarray`. Initialized to all zeros\n",
    "    \"\"\"\n",
    "\n",
    "    mu, stdev = 0, 0.01\n",
    "    self.input_dim = input_dim\n",
    "    self.cache = {\n",
    "        'h_start': np.zeros((1, hidden_dim)),\n",
    "        'xs': None,\n",
    "        'hs': None\n",
    "    }\n",
    "    self.W_hh = np.random.normal(mu, stdev, (hidden_dim, hidden_dim))\n",
    "    self.W_xh = np.random.normal(mu, stdev, (input_dim, hidden_dim))\n",
    "    self.W_hy = np.random.normal(mu, stdev, (hidden_dim, output_dim))\n",
    "    self.b_h = np.zeros((1, hidden_dim))\n",
    "    self.b_y = np.zeros((1, output_dim))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5f3fd0e811e9068cf8230649b148da21",
     "grade": false,
     "grade_id": "cell-e90af5372d951d74",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The following cell tests if your implementation is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "00c3497a25df57a35a4d08cdb3406074",
     "grade": false,
     "grade_id": "cell-9595194787574666",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test init passed\n"
     ]
    }
   ],
   "source": [
    "test_init_routine(init_routine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e8b75e936deebb02f55f7ee19fdeb7f1",
     "grade": false,
     "grade_id": "cell-81027fe5c9d77a7b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### 1.2.2 Forward propagation\n",
    "\n",
    "Now that we have an initialization method for declaring objects of our class, we need a method for performing the forward propagation step. Complete the `forward_prop_routine` function. This will later be linked to the `forward_prop` routine of the `SimpleRNN` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "0c3a5e91dfe32670754498a08282ad6f",
     "grade": true,
     "grade_id": "cell-76c90350a75c8c65",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def forward_prop_routine(self, xs, reset_h=False):\n",
    "    \"\"\"\n",
    "    Performs forward propagation of the input `xs` in sequential order, and then returns all predictions `ys_pred`\n",
    "    The steps are: [Input x] -> [Hidden h] -> [Output unnormalized z] -> [Prediction probabilities y]\n",
    "\n",
    "    Arguments:\n",
    "    xs - a python list of one-hot characters to predict the next character from,\n",
    "         Has a shape of list([1,OUTPUT_DIM]) and length `len(xs)`\n",
    "    reset_h - boolean value, whether or not the hidden state should be reset\n",
    "\n",
    "    Returns:\n",
    "    ys_pred - a python list of prediction probabilities for each character in `xs`. There are `len(xs)`\n",
    "              elements in ys_pred, each element `i` of shape (1, output_dim) and type `numpy.ndarray`\n",
    "              that contains the probabilities of what the next character xs[i+1] can be\n",
    "\n",
    "    Example:\n",
    "    xs - [np.array([0,0,1]), np.array([0,1,0]), np.array([1,0,0])]\n",
    "    ys_pred (ground truth answer) - [np.array([0,1,0]), np.array([1,0,0]), np.array([...])]\n",
    "    \"\"\"\n",
    "    # initialize the list of predictions\n",
    "    ys_pred = [0] * len(xs)\n",
    "\n",
    "    # load the last hidden state of the previous batch\n",
    "    if reset_h:\n",
    "        self.cache['h_start'] = np.zeros((1, self.W_hh.shape[0]))\n",
    "    hs = {-1: self.cache['h_start']}\n",
    "\n",
    "    # forward propagate\n",
    "    for index, item in enumerate(ys_pred):\n",
    "        z = xs[index] @ self.W_xh + hs[index-1] @ self.W_hh + self.b_h\n",
    "        h = np.tanh(z)\n",
    "        y = h @ self.W_hy + self.b_y\n",
    "        p = np.exp(y - np.max(y)) # softmax numerator\n",
    "        p = p / np.sum(p)         # softmax\n",
    "        ys_pred[index] = p\n",
    "        hs[index] = h\n",
    "    \n",
    "    # save list of hidden state in cache to use later in backprop\n",
    "    self.cache = {\n",
    "        'hs': hs,\n",
    "        'xs': xs,\n",
    "        'h_start': hs[len(xs) - 1]\n",
    "    }\n",
    "    return ys_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ffe0bd561a59bc4014d8dbf3508a899c",
     "grade": false,
     "grade_id": "cell-0cffc6927767f0fd",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The following cell tests if your implementation is correct (it assumes you correctly solved task 1.2.1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ef2fa074e9ad94c1c6d09dbb68d9530c",
     "grade": false,
     "grade_id": "cell-285af3fd03af809c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test passed, dimensions are correct\n"
     ]
    }
   ],
   "source": [
    "test_forward_prop_routine(init_routine, forward_prop_routine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b50d16ec08ef9da4541236762dd2283f",
     "grade": false,
     "grade_id": "cell-74f27955261e8417",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### 1.2.3 Backward propagation\n",
    "\n",
    "Now that you developed methods for initialization and forward propagation, it's time to implement the backward propagation algorithm as a method for our class.\n",
    "\n",
    "The cross-entropy loss will be used for this task, and just like in **IHA1**, backward pass of the softmax activation function and the cross-entropy loss can be combined into a simple formula: $\\mathbf{\\hat{p_t}} - \\mathbf{p_t}$, where $\\mathbf{\\hat{p_t}}$ is the predicted output vector for time step $t$ and $\\mathbf{p_t}$ is the ground truth one-hot vector for time step $t$.\n",
    "The full collection of backward pass formulas that are required for implementing `backward_prop_routine` are shown in Equations 5-12:  \n",
    "\n",
    "$$\n",
    "\\frac{d\\mathcal{L}_t}{d \\mathbf{y}_t}=\\hat{\\mathbf{p}}_t-\\mathbf{p}_t \\tag{5} ~,~ \\text{ for } t=1,\\dots,N, \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{d\\mathcal{L}}{d\\mathbf{W}_{hy}}=\\sum_{t=1}^N\\mathbf{h}_t^T\\frac{d\\mathcal{L}_t}{d \\mathbf{y}_t}, \\tag{6}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{d\\mathcal{L}}{d\\mathbf{b}_{y}}=\\sum_{t=1}^N\\frac{d\\mathcal{L}_t}{d \\mathbf{y}_t},   \\tag{7}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{d\\mathcal{L}}{d\\mathbf{h}_t}=\\frac{d \\mathcal{L}_t}{d \\mathbf{y}_t}\\mathbf{W}_{hy}^T+(1-\\mathbf{h}_{t+1}^2)\\odot\\frac{d\\mathcal{L}}{d\\mathbf{h}_{t+1}}\\mathbf{W}_{hh}^T   \\tag{8.1} ~,~ \\text{ for }t=1,\\dots,N-1,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{d\\mathcal{L}}{d\\mathbf{h}_N}=\\frac{d \\mathcal{L}_t}{d \\mathbf{y}_N}\\mathbf{W}_{hy}^T,   \\tag{8.2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{d\\mathcal{L}}{d\\mathbf{z}_t}=(1-\\mathbf{h}_t^2)\\odot\\frac{d\\mathcal{L}}{d\\mathbf{h}_t}   ~,~ \\text{ for } t=1,\\dots,N,\\tag{9}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{d\\mathcal{L}}{d\\mathbf{W}_{xh}}=\\sum_{t=1}^N\\mathbf{x}_t^T\\frac{d\\mathcal{L}}{d\\mathbf{z}_t},   \\tag{10}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{d\\mathcal{L}}{d\\mathbf{W}_{hh}}=\\sum_{t=1}^N\\mathbf{h}_{t-1}^T\\frac{d\\mathcal{L}}{d\\mathbf{z}_t},   \\tag{11}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{d\\mathcal{L}}{d\\mathbf{b}_h}=\\sum_{t=1}^N\\frac{d\\mathcal{L}}{d\\mathbf{z}_t},   \\tag{12}\n",
    "$$\n",
    "where $\\odot$ is the <a href=\"https://en.wikipedia.org/wiki/Hadamard_product_(matrices)\">hadamard product / elementwise multiplication</a>.  \n",
    "\n",
    "Complete the `backward_prop_routine` function below. This will later be linked to the `backward_prop` method of the `SimpleRNN` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "9c01e4f1d6658f6c2bf47727319ccef0",
     "grade": true,
     "grade_id": "cell-b0ed3463482a9b7a",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def backward_prop_routine(self, ys, ys_pred):\n",
    "    \"\"\"\n",
    "    Performs backward propagation, calculating the gradients of every trainable weight. The gradients should\n",
    "    be clipped into the interval [-5,5]\n",
    "\n",
    "    Arguments:\n",
    "    ys - a python list of true labels (one-hot vectors of type `numpy.ndarray` for every character to predict)\n",
    "         has a shape list( (1,OUTPUT_DIM) )\n",
    "    ys_pred - a python list of predicted labels (one-hot vectors of type `numpy.ndarray` for every character predicted)\n",
    "         has a shape list( (1,OUTPUT_DIM) )\n",
    "\n",
    "    Returns:\n",
    "    gradients - a dictionary of the gradients of every trainable weight. The keys are the weight variable names\n",
    "                and the values are the actual gradient value\n",
    "    \"\"\"\n",
    "    # extract from cache\n",
    "    hs = self.cache['hs']\n",
    "    xs = self.cache['xs']\n",
    "    \n",
    "    # intialize all variables\n",
    "    N = len(ys)\n",
    "    db_h = db_y = dW_hy = dW_xh = dW_hh = 0.0\n",
    "    y_gradients = [ys_pred[t] - ys[t] for t in range(N)]\n",
    "    \n",
    "    # loop over all time steps to compute gradients w.r.t each hiddent state\n",
    "    h_gradients = [y_gradients[t] @ self.W_hy.T for t in range(N)]\n",
    "    for t in range(N-1, 0, -1):\n",
    "        h_gradients[t-1] += (1 - hs[t] * hs[t]) * h_gradients[t] @ self.W_hh.T\n",
    "\n",
    "    # loop over each time step to accumulate the gradients\n",
    "    for t in range(N):\n",
    "        dz_t = (1 - hs[t] * hs[t]) * h_gradients[t]\n",
    "        db_h += dz_t\n",
    "        db_y += y_gradients[t]\n",
    "        dW_hy += hs[t].T @ y_gradients[t]\n",
    "        dW_xh += xs[t].T @ dz_t\n",
    "        dW_hh += hs[t-1].T @ dz_t\n",
    "        \n",
    "    # clip gradients to prevent explosion\n",
    "    for dparam in [dW_xh, dW_hh, dW_hy, db_h, db_y]:\n",
    "        np.clip(dparam, -5, 5, out=dparam)\n",
    "\n",
    "    # save gradients into dict and return\n",
    "    gradients = {\n",
    "        'W_xh': dW_xh, \n",
    "        'W_hh': dW_hh, \n",
    "        'W_hy': dW_hy, \n",
    "        'b_h': db_h, \n",
    "        'b_y': db_y\n",
    "    }\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "78bd713cb2cf1e6db5d95f5d9e7d3f63",
     "grade": false,
     "grade_id": "cell-277e664111175140",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The following cell tests if your implementation is correct (it assumes you correctly solved tasks 1.2.1 and 1.2.2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4f82f89395a7f4650b44dadf40098f61",
     "grade": false,
     "grade_id": "cell-6c90c566fcf8388f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your BPTT implementation is correct.\n"
     ]
    }
   ],
   "source": [
    "test_backward_prop_routine(init_routine, forward_prop_routine, backward_prop_routine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7e173be209fcee69b1bb2b90b57efc1f",
     "grade": false,
     "grade_id": "cell-27bb37bddc1c8291",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### 1.2.4 Applying the gradients\n",
    "\n",
    "Finally, we need a method that updates the paramters of the RNN cell, given the computed gradients of the loss with respect to each one of them. Complete the `apply_gradients_routine` function below. This will later be linked to the `apply_gradients` method of the `SimpleRNN` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "02925c28cb85c2167afded8f143c7dee",
     "grade": true,
     "grade_id": "cell-86233bb4464c563d",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def apply_gradients_routine(self, gradients, learning_rate):\n",
    "    \"\"\" Performs the weight update procedure. Updates every weight with its corresponding gradient \n",
    "    found in the `gradients` input dictionary\n",
    "\n",
    "    Arguments:\n",
    "    gradients - dictionary containing (key, value) pairs, where the key is a weight variable name of\n",
    "                the `SimpleRNN` class and the value is the gradient to apply to the matching weight\n",
    "                Example: {'W_xh':0.04, 'b_h':0.11}\n",
    "    learning_rate - the learning rate to use for this iteration of weight updates\n",
    "    \"\"\"\n",
    "    self.W_xh -= learning_rate * gradients['W_xh']\n",
    "    self.W_hh -= learning_rate * gradients['W_hh']\n",
    "    self.W_hy -= learning_rate * gradients['W_hy']\n",
    "    self.b_h -= learning_rate * gradients['b_h']\n",
    "    self.b_y -= learning_rate * gradients['b_y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d89888760cc8351156dd4b38e2e37357",
     "grade": false,
     "grade_id": "cell-0660819488941eb7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The following cell tests if your implementation is correct (it assumes you correctly solved task 1.2.1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5636499fd3046ff01566a81b29f477e2",
     "grade": false,
     "grade_id": "cell-bb2cdea3e67c6a3b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test apply gradients passed\n"
     ]
    }
   ],
   "source": [
    "test_apply_gradients_routine(init_routine, apply_gradients_routine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1fef1cc499275c4ad36e86c6b06f11c6",
     "grade": false,
     "grade_id": "cell-30440dc54623f06c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### 1.2.5 Putting everything together\n",
    "Now that all the parts of the RNN are implemented, we can define the class. The following cell defines the `SimpleRNN` class, linking all the functions you developed in the earlier tasks to corresponding methods. Additionally, it also implements another method, the `sample`, used to generate output from a trained RNN. Note that you don't have to change anything in this cell, just run it after solving the previous tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5036021730af040e49d53fd5f45cb5d5",
     "grade": false,
     "grade_id": "cell-126a1305361a1287",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "class SimpleRNN:\n",
    "    \"\"\"\n",
    "    The simple RNN class definition\n",
    "    Implements the initializations of the weights, the forward propagation from `x` to `y_pred`,\n",
    "    the backward propagation with respect to each trainable weight and the update weights rule.\n",
    "    \"\"\"\n",
    "    \n",
    "    # __init__ method is now the initialization_routine function\n",
    "    __init__ = init_routine\n",
    "        \n",
    "    # forward_prop method is the forward_prop_routine function\n",
    "    forward_prop = forward_prop_routine\n",
    "    \n",
    "    # backward_prop method is the backward_prop_routine function\n",
    "    backward_prop = backward_prop_routine\n",
    "    \n",
    "    # apply_gradients method is the apply_gradients_routine function\n",
    "    apply_gradients = apply_gradients_routine\n",
    "\n",
    "\n",
    "    def sample(self, seed, n, char_to_onehot, chars):\n",
    "        \"\"\" Given a seed character `seed`, generates a sequence of `n` characters by continuously feeding the output\n",
    "        character of the RNN at time t as the input for the next time step at time t+1. If the RNN is trained well, \n",
    "        this method should be able to generate sentences that resembles some kind of structure. \n",
    "        The character should be generated with a probability of the outputs of the network!\n",
    "        \n",
    "        Arguments:\n",
    "        seed - a first character of type str to feed into the SimpleRNN\n",
    "        n - the length of the character sequence to generate\n",
    "        char_to_onehot - a dict that maps keys of characters to its one-hot representation. \n",
    "                         You created this dict earlier in the lab\n",
    "        char - a list of chars (vocabulary). You also created this list earlier in the lab\n",
    "        \"\"\"\n",
    "        \n",
    "        saved_cache = copy.deepcopy(self.cache)\n",
    "        \n",
    "        char_list = []\n",
    "        h = self.cache['h_start']\n",
    "\n",
    "        # seed one-hot char\n",
    "        x = char_to_onehot[seed]\n",
    "\n",
    "        for t in range(n):\n",
    "            \"\"\" not relevant anymore\n",
    "            # perform forward prop. `forward_prop` is not called because then the hidden state for the\n",
    "            # training procedure will be overwritten by the hidden state generates by this sampling procedure\n",
    "            h = np.tanh(np.dot(x, self.W_xh) + np.dot(h, self.W_hh) + self.b_h)\n",
    "            y = np.dot(h, self.W_hy) + self.b_y\n",
    "            p = np.exp(y) / np.sum(np.exp(y))\n",
    "            \"\"\"\n",
    "            p = self.forward_prop(x)[0]\n",
    "            \n",
    "            # randomly pick a character with respect to the probabilities of the output of the network\n",
    "            ix = np.random.choice(range(self.input_dim), p=p.ravel())\n",
    "\n",
    "            x = np.zeros((1, self.input_dim))   \n",
    "            x[0,ix] = 1\n",
    "\n",
    "            # save the generated character\n",
    "            char_list.append(chars[ix])\n",
    "            \n",
    "        self.cache = saved_cache\n",
    "\n",
    "        return ''.join(char_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2e2745ec97c2751a6491248946136dba",
     "grade": false,
     "grade_id": "cell-aaa08e666d9fed34",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 1.3 Training the RNN <a class=\"anchor\" id=\"1.3\"></a>\n",
    "In this section, code has been provided to you to train a `SimpleRNN` cell by predicting the next charcter in sequence.  \n",
    "\n",
    "The code below has the following properties: \n",
    " * Loop over `data`, using `seq_length` words at a time as a batch to train with\n",
    " * When `data` has been fully iterated through, start over again\n",
    " * Start over again with `data` in an infinite loop and then interrupt the running cell\n",
    " when results from `sample` are good enough.  \n",
    " * Perform `forward_prop`, `backward_prop` and  `apply_gradients` with each batch of characters\n",
    " * every 1000th iteration:\n",
    "     * compute the cross-entropy loss of the current batch\n",
    "     * sample a character sequence of length 200\n",
    "     * print the loss and the sampled sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = SimpleRNN(len(chars), 500, len(chars))\n",
    "\n",
    "# the number of characters to perform one update of the network with,\n",
    "# can be thought of as a batch\n",
    "seq_length = 25\n",
    "\n",
    "# the learning rate to use\n",
    "learning_rate = 0.001\n",
    "\n",
    "while True: # the infinite loop\n",
    "    \n",
    "    # iterate over the data set\n",
    "    for i in range(len(data)//seq_length):\n",
    "        \n",
    "        # convert to one-hot\n",
    "        xs = [char_to_onehot[c] for c in data[i*seq_length:(i+1)*seq_length]]#inputs to the RNN\n",
    "        ys = [char_to_onehot[c] for c in data[i*seq_length+1:(i+1)*seq_length+1]]#the targets it should be outputting\n",
    "\n",
    "        # fp, bp and update\n",
    "        ys_pred = rnn.forward_prop(xs)\n",
    "        gradients = rnn.backward_prop(ys, ys_pred)\n",
    "        rnn.apply_gradients(gradients, learning_rate)\n",
    "\n",
    "        # print loss and sample a sentence every 1000th iteration\n",
    "        if i%1000==0:\n",
    "            loss = np.sum([ -1.0 *np.sum(y * np.log(y_pred + 1e-8)) for y, y_pred in zip(ys, ys_pred) ]) # x-entropy\n",
    "            print('iteration %d, loss: %f' % (i, loss))\n",
    "            text = rnn.sample('a', 200, char_to_onehot, chars)\n",
    "            print(text, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8656704cd15109f98b427a7defe0fa1b",
     "grade": false,
     "grade_id": "cell-31b8664715bebff9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Run the following cell to produce sentences using the trained RNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5a947804c59cbf08c78563e7d7c9c51f",
     "grade": false,
     "grade_id": "cell-ed2dd488942f6809",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Sentence 0: ------\n",
      "s car hn oshs wsis sbeed borl isipHtm not edsed r lo gihuc hr eo we une  wnt uoe Hloo whq aalre honyhp tntess tolrs onmsrzB wh hih gieerd ufims frgnor gae olnHwnrnbd ale har tor atu ibd xatduce wrm co\n",
      "------ Sentence 1: ------\n",
      " ;aen hily hhvuam re lnos iyeb pa, bae sr wo wyr wo in gi rrusn agsilo jhr me ,oly ee gk lea swod get ig hresitlk ool ld inm hnuFt nr te moa anunte wre ehn tonne no htmlhslawe, do) chs ptnp wunG twir \n",
      "------ Sentence 2: ------\n",
      "l wo bjiws ohy rhbr aodm tvrp wht ossrnc nbt el  ta' fonoa fhlnswlexrewmni w,oBa. cae oblid Yor,e hest) eys ond pet ,e ttsh xd worugegctl lonoi bi pas wms, hie yh e nmitn fosrr mewa cto xa wnu se hav \n",
      "------ Sentence 3: ------\n",
      "ir tolu Iil tonom tr  nr ddtl te, ahe oo Or,d pht  aomn, bole nh t rs heeasi sysrtho Mosr tod gier thaty zkwags storw. cle ho d, win lde ,leo,nd uwd fhla aeC hnr ,o nosl, oimaee tas iity eh\" wae chf a\n",
      "------ Sentence 4: ------\n",
      "iy s Ue meu hob ir yo, drrt ,i porooems tith fat pmd sf wauasss tois hote he, naw  ootue hah ra oa rnu ty whubi hk an loirt aaH, thethe gero onon fo anr aogml !aera\n",
      "t eor ha worumirc wous Dos tsnoce f\n",
      "------ Sentence 5: ------\n",
      "esrd upn uag wee the wi wmen gan tslia mpnd th uhe vis sisiaib istpnh goqd si\n",
      " col )a , we svyt latb tn tyedk ma anrr sanee  hiy he hotv nly ; nhg ito ses ut me wreapece hs, sc tptd muhm anri se oiry \n",
      "------ Sentence 6: ------\n",
      " ,Id 'llrp yh wse hit tsv oat he witsadec, tsy kod la,d oopae nt ohnd wte b,d tud nie tapwtrl t;n kan  hls tnunerta wi;trlr,od Wags uiver tdW wcuee dhe rigr ryrhe tnv hen sn'es tio ocd ss isee uo wgsr\n",
      "------ Sentence 7: ------\n",
      "t he ebc yee dbtts efar'clp gn whrs tc tiaEe fasr c hit ,oatotwl; tt st a were bol tovo ridn,utbfi cuad aen hhele hatr oe eosk, :age iiigs ;edctso nhm nelt seb wm nm,n the pue way ros aik oe sodL rerb\n",
      "------ Sentence 8: ------\n",
      "mlimwem sai Gosrel rorl arnuh Qod wklws thm fs wite F, oky lhcis  in resay- wnG ped ih eo wos m;ed te orie faarroauogn Gomherteed ynw iFni, roin tn k ae .bs sos uat ho tuteim wh,d to oun uel ao he lTd\n",
      "------ Sentence 9: ------\n",
      " ah sn dd besi nsO an eghet arcnb nsi he dsticteonwtwer we soy Gie tneB tpeledae ao wnt hair tnatogyn rert.ae f tom slp tuy tut su se teas . dt hivpGeoad gie phe seen tomlolliled stt ofcrs Lwh ravl if\n"
     ]
    }
   ],
   "source": [
    "# kafka, including upper chars\n",
    "for i in range(10):\n",
    "    print('------ Sentence %i: ------' %i)\n",
    "    print(rnn.sample('a', 200, char_to_onehot, chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3dfdd352e2576a1aadc7df1de4c0ef03",
     "grade": false,
     "grade_id": "cell-3a78b102d91ed859",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Here are some generated sentences for reference how good the results should be. Not quite passing any spell checking program, but there are some real english words in there and the sentences seem to have a reasonable structure.\n",
    "\n",
    "    ------ Sentence 0: ------\n",
    "    ted eat aread noup, seed, but. ythed whithit tued that and in ask chemr tirme ble serpareing. he wionly memind. Dedteren drager, chat ther se stealeay. The kpay shir in wha dangereag he torearr ieve w\n",
    "    ------ Sentence 1: ------\n",
    "    swayly wase Mlamer io beell hom nofrreang, hr meed tould, Grere, werned coull gitr theQr coule verustengs at jeas whear', fasmer's, ad ho mofello goin \"hey in normablite chas a) he. Heenew \"thel seefr\n",
    "    ------ Sentence 2: ------\n",
    "    r's avethen harly dsundiigex; thinew, oug lertinestad to the wourd jhen sorywe, thever shene It, thour\" hawss He:thon's batiar's sfin, luike he. Ar uver. Af. qanilew ucheed attricgwaynong roked t- an \n",
    "    ------ Sentence 3: ------\n",
    "    tor\n",
    "     unde, thab haw dtidtly kad sendeder, ithed far theme, veedlly nowlyin; and )rece thete disse, out louthe, row the rapeasterse at. Heed rnweem. \"ut he bleitny Grerorot ie reryibe. So in., Hers ane\n",
    "    ------ Sentence 4: ------\n",
    "    ding, fitse. . Whisly andit, she haing \"ya bistatily dlatkedor whing saded theust, but the stere stotady is eleaprcentake, got to mame luch ally, mus llablly, ach caling, at what, he want aar athen d'\n",
    "    ------ Sentence 5: ------\n",
    "    cll\"end sat what herpingoth thouthar ald beinvaver.\n",
    "\n",
    "    Tere sreaned on in she was athar thatssu fres is at lees morther all sroaven to somichary mothtrey norytiot a tior sow bercead lassedore towist at \n",
    "    ------ Sentence 6: ------\n",
    "    gly.\n",
    "\n",
    "    Shen in shend comrie t oreth ubserwer he dist in commicherr. Wrmeathithemy nougracl.y'e sate horkllomisted betrnowhrous wfverthr bleatekeM tusinitedy Herach mop wislrocly theirse\n",
    "     on illy ans in\n",
    "    ------ Sentence 7: ------\n",
    "    dd, puting ot been ay theed . Hemula, he farer\". The h hanor, and dandid to the sedertereveng untint.\n",
    "\n",
    "    Wken\", surme ceared thet lomp io wotcouUir onsur\"ont ar with, thancMmperny him., is sas bees coul\n",
    "    ------ Sentence 8: ------\n",
    "    d natmen\", he theretotist wist, Gregely, arly re caule he praser, and woll buccem, then atse. Hemu haded ditince and dleyrist comply. \"Vm inen beglings wet ou't dot, plest willay theren lipsest inpren\n",
    "    ------ Sentence 9: ------\n",
    "    byed Gregst oonsbecemed thet ad herladble, din ther, and at. Numferringtice st ead thelr way mors. Ium loomul themo head roon inf thet the. \"lencorn was ned in tuc'e foot butimed, -feritn be thand Yun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f5e457fb0c08a07d9504ba0290f2b79e",
     "grade": false,
     "grade_id": "cell-e01fb706e3e77e40",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "You may try different learning rates, different number of neurons for the hidden state and try to prune the text from special characters and upper characters, but the fact is that this is only one single RNN cell and it is as good as it gets.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f7be4351414f3dc4e27233b3870e8704",
     "grade": false,
     "grade_id": "cell-69f0dab9e82b0115",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 2 Neural Machine Translation <a class=\"anchor\" id=\"2\"></a>\n",
    "\n",
    "In this task you will implement a small neural machine translator using the Keras API. The final model will be able to translate english sentences into spanish sentences. Unlike the previous task, the input and output at each time step will now represent an entire word instead of a character.  \n",
    "\n",
    "The task is divided into two main sections:\n",
    " * Pre processing of data\n",
    " * Building a sequence-to-sequence RNN\n",
    " \n",
    "The principle of sequence-to-sequence modelling is shown in the figure below\n",
    "![seq2seq unrolled](utils/images/seq2seq.png)\n",
    "In your case, an english sentence is input to the encoder network, one word at each time step. After the sentence has been fed, there will now be a representation of the entire sentence encoded into the hidden state of the RNN. The hidden state of the decoder network is thereafter initialized with this representation and tries to generate the sequence of spanish words from the initial hidden state.  \n",
    "\n",
    "Run the cell below to import the necessary packages to get started with the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "effc1f6c16ac094eaf6f8ab899102549",
     "grade": false,
     "grade_id": "cell-245a4886ba97957a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import unidecode\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from utils.tests.ha2Tests import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4aac6ec95a53779286e1b12d2b17c369",
     "grade": false,
     "grade_id": "cell-b4d9ff24542835da",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.1 Pre-processing <a class=\"anchor\" id=\"2.1\"></a>\n",
    "The dataset to use consists of bilingual sentence pairs. Each sentence in the list of english sentences has a corresponding spanish translation at the same index in the list of spanish sentences.  \n",
    "\n",
    "Basic preprocessing has already been performed such as:  \n",
    " * to lower case\n",
    " * remove any padded spaces at the start and end of the row (trim)\n",
    " * convert from unicode character set to the ASCII character set. Example: á --> a and ñ --> n.\n",
    " * If the character is a question mark, punctuation or exclamation mark: ? ! ., put a space to the left of the character to make the char into a separate word.\n",
    " * remove any non-letter character\n",
    " * split each line into a tuple, where the left element is the source language sentence and the right element is the target language sentence\n",
    " * remove any sentence pair with number of words more than 5\n",
    "\n",
    "#### 2.1.1 Loading and inspecting the data set <a class=\"anchor\" id=\"2.1.1\"></a>\n",
    "Lets first load the english-spanish sentence pairs into memory and inspect some samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ecb81ee7fcde8806c59c1a7ac4079a08",
     "grade": false,
     "grade_id": "cell-3082498e5d14fae7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 114282 sentence pairs\n",
      "The shortest english sentence is 2 words\n",
      "The shortest spanish sentence is 2 words\n",
      "The longest english sentence is 5 words\n",
      "The longest spanish sentence is 5 words\n",
      "\n",
      "10 Random short sentence pairs:\n",
      "iron is hard . \t el hierro es duro .\n",
      "breathe in deeply . \t respira profundamente .\n",
      "i could kill you . \t lo podria matar .\n",
      "everything is ready . \t todo esta listo .\n",
      "are you a lawyer ? \t eres abogado ?\n",
      "ill send flowers . \t voy a mandar flores .\n",
      "ironic isnt it ? \t ironico verdad ?\n",
      "he broke the rules . \t el rompio las normas .\n",
      "we never use sugar . \t no usamos nunca azucar .\n",
      "im your friend . \t soy tu amigo .\n"
     ]
    }
   ],
   "source": [
    "# load data set\n",
    "lines = open('utils/spa.txt', 'r', encoding='UTF-8').read().strip().split('\\n')\n",
    "data = pickle.load( open( \"utils/spa-preprocessed.pkl\", \"rb\" ) )\n",
    "spa, eng = data['spa'], data['eng']\n",
    "SEQ_MAX_LEN = 5\n",
    "\n",
    "print('There are %i sentence pairs' % len(lines))\n",
    "print('The shortest english sentence is %i words' \n",
    "      % np.min(list(map(lambda line: len(line.split(' ')), eng))))\n",
    "print('The shortest spanish sentence is %i words' \n",
    "      % np.min(list(map(lambda line: len(line.split(' ')), spa))))\n",
    "print('The longest english sentence is %i words' \n",
    "      % np.max(list(map(lambda line: len(line.split(' ')), eng))))\n",
    "print('The longest spanish sentence is %i words' \n",
    "      % np.max(list(map(lambda line: len(line.split(' ')), spa))))\n",
    "print('\\n10 Random short sentence pairs:')\n",
    "for i in range(10):\n",
    "    ix = np.random.choice(int(len(eng)))\n",
    "    print(eng[ix],'\\t',spa[ix])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6e9b6db781a32688bf0e80b7c3fed9c3",
     "grade": false,
     "grade_id": "cell-1d324eeeaec95b0d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### 2.1.4 Word-to-index and index-to-word conversions <a class=\"anchor\" id=\"2.1.4\"></a>\n",
    "Just like in Task 1 earlier, you need dictionaries to transform between (in this case) the words and unique number representations. The following dictionaries/lists are implemented:  \n",
    "\n",
    " * `eng_ix_to_word` - a python list where each index is a unique number that represents the english word stored as value.\n",
    "                      `eng_ix_to_word[0]` should equal to `ZERO`\n",
    "                      `enx_ix_to_word[-1]` (the last element) should equal to `UNK` (unknown)\n",
    " * `eng_word_to_ix` - like `eng_ix_to_word` but reversed, a dictionary mapping every word into the unique number. The key is a word that also exists in `eng_ix_to_word` and the value is an integer, that matches the index of the word in `eng_ix_to_word`\n",
    " * `spa_ix_to_word` - same principle as `eng_ix_to_word`, but from the vocabulary of `spa`\n",
    "                      `spa_ix_to_word[0]` should equal to `ZERO`\n",
    "                      `spa_ix_to_word[-1]` (the last element) should equal to `UNK` (unknown)\n",
    " * `spa_word_to_ix` - same principle as `eng_word_to_ix`, but from the vocabulary of `spa`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c0a5cc008e4b29824644f5553837a564",
     "grade": false,
     "grade_id": "cell-a459812ea1439340",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "eng_vocab = set([word for sentence in eng for word in sentence.split(' ')])\n",
    "eng_ix_to_word = ['ZERO'] + list(eng_vocab) + ['UNK']\n",
    "eng_word_to_ix =  { word: index for index, word in enumerate(eng_ix_to_word)}\n",
    "spa_vocab = set([word for sentence in spa for word in sentence.split(' ')])\n",
    "spa_ix_to_word = ['ZERO'] + list(spa_vocab) + ['UNK']\n",
    "spa_word_to_ix =  { word: index for index, word in enumerate(spa_ix_to_word)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "57904094aa594914b948bcb405a6e14b",
     "grade": false,
     "grade_id": "cell-5cfb758da1a8770a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now we convert the words of `eng` and `spa` into their corresponding index numbers  \n",
    "\n",
    "Run the cell below to initialize these variables:  \n",
    " * X - A list of sentences. Each sentence is represented by a list where the i'th element is the index number representing the i'th word in the sentence string.\n",
    " * Y - same principle as `X`, but contains the converted spanish sentences  \n",
    " \n",
    " Example:\n",
    " if `eng_word_to_ix` is defined as `{'good':0, 'morning':1}`  \n",
    " and `eng` is defined as `[\"good morning\", \"morning\"]`  \n",
    " then `X` should result in `[[0,1],[1]]`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a63e9c0f34e4859e94fc06460e1ae328",
     "grade": false,
     "grade_id": "cell-41597a7b019841d9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "X = [ [eng_word_to_ix[word] for word in sentence.split(' ')] for sentence in eng]\n",
    "Y = [ [spa_word_to_ix[word] for word in sentence.split(' ')] for sentence in spa]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0f8a26208e3cccbe6d665d6fc45284ca",
     "grade": false,
     "grade_id": "cell-e87e7a013d284386",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### 2.1.5 Padding <a class=\"anchor\" id=\"2.1.5\"></a>\n",
    "Since the sentences are of variable lengths, padding is needed to make every sentence equal length. Every sentence list in `X` and `Y` are padded with zeros to make every list equal to the length of the maximum sentence length. The zeros are **prepended** before the first word.  \n",
    "\n",
    "Example:  \n",
    "if `X` is defined as `[[1,2,3], [1,2], [1]]`   \n",
    "then after applying padding, X is equal to `[[1,2,3], [0,1,2], [0,0,1]]`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "59c6a6079e6a56f0097e1a25a392841a",
     "grade": false,
     "grade_id": "cell-ff60390bb7636a51",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "X = pad_sequences(X, maxlen=SEQ_MAX_LEN, dtype='int32')\n",
    "Y = pad_sequences(Y, maxlen=SEQ_MAX_LEN, dtype='int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e0635f1fe84873fa1e26026b667fde9f",
     "grade": false,
     "grade_id": "cell-502981a32a6c5b6f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### 2.1.6 One-hot labels <a class=\"anchor\" id=\"2.1.6\"></a>\n",
    "The last pre processing converting the data into one-hot vectors.  \n",
    "\n",
    "For this task, **only** the labels (spanish words) are going to be transformed into one-hot vectors. The data points of english words are only going to be transformed into their corresponding numeric values by `eng_word_to_ix` because `Embedding` from the keras API already implements converting the inputs to one-hot vectors.  \n",
    "\n",
    "Transform `Y` to have the following properties:\n",
    " * define `Y` as a numpy matrix of shape (number of sentences, number of words of longest sentence, one-hot vector length). \n",
    " * dimension 0 represents each sentence (batch).\n",
    " * dimension 1 represents each word in a sentece (padded with zeros for equal length)\n",
    " * dimension 2 represents the one-hot vector for that particular word in that particular sentence. To pass the test, a onehot representation of word `1023` should be a zero vector with its 1023th index set to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "578991fcbd3cff75d1b64387f0349fec",
     "grade": true,
     "grade_id": "cell-1afbd922885825dd",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tip: the shape of Y should be: (22947, 5, 8805)\n"
     ]
    }
   ],
   "source": [
    "# TODO: convert each index number in Y into its corresponding one-hot vector\n",
    "num_output = len(spa_word_to_ix)\n",
    "print(\"Tip: the shape of Y should be: (%i, %i, %i)\" % (len(Y), len(Y[0]), num_output))\n",
    "\n",
    "def Y_to_onehot(Y, num_output):\n",
    "    # Complete function according to description above\n",
    "    # YOUR CODE HERE\n",
    "    from keras.utils import to_categorical\n",
    "    Y_ = []\n",
    "    for y in Y:\n",
    "        encoded = to_categorical(y, num_output)\n",
    "        Y_.append(encoded)\n",
    "    return np.array(Y_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4d7554a570e89bddeb80b6b5d3bcee68",
     "grade": false,
     "grade_id": "cell-4b7e350b4705b7fd",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Run the following cell to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "403577ab8a2dd3aad46eca2ec0499148",
     "grade": false,
     "grade_id": "cell-4ca82916bbb49405",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test passed\n"
     ]
    }
   ],
   "source": [
    "# test case \n",
    "test_Y_to_onehot(Y_to_onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "da368670f710fd04e2151601f977e521",
     "grade": false,
     "grade_id": "cell-ce0b6f9695d8e625",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "When you passed the test, run the cell below to convert the sentences of Y into lists of onehot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a66db52162f451acbc5f7c62afbc18aa",
     "grade": false,
     "grade_id": "cell-fd9b28bb297d5dde",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "Y_onehot = Y_to_onehot(Y, num_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "58a8b6ccd149f8ae93f59795efd4176d",
     "grade": false,
     "grade_id": "cell-c5a0ba0c96903a79",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.2 Implementing a sequence-to-sequence model <a class=\"anchor\" id=\"2.2\"></a>\n",
    "In this section you will define, train and evaluate a sequence-to-sequence RNN architecture with the help of the Keras API.  \n",
    "\n",
    "#### 2.2.1 Defining the architecture <a class=\"anchor\" id=\"2.2.1\"></a>\n",
    "\n",
    "Now you are going to define the architecture for the translator. There are several possible architectures for performing translation, which is a sequence-to-sequence task (an input sequence, the English sentence, mapped to an output sequence, the Spanish sentence). If you take inspiration from searching the internet, you might come across neural machine translators such as [Google's Neural Machine Translation System](http://tsong.me/blog/google-nmt/). This kind of deep architecture was trained for a week using 100 GPUs, which is completely out of reach for the purposes of this assignment. Instead we will ask you to implement a simpler architecture that, even though it doesn't possess all the important parts of a translation network, still performs acceptably.\n",
    "\n",
    "The architecture you are required to implement will closely resemble the image shown at the start of this task. First the input sequence is fed to an encoder network, which is in charge of summarizing the entire sequence in its internal memory. This internal memory is then fed to a decoder network, which uses this summarized content to produce the output sequence. Both the encoder and the decoder networks will be comprised of a single LSTM unit, with a memory cell of arbitrary dimension. Since we won't stack LSTM layers, this can be seen as a \"toy\" version of a complete LSTM architecure.\n",
    "\n",
    "#### Practical remarks\n",
    "\n",
    "Note that the input sequence for each training sentence is a vector with 5 elements, where each element corresponds to the number of that particular word in the vocabulary. \n",
    "\n",
    "Because similar word indexes don't correspond to similar words (e.g. index 133 corresponds to 'joke', whereas index 134 corresponds to 'silence'), it's easier to train our network if we map this data to a space where the semantic meaning of the word is related to its geometric position. For instance, we would like the word 'car' to be closer to the word 'bus' than it is to 'tree'. This can be accomplished with the `Embedding` layer module from Keras, which tries to find a suitable representation for positive integers in a higher-dimensional space by using dimensionality reduction techniques in the matrix of co-ocurrence statistics of our input data. This layer can use an already trained mapping (like Word2Vec), but in this exercise we'll train it ourselves.\n",
    "\n",
    "Besides deciding how to encode the input, there is another important practical remark to be done. Since the decoder network will also be implemented as an LSTM, it will require inputs. According to what was described earlier, the only important data for this network is the memory cell at each time-step. Unfortunately, there is no simple way to create an LSTM cell without inputs (you can, of course, customize the LSTM layer to not receive any inputs), so we'll simply feed zeros as input at each time-step and it's likely that the optimization procedure will tune the decoder weights to disregard this input eventually (because it's completely uncorrelated to the input and the label, so it doesn't help with the prediction at all).\n",
    "\n",
    "#### Keras tips\n",
    "\n",
    "- It's necessary to use the [Model class API](https://keras.io/models/model/) for implementing this model (because it has two inputs, the input sequence and the zeros to the decoder). Be sure to familiarize yourself with this way of defining models in Keras.\n",
    "- You can define an LSTM cell using the [LSTM layer](https://keras.io/layers/recurrent/#lstm).\n",
    "- The LSTM layer by default outputs only the output for the last time-step, which is great for the encoder part, since we don't care about its outputs anyway. However, for the decoder network we want the outputs at all time steps. To accomplish this, you can set the argument `return_sequences` to `True` when creating the LSTM layer.\n",
    "- You will need to feed the memory cell content of the encoder network at the last time-step to the decoder network. You can ask the LSTM layer to also output its memory cell contents by setting the `return_state` argument to `True` when creating the layer (this makes the LSTM layer output three things: the actual LSTM output and two variables with different parts of the LSTM internal memory cell).\n",
    "- You can specify the initial state of LSTM layers symbolically by calling them with the keyword argument  `initial_state`. The value of `initial_state` should be a tensor or list of tensors representing the initial state of the LSTM layer.\n",
    "\n",
    "\n",
    "Now, implement your sequence-to-sequence architecture in the function `create_model` by following the descriptions of the function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "0d52b0195834f0be18715584290ff3e7",
     "grade": true,
     "grade_id": "cell-38f1aa584c956f28",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Add your import statements here\n",
    "from keras import Input, Model\n",
    "from keras.layers import Activation, TimeDistributed, Dense, RepeatVector, Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "\n",
    "def create_model(input_n, X_seq_len, output_n, Y_seq_len, hidden_dim, embedding_dim):\n",
    "    \"\"\" Define a keras sequence-to-sequence model. \n",
    "    \n",
    "    Arguments:\n",
    "    input_n - integer, the number of inputs for the network (the length of a one-hot vector from `X`)\n",
    "    X_seq_len - integer, the length of a sequence from `X`. Should be constant and you made sure by using padding\n",
    "    output_n - integer, the number of outputs for the network (the length of a one-hot vector from `Y`)\n",
    "    Y_seq_len - integer, the length of a sequence from `Y`. Should be constant and you made sure by using padding\n",
    "    hidden_dim - integer, number of units in the LSTM's memory cell.\n",
    "    embedding_dim - output dimension of the embedding layer.\n",
    "    \n",
    "    Returns:\n",
    "    The compiled keras model\n",
    "    \n",
    "    \"\"\"\n",
    "    # Input and embedding layers\n",
    "    encoder_input_layer = Input(shape=[X_seq_len])\n",
    "    encoder_embedding_layer = Embedding(input_n, embedding_dim, input_length=X_seq_len, mask_zero=True)(encoder_input_layer)\n",
    "    \n",
    "    # Create the encoder LSTM.\n",
    "    # YOUR CODE HERE\n",
    "    encoder_outputs, state_h, state_c  = LSTM(hidden_dim, return_state=True)(encoder_embedding_layer)\n",
    "    encoder_states = [state_h, state_c]\n",
    "    \n",
    "    # Null input (should be fed with zeros) and repeating it the same number of times as there are words in the \n",
    "    # target sentence\n",
    "    null_input = Input(shape=[1])\n",
    "    repeated_null = RepeatVector(Y_seq_len)(null_input)\n",
    "    \n",
    "    # Create the decoder LSTM (feed it with the memory of the encoder network). Call it `decoder_layer`\n",
    "    # YOUR CODE HERE\n",
    "    decoder_layer, _, _ = LSTM(hidden_dim, return_sequences=True, return_state=True)(repeated_null,initial_state=encoder_states)\n",
    "    \n",
    "    # Add a fully connected layer and a softmax to the outputs of the decoder\n",
    "    decoder_fully_connected = TimeDistributed(Dense(output_n))(decoder_layer)\n",
    "    decoder_softmax = Activation('softmax')(decoder_fully_connected)\n",
    "    \n",
    "    # Create final model and compile it\n",
    "    model = Model([encoder_input_layer, null_input], decoder_softmax)\n",
    "    \n",
    "    # Compile the model. Use a loss function, optimizer, and metrics of your choice\n",
    "    # YOUR CODE HERE\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Add these arguments to the model for convenience\n",
    "    model.hidden_dim = hidden_dim\n",
    "    model.embedding_dim = embedding_dim\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7069ad2bd986c2fa40a6df328637a021",
     "grade": false,
     "grade_id": "cell-330f1845a4caf93a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### 2.2.2 Training the model <a class=\"anchor\" id=\"2.2.2\"></a>\n",
    "Now create the model and train it. Try out different hyper-parameters if you're not satisfied with the result. For obtaining a good speedup using the GPU, opt for large number of memory cells and large batch sizes (although not too large, that also has drawbacks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "0d27df11c0d2a35548069e5b2adb14f5",
     "grade": true,
     "grade_id": "cell-8784b37d2b07cbfc",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18357 samples, validate on 4590 samples\n",
      "Epoch 1/200\n",
      "18357/18357 [==============================] - ETA: 7:50 - loss: 9.0831 - acc: 0.0000e+0 - ETA: 3:57 - loss: 9.0817 - acc: 0.1316    - ETA: 2:39 - loss: 9.0802 - acc: 0.188 - ETA: 2:00 - loss: 9.0784 - acc: 0.221 - ETA: 1:37 - loss: 9.0762 - acc: 0.244 - ETA: 1:22 - loss: 9.0734 - acc: 0.259 - ETA: 1:10 - loss: 9.0700 - acc: 0.268 - ETA: 1:02 - loss: 9.0655 - acc: 0.275 - ETA: 55s - loss: 9.0596 - acc: 0.280 - ETA: 50s - loss: 9.0511 - acc: 0.28 - ETA: 46s - loss: 9.0405 - acc: 0.28 - ETA: 42s - loss: 9.0252 - acc: 0.28 - ETA: 39s - loss: 9.0028 - acc: 0.29 - ETA: 37s - loss: 8.9728 - acc: 0.29 - ETA: 34s - loss: 8.9299 - acc: 0.29 - ETA: 32s - loss: 8.8739 - acc: 0.29 - ETA: 30s - loss: 8.8013 - acc: 0.29 - ETA: 28s - loss: 8.7147 - acc: 0.28 - ETA: 27s - loss: 8.6165 - acc: 0.27 - ETA: 25s - loss: 8.5089 - acc: 0.27 - ETA: 24s - loss: 8.3976 - acc: 0.26 - ETA: 23s - loss: 8.2855 - acc: 0.26 - ETA: 22s - loss: 8.1735 - acc: 0.26 - ETA: 21s - loss: 8.0704 - acc: 0.27 - ETA: 20s - loss: 7.9705 - acc: 0.27 - ETA: 19s - loss: 7.8736 - acc: 0.27 - ETA: 18s - loss: 7.7862 - acc: 0.27 - ETA: 17s - loss: 7.7022 - acc: 0.27 - ETA: 17s - loss: 7.6249 - acc: 0.27 - ETA: 16s - loss: 7.5531 - acc: 0.28 - ETA: 15s - loss: 7.4889 - acc: 0.28 - ETA: 15s - loss: 7.4249 - acc: 0.28 - ETA: 14s - loss: 7.3655 - acc: 0.28 - ETA: 13s - loss: 7.3107 - acc: 0.28 - ETA: 13s - loss: 7.2566 - acc: 0.28 - ETA: 12s - loss: 7.2086 - acc: 0.28 - ETA: 12s - loss: 7.1615 - acc: 0.28 - ETA: 11s - loss: 7.1157 - acc: 0.28 - ETA: 11s - loss: 7.0723 - acc: 0.28 - ETA: 10s - loss: 7.0304 - acc: 0.28 - ETA: 10s - loss: 6.9913 - acc: 0.28 - ETA: 9s - loss: 6.9514 - acc: 0.2867 - ETA: 9s - loss: 6.9159 - acc: 0.287 - ETA: 8s - loss: 6.8809 - acc: 0.287 - ETA: 8s - loss: 6.8492 - acc: 0.287 - ETA: 7s - loss: 6.8151 - acc: 0.288 - ETA: 7s - loss: 6.7838 - acc: 0.288 - ETA: 7s - loss: 6.7540 - acc: 0.288 - ETA: 6s - loss: 6.7254 - acc: 0.289 - ETA: 6s - loss: 6.6979 - acc: 0.289 - ETA: 6s - loss: 6.6701 - acc: 0.289 - ETA: 5s - loss: 6.6436 - acc: 0.290 - ETA: 5s - loss: 6.6168 - acc: 0.290 - ETA: 5s - loss: 6.5922 - acc: 0.290 - ETA: 4s - loss: 6.5699 - acc: 0.290 - ETA: 4s - loss: 6.5451 - acc: 0.291 - ETA: 4s - loss: 6.5225 - acc: 0.291 - ETA: 3s - loss: 6.4983 - acc: 0.292 - ETA: 3s - loss: 6.4764 - acc: 0.292 - ETA: 3s - loss: 6.4550 - acc: 0.293 - ETA: 2s - loss: 6.4351 - acc: 0.293 - ETA: 2s - loss: 6.4153 - acc: 0.293 - ETA: 2s - loss: 6.3972 - acc: 0.294 - ETA: 2s - loss: 6.3777 - acc: 0.294 - ETA: 1s - loss: 6.3582 - acc: 0.295 - ETA: 1s - loss: 6.3381 - acc: 0.295 - ETA: 1s - loss: 6.3206 - acc: 0.296 - ETA: 0s - loss: 6.3030 - acc: 0.296 - ETA: 0s - loss: 6.2858 - acc: 0.296 - ETA: 0s - loss: 6.2692 - acc: 0.297 - ETA: 0s - loss: 6.2532 - acc: 0.297 - 21s 1ms/step - loss: 6.2416 - acc: 0.2975 - val_loss: 5.1243 - val_acc: 0.3251\n",
      "Epoch 2/200\n",
      "18357/18357 [==============================] - ETA: 10s - loss: 4.9763 - acc: 0.33 - ETA: 10s - loss: 4.9531 - acc: 0.32 - ETA: 10s - loss: 4.9252 - acc: 0.33 - ETA: 11s - loss: 4.9473 - acc: 0.32 - ETA: 11s - loss: 4.9706 - acc: 0.32 - ETA: 10s - loss: 4.9803 - acc: 0.32 - ETA: 10s - loss: 4.9774 - acc: 0.32 - ETA: 10s - loss: 4.9697 - acc: 0.32 - ETA: 9s - loss: 4.9682 - acc: 0.3261 - ETA: 9s - loss: 4.9726 - acc: 0.325 - ETA: 9s - loss: 4.9747 - acc: 0.324 - ETA: 9s - loss: 4.9730 - acc: 0.325 - ETA: 9s - loss: 4.9772 - acc: 0.324 - ETA: 9s - loss: 4.9787 - acc: 0.323 - ETA: 9s - loss: 4.9810 - acc: 0.323 - ETA: 8s - loss: 4.9822 - acc: 0.322 - ETA: 8s - loss: 4.9882 - acc: 0.321 - ETA: 8s - loss: 4.9853 - acc: 0.322 - ETA: 8s - loss: 4.9861 - acc: 0.321 - ETA: 8s - loss: 4.9899 - acc: 0.321 - ETA: 7s - loss: 4.9904 - acc: 0.321 - ETA: 7s - loss: 4.9929 - acc: 0.321 - ETA: 7s - loss: 4.9935 - acc: 0.320 - ETA: 7s - loss: 4.9861 - acc: 0.321 - ETA: 7s - loss: 4.9860 - acc: 0.321 - ETA: 7s - loss: 4.9887 - acc: 0.320 - ETA: 6s - loss: 4.9903 - acc: 0.320 - ETA: 6s - loss: 4.9874 - acc: 0.320 - ETA: 6s - loss: 4.9858 - acc: 0.320 - ETA: 6s - loss: 4.9865 - acc: 0.320 - ETA: 6s - loss: 4.9867 - acc: 0.320 - ETA: 6s - loss: 4.9824 - acc: 0.321 - ETA: 5s - loss: 4.9812 - acc: 0.321 - ETA: 5s - loss: 4.9798 - acc: 0.321 - ETA: 5s - loss: 4.9775 - acc: 0.321 - ETA: 5s - loss: 4.9791 - acc: 0.321 - ETA: 5s - loss: 4.9809 - acc: 0.320 - ETA: 5s - loss: 4.9817 - acc: 0.320 - ETA: 5s - loss: 4.9777 - acc: 0.320 - ETA: 4s - loss: 4.9763 - acc: 0.321 - ETA: 4s - loss: 4.9743 - acc: 0.321 - ETA: 4s - loss: 4.9723 - acc: 0.321 - ETA: 4s - loss: 4.9713 - acc: 0.321 - ETA: 4s - loss: 4.9679 - acc: 0.321 - ETA: 4s - loss: 4.9670 - acc: 0.321 - ETA: 3s - loss: 4.9666 - acc: 0.321 - ETA: 3s - loss: 4.9674 - acc: 0.321 - ETA: 3s - loss: 4.9690 - acc: 0.321 - ETA: 3s - loss: 4.9699 - acc: 0.321 - ETA: 3s - loss: 4.9700 - acc: 0.321 - ETA: 3s - loss: 4.9689 - acc: 0.321 - ETA: 3s - loss: 4.9697 - acc: 0.320 - ETA: 2s - loss: 4.9695 - acc: 0.321 - ETA: 2s - loss: 4.9706 - acc: 0.320 - ETA: 2s - loss: 4.9696 - acc: 0.321 - ETA: 2s - loss: 4.9688 - acc: 0.321 - ETA: 2s - loss: 4.9695 - acc: 0.320 - ETA: 2s - loss: 4.9717 - acc: 0.320 - ETA: 1s - loss: 4.9716 - acc: 0.320 - ETA: 1s - loss: 4.9705 - acc: 0.321 - ETA: 1s - loss: 4.9707 - acc: 0.321 - ETA: 1s - loss: 4.9685 - acc: 0.321 - ETA: 1s - loss: 4.9681 - acc: 0.321 - ETA: 1s - loss: 4.9673 - acc: 0.321 - ETA: 1s - loss: 4.9681 - acc: 0.321 - ETA: 0s - loss: 4.9680 - acc: 0.321 - ETA: 0s - loss: 4.9668 - acc: 0.321 - ETA: 0s - loss: 4.9655 - acc: 0.321 - ETA: 0s - loss: 4.9654 - acc: 0.321 - ETA: 0s - loss: 4.9642 - acc: 0.321 - ETA: 0s - loss: 4.9639 - acc: 0.321 - 13s 710us/step - loss: 4.9649 - acc: 0.3210 - val_loss: 4.9856 - val_acc: 0.3283\n",
      "Epoch 3/200\n",
      "18357/18357 [==============================] - ETA: 11s - loss: 4.8299 - acc: 0.31 - ETA: 10s - loss: 4.7901 - acc: 0.32 - ETA: 10s - loss: 4.7935 - acc: 0.32 - ETA: 11s - loss: 4.7928 - acc: 0.32 - ETA: 11s - loss: 4.7994 - acc: 0.32 - ETA: 11s - loss: 4.8042 - acc: 0.32 - ETA: 11s - loss: 4.7982 - acc: 0.32 - ETA: 11s - loss: 4.8024 - acc: 0.32 - ETA: 10s - loss: 4.8070 - acc: 0.32 - ETA: 10s - loss: 4.8017 - acc: 0.32 - ETA: 10s - loss: 4.8033 - acc: 0.32 - ETA: 9s - loss: 4.8100 - acc: 0.3219 - ETA: 9s - loss: 4.8123 - acc: 0.320 - ETA: 9s - loss: 4.8055 - acc: 0.322 - ETA: 9s - loss: 4.8031 - acc: 0.321 - ETA: 8s - loss: 4.8002 - acc: 0.321 - ETA: 8s - loss: 4.7963 - acc: 0.321 - ETA: 8s - loss: 4.7976 - acc: 0.321 - ETA: 8s - loss: 4.7964 - acc: 0.322 - ETA: 8s - loss: 4.7922 - acc: 0.322 - ETA: 8s - loss: 4.7922 - acc: 0.322 - ETA: 7s - loss: 4.7981 - acc: 0.321 - ETA: 7s - loss: 4.7976 - acc: 0.322 - ETA: 7s - loss: 4.7977 - acc: 0.321 - ETA: 7s - loss: 4.7971 - acc: 0.321 - ETA: 7s - loss: 4.7962 - acc: 0.322 - ETA: 6s - loss: 4.7978 - acc: 0.321 - ETA: 6s - loss: 4.7949 - acc: 0.322 - ETA: 6s - loss: 4.7899 - acc: 0.323 - ETA: 6s - loss: 4.7886 - acc: 0.323 - ETA: 6s - loss: 4.7891 - acc: 0.323 - ETA: 6s - loss: 4.7887 - acc: 0.323 - ETA: 5s - loss: 4.7889 - acc: 0.323 - ETA: 5s - loss: 4.7858 - acc: 0.323 - ETA: 5s - loss: 4.7875 - acc: 0.322 - ETA: 5s - loss: 4.7851 - acc: 0.323 - ETA: 5s - loss: 4.7824 - acc: 0.323 - ETA: 5s - loss: 4.7823 - acc: 0.323 - ETA: 5s - loss: 4.7808 - acc: 0.323 - ETA: 4s - loss: 4.7809 - acc: 0.324 - ETA: 4s - loss: 4.7807 - acc: 0.323 - ETA: 4s - loss: 4.7811 - acc: 0.323 - ETA: 4s - loss: 4.7802 - acc: 0.323 - ETA: 4s - loss: 4.7795 - acc: 0.323 - ETA: 4s - loss: 4.7788 - acc: 0.323 - ETA: 3s - loss: 4.7792 - acc: 0.323 - ETA: 3s - loss: 4.7797 - acc: 0.323 - ETA: 3s - loss: 4.7777 - acc: 0.324 - ETA: 3s - loss: 4.7795 - acc: 0.324 - ETA: 3s - loss: 4.7776 - acc: 0.324 - ETA: 3s - loss: 4.7754 - acc: 0.324 - ETA: 3s - loss: 4.7751 - acc: 0.324 - ETA: 2s - loss: 4.7747 - acc: 0.324 - ETA: 2s - loss: 4.7714 - acc: 0.325 - ETA: 2s - loss: 4.7697 - acc: 0.325 - ETA: 2s - loss: 4.7689 - acc: 0.325 - ETA: 2s - loss: 4.7675 - acc: 0.325 - ETA: 2s - loss: 4.7666 - acc: 0.325 - ETA: 1s - loss: 4.7673 - acc: 0.325 - ETA: 1s - loss: 4.7647 - acc: 0.325 - ETA: 1s - loss: 4.7625 - acc: 0.325 - ETA: 1s - loss: 4.7613 - acc: 0.325 - ETA: 1s - loss: 4.7593 - acc: 0.325 - ETA: 1s - loss: 4.7575 - acc: 0.326 - ETA: 1s - loss: 4.7571 - acc: 0.326 - ETA: 0s - loss: 4.7551 - acc: 0.326 - ETA: 0s - loss: 4.7529 - acc: 0.326 - ETA: 0s - loss: 4.7515 - acc: 0.327 - ETA: 0s - loss: 4.7494 - acc: 0.327 - ETA: 0s - loss: 4.7477 - acc: 0.327 - ETA: 0s - loss: 4.7459 - acc: 0.327 - 13s 698us/step - loss: 4.7466 - acc: 0.3276 - val_loss: 4.7868 - val_acc: 0.3464\n",
      "Epoch 4/200\n",
      "18357/18357 [==============================] - ETA: 13s - loss: 4.5675 - acc: 0.33 - ETA: 12s - loss: 4.5787 - acc: 0.33 - ETA: 11s - loss: 4.5570 - acc: 0.34 - ETA: 11s - loss: 4.5730 - acc: 0.33 - ETA: 10s - loss: 4.5597 - acc: 0.34 - ETA: 10s - loss: 4.5475 - acc: 0.34 - ETA: 10s - loss: 4.5345 - acc: 0.34 - ETA: 10s - loss: 4.5377 - acc: 0.34 - ETA: 10s - loss: 4.5565 - acc: 0.33 - ETA: 10s - loss: 4.5595 - acc: 0.33 - ETA: 9s - loss: 4.5662 - acc: 0.3387 - ETA: 9s - loss: 4.5653 - acc: 0.338 - ETA: 9s - loss: 4.5663 - acc: 0.338 - ETA: 9s - loss: 4.5621 - acc: 0.339 - ETA: 9s - loss: 4.5612 - acc: 0.340 - ETA: 8s - loss: 4.5617 - acc: 0.340 - ETA: 8s - loss: 4.5672 - acc: 0.339 - ETA: 8s - loss: 4.5617 - acc: 0.340 - ETA: 8s - loss: 4.5608 - acc: 0.340 - ETA: 8s - loss: 4.5587 - acc: 0.341 - ETA: 8s - loss: 4.5592 - acc: 0.342 - ETA: 7s - loss: 4.5620 - acc: 0.342 - ETA: 7s - loss: 4.5638 - acc: 0.342 - ETA: 7s - loss: 4.5634 - acc: 0.342 - ETA: 7s - loss: 4.5602 - acc: 0.343 - ETA: 7s - loss: 4.5578 - acc: 0.343 - ETA: 7s - loss: 4.5601 - acc: 0.342 - ETA: 6s - loss: 4.5563 - acc: 0.342 - ETA: 6s - loss: 4.5551 - acc: 0.342 - ETA: 6s - loss: 4.5578 - acc: 0.342 - ETA: 6s - loss: 4.5609 - acc: 0.342 - ETA: 6s - loss: 4.5612 - acc: 0.342 - ETA: 6s - loss: 4.5632 - acc: 0.341 - ETA: 5s - loss: 4.5651 - acc: 0.341 - ETA: 5s - loss: 4.5658 - acc: 0.341 - ETA: 5s - loss: 4.5660 - acc: 0.341 - ETA: 5s - loss: 4.5688 - acc: 0.340 - ETA: 5s - loss: 4.5710 - acc: 0.340 - ETA: 5s - loss: 4.5719 - acc: 0.340 - ETA: 4s - loss: 4.5726 - acc: 0.340 - ETA: 4s - loss: 4.5732 - acc: 0.340 - ETA: 4s - loss: 4.5721 - acc: 0.340 - ETA: 4s - loss: 4.5719 - acc: 0.340 - ETA: 4s - loss: 4.5725 - acc: 0.340 - ETA: 4s - loss: 4.5738 - acc: 0.340 - ETA: 3s - loss: 4.5747 - acc: 0.340 - ETA: 3s - loss: 4.5744 - acc: 0.340 - ETA: 3s - loss: 4.5738 - acc: 0.340 - ETA: 3s - loss: 4.5739 - acc: 0.340 - ETA: 3s - loss: 4.5740 - acc: 0.340 - ETA: 3s - loss: 4.5713 - acc: 0.341 - ETA: 2s - loss: 4.5706 - acc: 0.341 - ETA: 2s - loss: 4.5716 - acc: 0.340 - ETA: 2s - loss: 4.5696 - acc: 0.341 - ETA: 2s - loss: 4.5701 - acc: 0.341 - ETA: 2s - loss: 4.5695 - acc: 0.340 - ETA: 2s - loss: 4.5688 - acc: 0.341 - ETA: 2s - loss: 4.5672 - acc: 0.341 - ETA: 1s - loss: 4.5654 - acc: 0.341 - ETA: 1s - loss: 4.5672 - acc: 0.341 - ETA: 1s - loss: 4.5661 - acc: 0.341 - ETA: 1s - loss: 4.5646 - acc: 0.341 - ETA: 1s - loss: 4.5649 - acc: 0.341 - ETA: 1s - loss: 4.5645 - acc: 0.341 - ETA: 1s - loss: 4.5624 - acc: 0.341 - ETA: 0s - loss: 4.5630 - acc: 0.341 - ETA: 0s - loss: 4.5630 - acc: 0.341 - ETA: 0s - loss: 4.5634 - acc: 0.340 - ETA: 0s - loss: 4.5641 - acc: 0.340 - ETA: 0s - loss: 4.5633 - acc: 0.340 - ETA: 0s - loss: 4.5622 - acc: 0.341 - 12s 676us/step - loss: 4.5621 - acc: 0.3412 - val_loss: 4.6712 - val_acc: 0.3511\n",
      "Epoch 5/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18357/18357 [==============================] - ETA: 10s - loss: 4.3869 - acc: 0.33 - ETA: 9s - loss: 4.3936 - acc: 0.3418 - ETA: 9s - loss: 4.3994 - acc: 0.345 - ETA: 9s - loss: 4.4164 - acc: 0.344 - ETA: 9s - loss: 4.4338 - acc: 0.342 - ETA: 9s - loss: 4.4261 - acc: 0.342 - ETA: 9s - loss: 4.4259 - acc: 0.343 - ETA: 9s - loss: 4.4248 - acc: 0.342 - ETA: 9s - loss: 4.4283 - acc: 0.340 - ETA: 9s - loss: 4.4189 - acc: 0.342 - ETA: 8s - loss: 4.4177 - acc: 0.343 - ETA: 8s - loss: 4.4121 - acc: 0.345 - ETA: 8s - loss: 4.4174 - acc: 0.345 - ETA: 8s - loss: 4.4217 - acc: 0.344 - ETA: 8s - loss: 4.4177 - acc: 0.344 - ETA: 8s - loss: 4.4164 - acc: 0.345 - ETA: 8s - loss: 4.4212 - acc: 0.345 - ETA: 7s - loss: 4.4192 - acc: 0.346 - ETA: 7s - loss: 4.4197 - acc: 0.346 - ETA: 7s - loss: 4.4210 - acc: 0.346 - ETA: 7s - loss: 4.4264 - acc: 0.346 - ETA: 7s - loss: 4.4242 - acc: 0.346 - ETA: 7s - loss: 4.4241 - acc: 0.346 - ETA: 7s - loss: 4.4191 - acc: 0.347 - ETA: 6s - loss: 4.4160 - acc: 0.347 - ETA: 6s - loss: 4.4167 - acc: 0.346 - ETA: 6s - loss: 4.4175 - acc: 0.346 - ETA: 6s - loss: 4.4154 - acc: 0.346 - ETA: 6s - loss: 4.4168 - acc: 0.346 - ETA: 6s - loss: 4.4191 - acc: 0.346 - ETA: 5s - loss: 4.4224 - acc: 0.345 - ETA: 5s - loss: 4.4219 - acc: 0.345 - ETA: 5s - loss: 4.4254 - acc: 0.345 - ETA: 5s - loss: 4.4277 - acc: 0.345 - ETA: 5s - loss: 4.4280 - acc: 0.345 - ETA: 5s - loss: 4.4286 - acc: 0.345 - ETA: 5s - loss: 4.4286 - acc: 0.345 - ETA: 4s - loss: 4.4274 - acc: 0.345 - ETA: 4s - loss: 4.4268 - acc: 0.345 - ETA: 4s - loss: 4.4267 - acc: 0.345 - ETA: 4s - loss: 4.4289 - acc: 0.345 - ETA: 4s - loss: 4.4281 - acc: 0.345 - ETA: 4s - loss: 4.4293 - acc: 0.345 - ETA: 4s - loss: 4.4283 - acc: 0.345 - ETA: 3s - loss: 4.4244 - acc: 0.345 - ETA: 3s - loss: 4.4255 - acc: 0.345 - ETA: 3s - loss: 4.4255 - acc: 0.345 - ETA: 3s - loss: 4.4240 - acc: 0.345 - ETA: 3s - loss: 4.4255 - acc: 0.345 - ETA: 3s - loss: 4.4263 - acc: 0.345 - ETA: 3s - loss: 4.4291 - acc: 0.345 - ETA: 2s - loss: 4.4299 - acc: 0.345 - ETA: 2s - loss: 4.4306 - acc: 0.345 - ETA: 2s - loss: 4.4310 - acc: 0.345 - ETA: 2s - loss: 4.4315 - acc: 0.345 - ETA: 2s - loss: 4.4328 - acc: 0.345 - ETA: 2s - loss: 4.4331 - acc: 0.345 - ETA: 2s - loss: 4.4332 - acc: 0.345 - ETA: 1s - loss: 4.4335 - acc: 0.345 - ETA: 1s - loss: 4.4322 - acc: 0.345 - ETA: 1s - loss: 4.4328 - acc: 0.345 - ETA: 1s - loss: 4.4314 - acc: 0.346 - ETA: 1s - loss: 4.4306 - acc: 0.346 - ETA: 1s - loss: 4.4302 - acc: 0.345 - ETA: 1s - loss: 4.4310 - acc: 0.345 - ETA: 0s - loss: 4.4314 - acc: 0.345 - ETA: 0s - loss: 4.4301 - acc: 0.345 - ETA: 0s - loss: 4.4314 - acc: 0.345 - ETA: 0s - loss: 4.4311 - acc: 0.345 - ETA: 0s - loss: 4.4294 - acc: 0.346 - ETA: 0s - loss: 4.4291 - acc: 0.346 - 12s 669us/step - loss: 4.4289 - acc: 0.3460 - val_loss: 4.5802 - val_acc: 0.3538\n",
      "Epoch 6/200\n",
      "18357/18357 [==============================] - ETA: 9s - loss: 4.3800 - acc: 0.346 - ETA: 9s - loss: 4.3190 - acc: 0.351 - ETA: 9s - loss: 4.3228 - acc: 0.346 - ETA: 9s - loss: 4.3150 - acc: 0.346 - ETA: 9s - loss: 4.3106 - acc: 0.346 - ETA: 9s - loss: 4.3282 - acc: 0.343 - ETA: 9s - loss: 4.3258 - acc: 0.344 - ETA: 9s - loss: 4.3260 - acc: 0.343 - ETA: 8s - loss: 4.3203 - acc: 0.343 - ETA: 8s - loss: 4.3164 - acc: 0.344 - ETA: 8s - loss: 4.3119 - acc: 0.346 - ETA: 8s - loss: 4.3172 - acc: 0.345 - ETA: 8s - loss: 4.3179 - acc: 0.345 - ETA: 8s - loss: 4.3201 - acc: 0.345 - ETA: 8s - loss: 4.3169 - acc: 0.346 - ETA: 7s - loss: 4.3132 - acc: 0.346 - ETA: 7s - loss: 4.3114 - acc: 0.347 - ETA: 7s - loss: 4.3094 - acc: 0.347 - ETA: 7s - loss: 4.3069 - acc: 0.347 - ETA: 7s - loss: 4.3041 - acc: 0.348 - ETA: 7s - loss: 4.3036 - acc: 0.348 - ETA: 7s - loss: 4.3024 - acc: 0.348 - ETA: 6s - loss: 4.3016 - acc: 0.348 - ETA: 6s - loss: 4.3018 - acc: 0.348 - ETA: 6s - loss: 4.3041 - acc: 0.349 - ETA: 6s - loss: 4.3033 - acc: 0.349 - ETA: 6s - loss: 4.3039 - acc: 0.350 - ETA: 6s - loss: 4.3027 - acc: 0.351 - ETA: 6s - loss: 4.3027 - acc: 0.351 - ETA: 5s - loss: 4.3042 - acc: 0.351 - ETA: 5s - loss: 4.3073 - acc: 0.351 - ETA: 5s - loss: 4.3050 - acc: 0.351 - ETA: 5s - loss: 4.3054 - acc: 0.351 - ETA: 5s - loss: 4.3071 - acc: 0.351 - ETA: 5s - loss: 4.3061 - acc: 0.351 - ETA: 5s - loss: 4.3096 - acc: 0.351 - ETA: 4s - loss: 4.3111 - acc: 0.350 - ETA: 4s - loss: 4.3093 - acc: 0.351 - ETA: 4s - loss: 4.3116 - acc: 0.350 - ETA: 4s - loss: 4.3149 - acc: 0.350 - ETA: 4s - loss: 4.3158 - acc: 0.350 - ETA: 4s - loss: 4.3133 - acc: 0.351 - ETA: 4s - loss: 4.3118 - acc: 0.351 - ETA: 3s - loss: 4.3110 - acc: 0.352 - ETA: 3s - loss: 4.3108 - acc: 0.352 - ETA: 3s - loss: 4.3094 - acc: 0.352 - ETA: 3s - loss: 4.3078 - acc: 0.353 - ETA: 3s - loss: 4.3065 - acc: 0.353 - ETA: 3s - loss: 4.3061 - acc: 0.353 - ETA: 3s - loss: 4.3064 - acc: 0.353 - ETA: 2s - loss: 4.3084 - acc: 0.352 - ETA: 2s - loss: 4.3071 - acc: 0.353 - ETA: 2s - loss: 4.3073 - acc: 0.353 - ETA: 2s - loss: 4.3072 - acc: 0.353 - ETA: 2s - loss: 4.3084 - acc: 0.353 - ETA: 2s - loss: 4.3076 - acc: 0.353 - ETA: 2s - loss: 4.3058 - acc: 0.354 - ETA: 1s - loss: 4.3056 - acc: 0.354 - ETA: 1s - loss: 4.3052 - acc: 0.354 - ETA: 1s - loss: 4.3054 - acc: 0.354 - ETA: 1s - loss: 4.3052 - acc: 0.354 - ETA: 1s - loss: 4.3055 - acc: 0.355 - ETA: 1s - loss: 4.3055 - acc: 0.355 - ETA: 1s - loss: 4.3075 - acc: 0.355 - ETA: 0s - loss: 4.3082 - acc: 0.355 - ETA: 0s - loss: 4.3073 - acc: 0.356 - ETA: 0s - loss: 4.3069 - acc: 0.356 - ETA: 0s - loss: 4.3060 - acc: 0.356 - ETA: 0s - loss: 4.3064 - acc: 0.356 - ETA: 0s - loss: 4.3044 - acc: 0.357 - ETA: 0s - loss: 4.3039 - acc: 0.357 - 12s 650us/step - loss: 4.3039 - acc: 0.3576 - val_loss: 4.4787 - val_acc: 0.3777\n",
      "Epoch 7/200\n",
      "18357/18357 [==============================] - ETA: 9s - loss: 4.2510 - acc: 0.366 - ETA: 9s - loss: 4.2173 - acc: 0.366 - ETA: 9s - loss: 4.1854 - acc: 0.370 - ETA: 9s - loss: 4.1844 - acc: 0.372 - ETA: 9s - loss: 4.1729 - acc: 0.374 - ETA: 9s - loss: 4.1495 - acc: 0.377 - ETA: 9s - loss: 4.1678 - acc: 0.375 - ETA: 8s - loss: 4.1602 - acc: 0.377 - ETA: 8s - loss: 4.1558 - acc: 0.377 - ETA: 8s - loss: 4.1508 - acc: 0.379 - ETA: 8s - loss: 4.1475 - acc: 0.381 - ETA: 8s - loss: 4.1497 - acc: 0.381 - ETA: 8s - loss: 4.1448 - acc: 0.382 - ETA: 8s - loss: 4.1465 - acc: 0.382 - ETA: 8s - loss: 4.1435 - acc: 0.383 - ETA: 8s - loss: 4.1464 - acc: 0.382 - ETA: 8s - loss: 4.1522 - acc: 0.381 - ETA: 8s - loss: 4.1491 - acc: 0.382 - ETA: 8s - loss: 4.1574 - acc: 0.381 - ETA: 8s - loss: 4.1584 - acc: 0.381 - ETA: 7s - loss: 4.1554 - acc: 0.382 - ETA: 7s - loss: 4.1568 - acc: 0.382 - ETA: 7s - loss: 4.1563 - acc: 0.383 - ETA: 7s - loss: 4.1539 - acc: 0.383 - ETA: 7s - loss: 4.1568 - acc: 0.383 - ETA: 7s - loss: 4.1588 - acc: 0.382 - ETA: 7s - loss: 4.1571 - acc: 0.383 - ETA: 7s - loss: 4.1590 - acc: 0.383 - ETA: 6s - loss: 4.1565 - acc: 0.383 - ETA: 6s - loss: 4.1581 - acc: 0.383 - ETA: 6s - loss: 4.1549 - acc: 0.384 - ETA: 6s - loss: 4.1547 - acc: 0.384 - ETA: 6s - loss: 4.1530 - acc: 0.385 - ETA: 6s - loss: 4.1505 - acc: 0.385 - ETA: 5s - loss: 4.1504 - acc: 0.385 - ETA: 5s - loss: 4.1528 - acc: 0.385 - ETA: 5s - loss: 4.1581 - acc: 0.385 - ETA: 5s - loss: 4.1568 - acc: 0.385 - ETA: 5s - loss: 4.1577 - acc: 0.385 - ETA: 5s - loss: 4.1586 - acc: 0.385 - ETA: 4s - loss: 4.1606 - acc: 0.385 - ETA: 4s - loss: 4.1617 - acc: 0.385 - ETA: 4s - loss: 4.1602 - acc: 0.385 - ETA: 4s - loss: 4.1595 - acc: 0.386 - ETA: 4s - loss: 4.1621 - acc: 0.386 - ETA: 4s - loss: 4.1612 - acc: 0.386 - ETA: 4s - loss: 4.1638 - acc: 0.386 - ETA: 3s - loss: 4.1644 - acc: 0.386 - ETA: 3s - loss: 4.1642 - acc: 0.386 - ETA: 3s - loss: 4.1644 - acc: 0.386 - ETA: 3s - loss: 4.1661 - acc: 0.386 - ETA: 3s - loss: 4.1668 - acc: 0.386 - ETA: 3s - loss: 4.1654 - acc: 0.386 - ETA: 2s - loss: 4.1660 - acc: 0.386 - ETA: 2s - loss: 4.1683 - acc: 0.386 - ETA: 2s - loss: 4.1690 - acc: 0.386 - ETA: 2s - loss: 4.1697 - acc: 0.386 - ETA: 2s - loss: 4.1673 - acc: 0.386 - ETA: 2s - loss: 4.1677 - acc: 0.386 - ETA: 1s - loss: 4.1678 - acc: 0.386 - ETA: 1s - loss: 4.1678 - acc: 0.386 - ETA: 1s - loss: 4.1658 - acc: 0.387 - ETA: 1s - loss: 4.1678 - acc: 0.386 - ETA: 1s - loss: 4.1657 - acc: 0.387 - ETA: 1s - loss: 4.1657 - acc: 0.387 - ETA: 0s - loss: 4.1658 - acc: 0.387 - ETA: 0s - loss: 4.1660 - acc: 0.387 - ETA: 0s - loss: 4.1655 - acc: 0.387 - ETA: 0s - loss: 4.1666 - acc: 0.387 - ETA: 0s - loss: 4.1672 - acc: 0.387 - ETA: 0s - loss: 4.1667 - acc: 0.387 - 13s 721us/step - loss: 4.1674 - acc: 0.3877 - val_loss: 4.3684 - val_acc: 0.3991\n",
      "Epoch 8/200\n",
      "18357/18357 [==============================] - ETA: 10s - loss: 3.8918 - acc: 0.41 - ETA: 9s - loss: 3.9504 - acc: 0.4051 - ETA: 9s - loss: 3.9777 - acc: 0.403 - ETA: 9s - loss: 3.9459 - acc: 0.406 - ETA: 9s - loss: 3.9543 - acc: 0.406 - ETA: 9s - loss: 3.9602 - acc: 0.403 - ETA: 9s - loss: 3.9781 - acc: 0.401 - ETA: 9s - loss: 3.9973 - acc: 0.398 - ETA: 9s - loss: 4.0010 - acc: 0.400 - ETA: 9s - loss: 4.0086 - acc: 0.400 - ETA: 9s - loss: 4.0090 - acc: 0.400 - ETA: 8s - loss: 4.0082 - acc: 0.401 - ETA: 8s - loss: 4.0092 - acc: 0.402 - ETA: 8s - loss: 4.0033 - acc: 0.403 - ETA: 8s - loss: 3.9995 - acc: 0.404 - ETA: 8s - loss: 3.9987 - acc: 0.405 - ETA: 8s - loss: 3.9979 - acc: 0.404 - ETA: 8s - loss: 3.9979 - acc: 0.404 - ETA: 7s - loss: 4.0030 - acc: 0.403 - ETA: 7s - loss: 4.0032 - acc: 0.404 - ETA: 7s - loss: 4.0033 - acc: 0.404 - ETA: 7s - loss: 4.0020 - acc: 0.405 - ETA: 7s - loss: 4.0071 - acc: 0.403 - ETA: 7s - loss: 4.0086 - acc: 0.404 - ETA: 6s - loss: 4.0128 - acc: 0.404 - ETA: 6s - loss: 4.0147 - acc: 0.404 - ETA: 6s - loss: 4.0134 - acc: 0.404 - ETA: 6s - loss: 4.0101 - acc: 0.405 - ETA: 6s - loss: 4.0122 - acc: 0.404 - ETA: 6s - loss: 4.0116 - acc: 0.405 - ETA: 6s - loss: 4.0136 - acc: 0.405 - ETA: 5s - loss: 4.0127 - acc: 0.405 - ETA: 5s - loss: 4.0141 - acc: 0.404 - ETA: 5s - loss: 4.0125 - acc: 0.405 - ETA: 5s - loss: 4.0128 - acc: 0.405 - ETA: 5s - loss: 4.0124 - acc: 0.405 - ETA: 5s - loss: 4.0105 - acc: 0.406 - ETA: 5s - loss: 4.0122 - acc: 0.406 - ETA: 4s - loss: 4.0101 - acc: 0.406 - ETA: 4s - loss: 4.0126 - acc: 0.406 - ETA: 4s - loss: 4.0146 - acc: 0.405 - ETA: 4s - loss: 4.0143 - acc: 0.405 - ETA: 4s - loss: 4.0163 - acc: 0.406 - ETA: 4s - loss: 4.0167 - acc: 0.406 - ETA: 3s - loss: 4.0156 - acc: 0.406 - ETA: 3s - loss: 4.0167 - acc: 0.406 - ETA: 3s - loss: 4.0176 - acc: 0.406 - ETA: 3s - loss: 4.0183 - acc: 0.406 - ETA: 3s - loss: 4.0176 - acc: 0.405 - ETA: 3s - loss: 4.0178 - acc: 0.405 - ETA: 3s - loss: 4.0187 - acc: 0.405 - ETA: 2s - loss: 4.0169 - acc: 0.405 - ETA: 2s - loss: 4.0183 - acc: 0.405 - ETA: 2s - loss: 4.0175 - acc: 0.405 - ETA: 2s - loss: 4.0171 - acc: 0.406 - ETA: 2s - loss: 4.0171 - acc: 0.406 - ETA: 2s - loss: 4.0175 - acc: 0.406 - ETA: 2s - loss: 4.0166 - acc: 0.406 - ETA: 1s - loss: 4.0151 - acc: 0.406 - ETA: 1s - loss: 4.0136 - acc: 0.406 - ETA: 1s - loss: 4.0119 - acc: 0.407 - ETA: 1s - loss: 4.0132 - acc: 0.407 - ETA: 1s - loss: 4.0135 - acc: 0.407 - ETA: 1s - loss: 4.0141 - acc: 0.407 - ETA: 1s - loss: 4.0146 - acc: 0.406 - ETA: 0s - loss: 4.0143 - acc: 0.407 - ETA: 0s - loss: 4.0130 - acc: 0.407 - ETA: 0s - loss: 4.0130 - acc: 0.407 - ETA: 0s - loss: 4.0123 - acc: 0.407 - ETA: 0s - loss: 4.0129 - acc: 0.407 - ETA: 0s - loss: 4.0139 - acc: 0.407 - 12s 680us/step - loss: 4.0144 - acc: 0.4072 - val_loss: 4.2327 - val_acc: 0.4131\n",
      "Epoch 9/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18357/18357 [==============================] - ETA: 9s - loss: 3.8177 - acc: 0.421 - ETA: 9s - loss: 3.7846 - acc: 0.429 - ETA: 9s - loss: 3.7963 - acc: 0.426 - ETA: 9s - loss: 3.8030 - acc: 0.425 - ETA: 9s - loss: 3.8203 - acc: 0.424 - ETA: 9s - loss: 3.8485 - acc: 0.421 - ETA: 9s - loss: 3.8562 - acc: 0.421 - ETA: 9s - loss: 3.8474 - acc: 0.421 - ETA: 9s - loss: 3.8487 - acc: 0.422 - ETA: 9s - loss: 3.8612 - acc: 0.420 - ETA: 8s - loss: 3.8599 - acc: 0.421 - ETA: 8s - loss: 3.8484 - acc: 0.422 - ETA: 8s - loss: 3.8478 - acc: 0.423 - ETA: 8s - loss: 3.8548 - acc: 0.422 - ETA: 8s - loss: 3.8478 - acc: 0.423 - ETA: 8s - loss: 3.8483 - acc: 0.422 - ETA: 8s - loss: 3.8515 - acc: 0.423 - ETA: 7s - loss: 3.8480 - acc: 0.424 - ETA: 7s - loss: 3.8479 - acc: 0.424 - ETA: 7s - loss: 3.8441 - acc: 0.424 - ETA: 7s - loss: 3.8411 - acc: 0.425 - ETA: 7s - loss: 3.8439 - acc: 0.425 - ETA: 7s - loss: 3.8450 - acc: 0.425 - ETA: 6s - loss: 3.8476 - acc: 0.424 - ETA: 6s - loss: 3.8471 - acc: 0.424 - ETA: 6s - loss: 3.8530 - acc: 0.425 - ETA: 6s - loss: 3.8536 - acc: 0.424 - ETA: 6s - loss: 3.8533 - acc: 0.424 - ETA: 6s - loss: 3.8555 - acc: 0.424 - ETA: 6s - loss: 3.8575 - acc: 0.423 - ETA: 6s - loss: 3.8597 - acc: 0.423 - ETA: 5s - loss: 3.8619 - acc: 0.422 - ETA: 5s - loss: 3.8614 - acc: 0.422 - ETA: 5s - loss: 3.8625 - acc: 0.422 - ETA: 5s - loss: 3.8604 - acc: 0.423 - ETA: 5s - loss: 3.8610 - acc: 0.423 - ETA: 5s - loss: 3.8597 - acc: 0.423 - ETA: 5s - loss: 3.8631 - acc: 0.423 - ETA: 5s - loss: 3.8648 - acc: 0.423 - ETA: 4s - loss: 3.8636 - acc: 0.423 - ETA: 4s - loss: 3.8618 - acc: 0.423 - ETA: 4s - loss: 3.8602 - acc: 0.423 - ETA: 4s - loss: 3.8603 - acc: 0.423 - ETA: 4s - loss: 3.8571 - acc: 0.423 - ETA: 4s - loss: 3.8571 - acc: 0.423 - ETA: 3s - loss: 3.8555 - acc: 0.423 - ETA: 3s - loss: 3.8554 - acc: 0.424 - ETA: 3s - loss: 3.8542 - acc: 0.424 - ETA: 3s - loss: 3.8554 - acc: 0.424 - ETA: 3s - loss: 3.8557 - acc: 0.424 - ETA: 3s - loss: 3.8592 - acc: 0.423 - ETA: 3s - loss: 3.8595 - acc: 0.423 - ETA: 2s - loss: 3.8582 - acc: 0.423 - ETA: 2s - loss: 3.8577 - acc: 0.423 - ETA: 2s - loss: 3.8579 - acc: 0.423 - ETA: 2s - loss: 3.8584 - acc: 0.423 - ETA: 2s - loss: 3.8580 - acc: 0.423 - ETA: 2s - loss: 3.8583 - acc: 0.423 - ETA: 1s - loss: 3.8580 - acc: 0.423 - ETA: 1s - loss: 3.8588 - acc: 0.423 - ETA: 1s - loss: 3.8579 - acc: 0.423 - ETA: 1s - loss: 3.8555 - acc: 0.423 - ETA: 1s - loss: 3.8536 - acc: 0.424 - ETA: 1s - loss: 3.8524 - acc: 0.424 - ETA: 1s - loss: 3.8505 - acc: 0.424 - ETA: 0s - loss: 3.8501 - acc: 0.424 - ETA: 0s - loss: 3.8491 - acc: 0.424 - ETA: 0s - loss: 3.8505 - acc: 0.424 - ETA: 0s - loss: 3.8504 - acc: 0.424 - ETA: 0s - loss: 3.8493 - acc: 0.424 - ETA: 0s - loss: 3.8473 - acc: 0.425 - 13s 683us/step - loss: 3.8457 - acc: 0.4251 - val_loss: 4.1198 - val_acc: 0.4257\n",
      "Epoch 10/200\n",
      "18357/18357 [==============================] - ETA: 9s - loss: 3.6783 - acc: 0.442 - ETA: 9s - loss: 3.7106 - acc: 0.439 - ETA: 9s - loss: 3.7230 - acc: 0.437 - ETA: 9s - loss: 3.7006 - acc: 0.437 - ETA: 9s - loss: 3.6869 - acc: 0.438 - ETA: 9s - loss: 3.6741 - acc: 0.441 - ETA: 9s - loss: 3.6825 - acc: 0.439 - ETA: 9s - loss: 3.6741 - acc: 0.439 - ETA: 8s - loss: 3.6869 - acc: 0.438 - ETA: 8s - loss: 3.6808 - acc: 0.439 - ETA: 8s - loss: 3.6999 - acc: 0.437 - ETA: 8s - loss: 3.7091 - acc: 0.436 - ETA: 8s - loss: 3.7073 - acc: 0.437 - ETA: 8s - loss: 3.6961 - acc: 0.438 - ETA: 8s - loss: 3.6945 - acc: 0.439 - ETA: 7s - loss: 3.7010 - acc: 0.438 - ETA: 7s - loss: 3.6978 - acc: 0.439 - ETA: 7s - loss: 3.6982 - acc: 0.438 - ETA: 7s - loss: 3.7015 - acc: 0.438 - ETA: 7s - loss: 3.7033 - acc: 0.438 - ETA: 7s - loss: 3.6980 - acc: 0.439 - ETA: 7s - loss: 3.6949 - acc: 0.439 - ETA: 6s - loss: 3.6935 - acc: 0.439 - ETA: 6s - loss: 3.6893 - acc: 0.440 - ETA: 6s - loss: 3.6896 - acc: 0.440 - ETA: 6s - loss: 3.6869 - acc: 0.440 - ETA: 6s - loss: 3.6878 - acc: 0.440 - ETA: 6s - loss: 3.6911 - acc: 0.439 - ETA: 6s - loss: 3.6948 - acc: 0.439 - ETA: 6s - loss: 3.6952 - acc: 0.439 - ETA: 5s - loss: 3.6924 - acc: 0.439 - ETA: 5s - loss: 3.6897 - acc: 0.440 - ETA: 5s - loss: 3.6878 - acc: 0.440 - ETA: 5s - loss: 3.6879 - acc: 0.440 - ETA: 5s - loss: 3.6876 - acc: 0.440 - ETA: 5s - loss: 3.6855 - acc: 0.440 - ETA: 5s - loss: 3.6862 - acc: 0.440 - ETA: 4s - loss: 3.6874 - acc: 0.440 - ETA: 4s - loss: 3.6885 - acc: 0.439 - ETA: 4s - loss: 3.6890 - acc: 0.439 - ETA: 4s - loss: 3.6908 - acc: 0.439 - ETA: 4s - loss: 3.6913 - acc: 0.439 - ETA: 4s - loss: 3.6924 - acc: 0.439 - ETA: 4s - loss: 3.6939 - acc: 0.439 - ETA: 3s - loss: 3.6922 - acc: 0.439 - ETA: 3s - loss: 3.6920 - acc: 0.439 - ETA: 3s - loss: 3.6922 - acc: 0.439 - ETA: 3s - loss: 3.6896 - acc: 0.439 - ETA: 3s - loss: 3.6882 - acc: 0.439 - ETA: 3s - loss: 3.6869 - acc: 0.440 - ETA: 3s - loss: 3.6846 - acc: 0.440 - ETA: 2s - loss: 3.6854 - acc: 0.440 - ETA: 2s - loss: 3.6842 - acc: 0.440 - ETA: 2s - loss: 3.6833 - acc: 0.441 - ETA: 2s - loss: 3.6830 - acc: 0.441 - ETA: 2s - loss: 3.6824 - acc: 0.441 - ETA: 2s - loss: 3.6819 - acc: 0.441 - ETA: 2s - loss: 3.6832 - acc: 0.441 - ETA: 1s - loss: 3.6839 - acc: 0.441 - ETA: 1s - loss: 3.6842 - acc: 0.440 - ETA: 1s - loss: 3.6838 - acc: 0.441 - ETA: 1s - loss: 3.6833 - acc: 0.441 - ETA: 1s - loss: 3.6835 - acc: 0.441 - ETA: 1s - loss: 3.6842 - acc: 0.441 - ETA: 0s - loss: 3.6829 - acc: 0.441 - ETA: 0s - loss: 3.6820 - acc: 0.441 - ETA: 0s - loss: 3.6822 - acc: 0.441 - ETA: 0s - loss: 3.6820 - acc: 0.441 - ETA: 0s - loss: 3.6817 - acc: 0.441 - ETA: 0s - loss: 3.6807 - acc: 0.441 - ETA: 0s - loss: 3.6806 - acc: 0.441 - 12s 667us/step - loss: 3.6801 - acc: 0.4419 - val_loss: 3.9998 - val_acc: 0.4393\n",
      "Epoch 11/200\n",
      "18357/18357 [==============================] - ETA: 9s - loss: 3.5963 - acc: 0.449 - ETA: 9s - loss: 3.4928 - acc: 0.456 - ETA: 9s - loss: 3.4735 - acc: 0.458 - ETA: 9s - loss: 3.4580 - acc: 0.461 - ETA: 9s - loss: 3.4829 - acc: 0.459 - ETA: 9s - loss: 3.4858 - acc: 0.460 - ETA: 9s - loss: 3.5023 - acc: 0.456 - ETA: 8s - loss: 3.5107 - acc: 0.455 - ETA: 8s - loss: 3.4984 - acc: 0.457 - ETA: 8s - loss: 3.5000 - acc: 0.457 - ETA: 8s - loss: 3.5027 - acc: 0.456 - ETA: 8s - loss: 3.5017 - acc: 0.455 - ETA: 8s - loss: 3.5126 - acc: 0.454 - ETA: 8s - loss: 3.5104 - acc: 0.454 - ETA: 8s - loss: 3.5100 - acc: 0.454 - ETA: 8s - loss: 3.5112 - acc: 0.454 - ETA: 7s - loss: 3.5157 - acc: 0.454 - ETA: 7s - loss: 3.5167 - acc: 0.453 - ETA: 7s - loss: 3.5182 - acc: 0.453 - ETA: 7s - loss: 3.5150 - acc: 0.454 - ETA: 7s - loss: 3.5164 - acc: 0.454 - ETA: 7s - loss: 3.5161 - acc: 0.455 - ETA: 7s - loss: 3.5155 - acc: 0.455 - ETA: 6s - loss: 3.5119 - acc: 0.456 - ETA: 6s - loss: 3.5114 - acc: 0.456 - ETA: 6s - loss: 3.5147 - acc: 0.456 - ETA: 6s - loss: 3.5150 - acc: 0.456 - ETA: 6s - loss: 3.5172 - acc: 0.456 - ETA: 6s - loss: 3.5211 - acc: 0.455 - ETA: 6s - loss: 3.5189 - acc: 0.456 - ETA: 5s - loss: 3.5193 - acc: 0.456 - ETA: 5s - loss: 3.5181 - acc: 0.456 - ETA: 5s - loss: 3.5215 - acc: 0.456 - ETA: 5s - loss: 3.5220 - acc: 0.456 - ETA: 5s - loss: 3.5194 - acc: 0.457 - ETA: 5s - loss: 3.5190 - acc: 0.457 - ETA: 5s - loss: 3.5236 - acc: 0.457 - ETA: 4s - loss: 3.5226 - acc: 0.457 - ETA: 4s - loss: 3.5233 - acc: 0.457 - ETA: 4s - loss: 3.5217 - acc: 0.457 - ETA: 4s - loss: 3.5221 - acc: 0.458 - ETA: 4s - loss: 3.5217 - acc: 0.458 - ETA: 4s - loss: 3.5220 - acc: 0.457 - ETA: 4s - loss: 3.5245 - acc: 0.457 - ETA: 3s - loss: 3.5243 - acc: 0.457 - ETA: 3s - loss: 3.5258 - acc: 0.457 - ETA: 3s - loss: 3.5245 - acc: 0.458 - ETA: 3s - loss: 3.5276 - acc: 0.458 - ETA: 3s - loss: 3.5277 - acc: 0.458 - ETA: 3s - loss: 3.5275 - acc: 0.458 - ETA: 3s - loss: 3.5263 - acc: 0.458 - ETA: 2s - loss: 3.5257 - acc: 0.459 - ETA: 2s - loss: 3.5267 - acc: 0.459 - ETA: 2s - loss: 3.5258 - acc: 0.459 - ETA: 2s - loss: 3.5238 - acc: 0.459 - ETA: 2s - loss: 3.5228 - acc: 0.459 - ETA: 2s - loss: 3.5206 - acc: 0.460 - ETA: 2s - loss: 3.5213 - acc: 0.460 - ETA: 1s - loss: 3.5203 - acc: 0.460 - ETA: 1s - loss: 3.5199 - acc: 0.460 - ETA: 1s - loss: 3.5169 - acc: 0.460 - ETA: 1s - loss: 3.5147 - acc: 0.461 - ETA: 1s - loss: 3.5150 - acc: 0.460 - ETA: 1s - loss: 3.5149 - acc: 0.461 - ETA: 0s - loss: 3.5152 - acc: 0.461 - ETA: 0s - loss: 3.5141 - acc: 0.461 - ETA: 0s - loss: 3.5143 - acc: 0.461 - ETA: 0s - loss: 3.5135 - acc: 0.461 - ETA: 0s - loss: 3.5122 - acc: 0.461 - ETA: 0s - loss: 3.5122 - acc: 0.461 - ETA: 0s - loss: 3.5116 - acc: 0.461 - 12s 655us/step - loss: 3.5110 - acc: 0.4619 - val_loss: 3.8786 - val_acc: 0.4548\n",
      "Epoch 12/200\n",
      "18357/18357 [==============================] - ETA: 9s - loss: 3.3899 - acc: 0.464 - ETA: 9s - loss: 3.3527 - acc: 0.470 - ETA: 9s - loss: 3.3499 - acc: 0.468 - ETA: 9s - loss: 3.3472 - acc: 0.472 - ETA: 9s - loss: 3.3608 - acc: 0.472 - ETA: 9s - loss: 3.3609 - acc: 0.471 - ETA: 9s - loss: 3.3605 - acc: 0.473 - ETA: 9s - loss: 3.3471 - acc: 0.474 - ETA: 8s - loss: 3.3440 - acc: 0.477 - ETA: 8s - loss: 3.3415 - acc: 0.477 - ETA: 8s - loss: 3.3422 - acc: 0.478 - ETA: 8s - loss: 3.3482 - acc: 0.477 - ETA: 8s - loss: 3.3506 - acc: 0.477 - ETA: 8s - loss: 3.3505 - acc: 0.477 - ETA: 8s - loss: 3.3545 - acc: 0.477 - ETA: 8s - loss: 3.3610 - acc: 0.476 - ETA: 7s - loss: 3.3625 - acc: 0.476 - ETA: 7s - loss: 3.3653 - acc: 0.475 - ETA: 7s - loss: 3.3652 - acc: 0.475 - ETA: 7s - loss: 3.3627 - acc: 0.475 - ETA: 7s - loss: 3.3571 - acc: 0.476 - ETA: 7s - loss: 3.3516 - acc: 0.477 - ETA: 7s - loss: 3.3496 - acc: 0.477 - ETA: 6s - loss: 3.3536 - acc: 0.477 - ETA: 6s - loss: 3.3570 - acc: 0.476 - ETA: 6s - loss: 3.3602 - acc: 0.476 - ETA: 6s - loss: 3.3607 - acc: 0.477 - ETA: 6s - loss: 3.3583 - acc: 0.477 - ETA: 6s - loss: 3.3567 - acc: 0.477 - ETA: 6s - loss: 3.3554 - acc: 0.477 - ETA: 5s - loss: 3.3546 - acc: 0.478 - ETA: 5s - loss: 3.3559 - acc: 0.478 - ETA: 5s - loss: 3.3574 - acc: 0.478 - ETA: 5s - loss: 3.3541 - acc: 0.478 - ETA: 5s - loss: 3.3553 - acc: 0.478 - ETA: 5s - loss: 3.3515 - acc: 0.478 - ETA: 5s - loss: 3.3500 - acc: 0.479 - ETA: 4s - loss: 3.3504 - acc: 0.479 - ETA: 4s - loss: 3.3468 - acc: 0.479 - ETA: 4s - loss: 3.3475 - acc: 0.479 - ETA: 4s - loss: 3.3451 - acc: 0.479 - ETA: 4s - loss: 3.3421 - acc: 0.480 - ETA: 4s - loss: 3.3428 - acc: 0.480 - ETA: 4s - loss: 3.3409 - acc: 0.480 - ETA: 3s - loss: 3.3416 - acc: 0.480 - ETA: 3s - loss: 3.3420 - acc: 0.480 - ETA: 3s - loss: 3.3406 - acc: 0.481 - ETA: 3s - loss: 3.3404 - acc: 0.481 - ETA: 3s - loss: 3.3395 - acc: 0.481 - ETA: 3s - loss: 3.3381 - acc: 0.481 - ETA: 3s - loss: 3.3393 - acc: 0.481 - ETA: 2s - loss: 3.3400 - acc: 0.481 - ETA: 2s - loss: 3.3395 - acc: 0.481 - ETA: 2s - loss: 3.3394 - acc: 0.481 - ETA: 2s - loss: 3.3376 - acc: 0.481 - ETA: 2s - loss: 3.3390 - acc: 0.481 - ETA: 2s - loss: 3.3379 - acc: 0.481 - ETA: 1s - loss: 3.3379 - acc: 0.481 - ETA: 1s - loss: 3.3369 - acc: 0.481 - ETA: 1s - loss: 3.3341 - acc: 0.482 - ETA: 1s - loss: 3.3349 - acc: 0.482 - ETA: 1s - loss: 3.3352 - acc: 0.482 - ETA: 1s - loss: 3.3338 - acc: 0.482 - ETA: 1s - loss: 3.3345 - acc: 0.482 - ETA: 0s - loss: 3.3342 - acc: 0.482 - ETA: 0s - loss: 3.3345 - acc: 0.482 - ETA: 0s - loss: 3.3347 - acc: 0.482 - ETA: 0s - loss: 3.3322 - acc: 0.483 - ETA: 0s - loss: 3.3329 - acc: 0.483 - ETA: 0s - loss: 3.3350 - acc: 0.482 - ETA: 0s - loss: 3.3356 - acc: 0.482 - 12s 656us/step - loss: 3.3361 - acc: 0.4828 - val_loss: 3.7559 - val_acc: 0.4694\n",
      "Epoch 13/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18357/18357 [==============================] - ETA: 10s - loss: 3.1361 - acc: 0.49 - ETA: 9s - loss: 3.0986 - acc: 0.5059 - ETA: 9s - loss: 3.1245 - acc: 0.502 - ETA: 9s - loss: 3.1026 - acc: 0.504 - ETA: 9s - loss: 3.0934 - acc: 0.506 - ETA: 9s - loss: 3.0976 - acc: 0.505 - ETA: 9s - loss: 3.1149 - acc: 0.500 - ETA: 9s - loss: 3.1118 - acc: 0.503 - ETA: 8s - loss: 3.1127 - acc: 0.504 - ETA: 8s - loss: 3.1160 - acc: 0.504 - ETA: 8s - loss: 3.1142 - acc: 0.504 - ETA: 8s - loss: 3.1221 - acc: 0.502 - ETA: 8s - loss: 3.1278 - acc: 0.501 - ETA: 8s - loss: 3.1314 - acc: 0.501 - ETA: 8s - loss: 3.1386 - acc: 0.501 - ETA: 8s - loss: 3.1351 - acc: 0.501 - ETA: 7s - loss: 3.1360 - acc: 0.501 - ETA: 7s - loss: 3.1410 - acc: 0.501 - ETA: 7s - loss: 3.1405 - acc: 0.501 - ETA: 7s - loss: 3.1444 - acc: 0.500 - ETA: 7s - loss: 3.1480 - acc: 0.500 - ETA: 7s - loss: 3.1503 - acc: 0.500 - ETA: 7s - loss: 3.1465 - acc: 0.500 - ETA: 6s - loss: 3.1467 - acc: 0.500 - ETA: 6s - loss: 3.1507 - acc: 0.499 - ETA: 6s - loss: 3.1498 - acc: 0.499 - ETA: 6s - loss: 3.1493 - acc: 0.499 - ETA: 6s - loss: 3.1540 - acc: 0.499 - ETA: 6s - loss: 3.1604 - acc: 0.498 - ETA: 6s - loss: 3.1601 - acc: 0.499 - ETA: 5s - loss: 3.1628 - acc: 0.498 - ETA: 5s - loss: 3.1625 - acc: 0.499 - ETA: 5s - loss: 3.1615 - acc: 0.499 - ETA: 5s - loss: 3.1580 - acc: 0.499 - ETA: 5s - loss: 3.1583 - acc: 0.499 - ETA: 5s - loss: 3.1587 - acc: 0.500 - ETA: 5s - loss: 3.1595 - acc: 0.499 - ETA: 4s - loss: 3.1598 - acc: 0.500 - ETA: 4s - loss: 3.1596 - acc: 0.500 - ETA: 4s - loss: 3.1585 - acc: 0.500 - ETA: 4s - loss: 3.1608 - acc: 0.500 - ETA: 4s - loss: 3.1624 - acc: 0.500 - ETA: 4s - loss: 3.1626 - acc: 0.500 - ETA: 4s - loss: 3.1639 - acc: 0.500 - ETA: 3s - loss: 3.1643 - acc: 0.500 - ETA: 3s - loss: 3.1633 - acc: 0.500 - ETA: 3s - loss: 3.1634 - acc: 0.500 - ETA: 3s - loss: 3.1628 - acc: 0.500 - ETA: 3s - loss: 3.1659 - acc: 0.500 - ETA: 3s - loss: 3.1680 - acc: 0.499 - ETA: 3s - loss: 3.1678 - acc: 0.499 - ETA: 2s - loss: 3.1675 - acc: 0.499 - ETA: 2s - loss: 3.1671 - acc: 0.500 - ETA: 2s - loss: 3.1669 - acc: 0.500 - ETA: 2s - loss: 3.1647 - acc: 0.500 - ETA: 2s - loss: 3.1651 - acc: 0.500 - ETA: 2s - loss: 3.1654 - acc: 0.500 - ETA: 2s - loss: 3.1655 - acc: 0.500 - ETA: 1s - loss: 3.1640 - acc: 0.501 - ETA: 1s - loss: 3.1651 - acc: 0.501 - ETA: 1s - loss: 3.1646 - acc: 0.501 - ETA: 1s - loss: 3.1628 - acc: 0.501 - ETA: 1s - loss: 3.1633 - acc: 0.501 - ETA: 1s - loss: 3.1625 - acc: 0.501 - ETA: 0s - loss: 3.1625 - acc: 0.501 - ETA: 0s - loss: 3.1629 - acc: 0.501 - ETA: 0s - loss: 3.1639 - acc: 0.501 - ETA: 0s - loss: 3.1645 - acc: 0.501 - ETA: 0s - loss: 3.1652 - acc: 0.501 - ETA: 0s - loss: 3.1657 - acc: 0.501 - ETA: 0s - loss: 3.1658 - acc: 0.501 - 12s 657us/step - loss: 3.1659 - acc: 0.5015 - val_loss: 3.6361 - val_acc: 0.4824\n",
      "Epoch 14/200\n",
      "18357/18357 [==============================] - ETA: 9s - loss: 3.0321 - acc: 0.516 - ETA: 9s - loss: 2.9676 - acc: 0.519 - ETA: 9s - loss: 2.9654 - acc: 0.516 - ETA: 9s - loss: 2.9502 - acc: 0.519 - ETA: 9s - loss: 2.9838 - acc: 0.513 - ETA: 9s - loss: 2.9934 - acc: 0.512 - ETA: 9s - loss: 2.9869 - acc: 0.512 - ETA: 9s - loss: 2.9851 - acc: 0.514 - ETA: 9s - loss: 2.9901 - acc: 0.514 - ETA: 9s - loss: 2.9955 - acc: 0.513 - ETA: 9s - loss: 2.9961 - acc: 0.512 - ETA: 8s - loss: 2.9943 - acc: 0.513 - ETA: 8s - loss: 2.9893 - acc: 0.515 - ETA: 8s - loss: 2.9900 - acc: 0.516 - ETA: 8s - loss: 2.9896 - acc: 0.517 - ETA: 8s - loss: 2.9862 - acc: 0.517 - ETA: 8s - loss: 2.9892 - acc: 0.517 - ETA: 8s - loss: 2.9958 - acc: 0.516 - ETA: 7s - loss: 2.9925 - acc: 0.517 - ETA: 7s - loss: 2.9880 - acc: 0.518 - ETA: 7s - loss: 2.9972 - acc: 0.516 - ETA: 7s - loss: 2.9961 - acc: 0.516 - ETA: 7s - loss: 2.9982 - acc: 0.516 - ETA: 7s - loss: 3.0044 - acc: 0.515 - ETA: 7s - loss: 3.0045 - acc: 0.515 - ETA: 6s - loss: 3.0049 - acc: 0.515 - ETA: 6s - loss: 3.0036 - acc: 0.515 - ETA: 6s - loss: 2.9989 - acc: 0.516 - ETA: 6s - loss: 2.9979 - acc: 0.516 - ETA: 6s - loss: 2.9985 - acc: 0.516 - ETA: 6s - loss: 3.0007 - acc: 0.516 - ETA: 5s - loss: 3.0012 - acc: 0.516 - ETA: 5s - loss: 3.0033 - acc: 0.515 - ETA: 5s - loss: 3.0002 - acc: 0.516 - ETA: 5s - loss: 2.9994 - acc: 0.516 - ETA: 5s - loss: 3.0009 - acc: 0.516 - ETA: 5s - loss: 3.0012 - acc: 0.516 - ETA: 5s - loss: 2.9979 - acc: 0.517 - ETA: 4s - loss: 2.9975 - acc: 0.517 - ETA: 4s - loss: 2.9971 - acc: 0.516 - ETA: 4s - loss: 2.9979 - acc: 0.516 - ETA: 4s - loss: 2.9991 - acc: 0.516 - ETA: 4s - loss: 2.9978 - acc: 0.516 - ETA: 4s - loss: 2.9991 - acc: 0.516 - ETA: 4s - loss: 2.9975 - acc: 0.516 - ETA: 3s - loss: 2.9965 - acc: 0.517 - ETA: 3s - loss: 2.9997 - acc: 0.516 - ETA: 3s - loss: 2.9975 - acc: 0.517 - ETA: 3s - loss: 2.9982 - acc: 0.516 - ETA: 3s - loss: 2.9991 - acc: 0.516 - ETA: 3s - loss: 3.0015 - acc: 0.516 - ETA: 2s - loss: 3.0010 - acc: 0.516 - ETA: 2s - loss: 2.9996 - acc: 0.516 - ETA: 2s - loss: 2.9991 - acc: 0.516 - ETA: 2s - loss: 3.0001 - acc: 0.516 - ETA: 2s - loss: 3.0004 - acc: 0.516 - ETA: 2s - loss: 3.0002 - acc: 0.516 - ETA: 2s - loss: 3.0014 - acc: 0.516 - ETA: 1s - loss: 3.0009 - acc: 0.516 - ETA: 1s - loss: 3.0015 - acc: 0.516 - ETA: 1s - loss: 3.0008 - acc: 0.516 - ETA: 1s - loss: 3.0014 - acc: 0.516 - ETA: 1s - loss: 3.0026 - acc: 0.516 - ETA: 1s - loss: 3.0025 - acc: 0.516 - ETA: 1s - loss: 3.0030 - acc: 0.515 - ETA: 0s - loss: 3.0032 - acc: 0.515 - ETA: 0s - loss: 3.0015 - acc: 0.516 - ETA: 0s - loss: 3.0000 - acc: 0.516 - ETA: 0s - loss: 3.0003 - acc: 0.516 - ETA: 0s - loss: 2.9997 - acc: 0.516 - ETA: 0s - loss: 3.0007 - acc: 0.516 - 12s 671us/step - loss: 3.0015 - acc: 0.5163 - val_loss: 3.5282 - val_acc: 0.4937\n",
      "Epoch 15/200\n",
      "18357/18357 [==============================] - ETA: 9s - loss: 2.7718 - acc: 0.548 - ETA: 9s - loss: 2.7768 - acc: 0.548 - ETA: 10s - loss: 2.8051 - acc: 0.54 - ETA: 9s - loss: 2.8158 - acc: 0.5357 - ETA: 9s - loss: 2.8363 - acc: 0.534 - ETA: 9s - loss: 2.8387 - acc: 0.530 - ETA: 9s - loss: 2.8411 - acc: 0.530 - ETA: 9s - loss: 2.8300 - acc: 0.531 - ETA: 9s - loss: 2.8364 - acc: 0.530 - ETA: 9s - loss: 2.8372 - acc: 0.529 - ETA: 8s - loss: 2.8351 - acc: 0.531 - ETA: 8s - loss: 2.8353 - acc: 0.530 - ETA: 8s - loss: 2.8229 - acc: 0.532 - ETA: 8s - loss: 2.8214 - acc: 0.533 - ETA: 8s - loss: 2.8237 - acc: 0.531 - ETA: 8s - loss: 2.8210 - acc: 0.532 - ETA: 7s - loss: 2.8217 - acc: 0.532 - ETA: 7s - loss: 2.8207 - acc: 0.532 - ETA: 7s - loss: 2.8237 - acc: 0.531 - ETA: 7s - loss: 2.8262 - acc: 0.531 - ETA: 7s - loss: 2.8291 - acc: 0.531 - ETA: 7s - loss: 2.8328 - acc: 0.530 - ETA: 7s - loss: 2.8344 - acc: 0.530 - ETA: 6s - loss: 2.8356 - acc: 0.530 - ETA: 6s - loss: 2.8375 - acc: 0.530 - ETA: 6s - loss: 2.8426 - acc: 0.529 - ETA: 6s - loss: 2.8413 - acc: 0.529 - ETA: 6s - loss: 2.8406 - acc: 0.529 - ETA: 6s - loss: 2.8428 - acc: 0.529 - ETA: 6s - loss: 2.8451 - acc: 0.529 - ETA: 5s - loss: 2.8428 - acc: 0.530 - ETA: 5s - loss: 2.8456 - acc: 0.529 - ETA: 5s - loss: 2.8435 - acc: 0.529 - ETA: 5s - loss: 2.8418 - acc: 0.530 - ETA: 5s - loss: 2.8415 - acc: 0.530 - ETA: 5s - loss: 2.8431 - acc: 0.530 - ETA: 5s - loss: 2.8423 - acc: 0.530 - ETA: 4s - loss: 2.8435 - acc: 0.530 - ETA: 4s - loss: 2.8426 - acc: 0.530 - ETA: 4s - loss: 2.8415 - acc: 0.530 - ETA: 4s - loss: 2.8429 - acc: 0.530 - ETA: 4s - loss: 2.8441 - acc: 0.530 - ETA: 4s - loss: 2.8426 - acc: 0.530 - ETA: 4s - loss: 2.8410 - acc: 0.530 - ETA: 3s - loss: 2.8424 - acc: 0.530 - ETA: 3s - loss: 2.8423 - acc: 0.530 - ETA: 3s - loss: 2.8412 - acc: 0.531 - ETA: 3s - loss: 2.8410 - acc: 0.531 - ETA: 3s - loss: 2.8398 - acc: 0.531 - ETA: 3s - loss: 2.8408 - acc: 0.531 - ETA: 3s - loss: 2.8395 - acc: 0.531 - ETA: 2s - loss: 2.8386 - acc: 0.531 - ETA: 2s - loss: 2.8385 - acc: 0.531 - ETA: 2s - loss: 2.8379 - acc: 0.531 - ETA: 2s - loss: 2.8381 - acc: 0.532 - ETA: 2s - loss: 2.8403 - acc: 0.531 - ETA: 2s - loss: 2.8403 - acc: 0.532 - ETA: 2s - loss: 2.8398 - acc: 0.532 - ETA: 1s - loss: 2.8384 - acc: 0.532 - ETA: 1s - loss: 2.8383 - acc: 0.532 - ETA: 1s - loss: 2.8382 - acc: 0.532 - ETA: 1s - loss: 2.8369 - acc: 0.532 - ETA: 1s - loss: 2.8367 - acc: 0.532 - ETA: 1s - loss: 2.8375 - acc: 0.532 - ETA: 1s - loss: 2.8372 - acc: 0.532 - ETA: 0s - loss: 2.8367 - acc: 0.532 - ETA: 0s - loss: 2.8366 - acc: 0.532 - ETA: 0s - loss: 2.8375 - acc: 0.532 - ETA: 0s - loss: 2.8368 - acc: 0.532 - ETA: 0s - loss: 2.8375 - acc: 0.532 - ETA: 0s - loss: 2.8372 - acc: 0.532 - 13s 690us/step - loss: 2.8375 - acc: 0.5324 - val_loss: 3.4355 - val_acc: 0.5017\n",
      "Epoch 16/200\n",
      "18357/18357 [==============================] - ETA: 9s - loss: 2.7829 - acc: 0.535 - ETA: 10s - loss: 2.7740 - acc: 0.53 - ETA: 10s - loss: 2.7301 - acc: 0.53 - ETA: 9s - loss: 2.7271 - acc: 0.5373 - ETA: 9s - loss: 2.7206 - acc: 0.537 - ETA: 9s - loss: 2.7020 - acc: 0.538 - ETA: 9s - loss: 2.7078 - acc: 0.538 - ETA: 9s - loss: 2.7010 - acc: 0.538 - ETA: 9s - loss: 2.7058 - acc: 0.537 - ETA: 8s - loss: 2.7060 - acc: 0.537 - ETA: 8s - loss: 2.7058 - acc: 0.538 - ETA: 8s - loss: 2.7066 - acc: 0.538 - ETA: 8s - loss: 2.7139 - acc: 0.537 - ETA: 8s - loss: 2.7092 - acc: 0.538 - ETA: 8s - loss: 2.7033 - acc: 0.540 - ETA: 8s - loss: 2.6994 - acc: 0.540 - ETA: 7s - loss: 2.6984 - acc: 0.540 - ETA: 7s - loss: 2.6993 - acc: 0.540 - ETA: 7s - loss: 2.6968 - acc: 0.540 - ETA: 7s - loss: 2.6944 - acc: 0.541 - ETA: 7s - loss: 2.6965 - acc: 0.542 - ETA: 7s - loss: 2.6968 - acc: 0.542 - ETA: 7s - loss: 2.6965 - acc: 0.542 - ETA: 6s - loss: 2.6988 - acc: 0.542 - ETA: 6s - loss: 2.6989 - acc: 0.542 - ETA: 6s - loss: 2.6947 - acc: 0.542 - ETA: 6s - loss: 2.6961 - acc: 0.542 - ETA: 6s - loss: 2.6991 - acc: 0.542 - ETA: 6s - loss: 2.7010 - acc: 0.542 - ETA: 6s - loss: 2.6981 - acc: 0.543 - ETA: 5s - loss: 2.6980 - acc: 0.543 - ETA: 5s - loss: 2.6987 - acc: 0.542 - ETA: 5s - loss: 2.6987 - acc: 0.542 - ETA: 5s - loss: 2.6971 - acc: 0.542 - ETA: 5s - loss: 2.6963 - acc: 0.542 - ETA: 5s - loss: 2.6944 - acc: 0.543 - ETA: 5s - loss: 2.6943 - acc: 0.543 - ETA: 4s - loss: 2.6941 - acc: 0.543 - ETA: 4s - loss: 2.6929 - acc: 0.543 - ETA: 4s - loss: 2.6927 - acc: 0.543 - ETA: 4s - loss: 2.6921 - acc: 0.543 - ETA: 4s - loss: 2.6913 - acc: 0.543 - ETA: 4s - loss: 2.6917 - acc: 0.543 - ETA: 4s - loss: 2.6901 - acc: 0.543 - ETA: 3s - loss: 2.6896 - acc: 0.544 - ETA: 3s - loss: 2.6911 - acc: 0.543 - ETA: 3s - loss: 2.6919 - acc: 0.543 - ETA: 3s - loss: 2.6912 - acc: 0.544 - ETA: 3s - loss: 2.6912 - acc: 0.544 - ETA: 3s - loss: 2.6900 - acc: 0.544 - ETA: 2s - loss: 2.6902 - acc: 0.544 - ETA: 2s - loss: 2.6905 - acc: 0.544 - ETA: 2s - loss: 2.6895 - acc: 0.544 - ETA: 2s - loss: 2.6879 - acc: 0.544 - ETA: 2s - loss: 2.6857 - acc: 0.545 - ETA: 2s - loss: 2.6845 - acc: 0.545 - ETA: 2s - loss: 2.6842 - acc: 0.545 - ETA: 1s - loss: 2.6837 - acc: 0.546 - ETA: 1s - loss: 2.6826 - acc: 0.546 - ETA: 1s - loss: 2.6820 - acc: 0.546 - ETA: 1s - loss: 2.6825 - acc: 0.546 - ETA: 1s - loss: 2.6819 - acc: 0.546 - ETA: 1s - loss: 2.6819 - acc: 0.546 - ETA: 1s - loss: 2.6818 - acc: 0.546 - ETA: 0s - loss: 2.6822 - acc: 0.546 - ETA: 0s - loss: 2.6813 - acc: 0.546 - ETA: 0s - loss: 2.6822 - acc: 0.546 - ETA: 0s - loss: 2.6810 - acc: 0.546 - ETA: 0s - loss: 2.6804 - acc: 0.547 - ETA: 0s - loss: 2.6813 - acc: 0.546 - ETA: 0s - loss: 2.6817 - acc: 0.547 - 12s 652us/step - loss: 2.6814 - acc: 0.5471 - val_loss: 3.3411 - val_acc: 0.5119\n",
      "Epoch 17/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18357/18357 [==============================] - ETA: 9s - loss: 2.6203 - acc: 0.559 - ETA: 9s - loss: 2.5376 - acc: 0.567 - ETA: 9s - loss: 2.5474 - acc: 0.560 - ETA: 9s - loss: 2.5438 - acc: 0.557 - ETA: 9s - loss: 2.5375 - acc: 0.557 - ETA: 9s - loss: 2.5395 - acc: 0.557 - ETA: 9s - loss: 2.5319 - acc: 0.555 - ETA: 9s - loss: 2.5315 - acc: 0.556 - ETA: 8s - loss: 2.5379 - acc: 0.555 - ETA: 8s - loss: 2.5407 - acc: 0.555 - ETA: 8s - loss: 2.5410 - acc: 0.555 - ETA: 8s - loss: 2.5400 - acc: 0.555 - ETA: 8s - loss: 2.5440 - acc: 0.556 - ETA: 8s - loss: 2.5461 - acc: 0.555 - ETA: 8s - loss: 2.5466 - acc: 0.556 - ETA: 7s - loss: 2.5442 - acc: 0.556 - ETA: 7s - loss: 2.5408 - acc: 0.557 - ETA: 7s - loss: 2.5363 - acc: 0.558 - ETA: 7s - loss: 2.5354 - acc: 0.558 - ETA: 7s - loss: 2.5362 - acc: 0.558 - ETA: 7s - loss: 2.5310 - acc: 0.559 - ETA: 7s - loss: 2.5285 - acc: 0.560 - ETA: 6s - loss: 2.5278 - acc: 0.560 - ETA: 6s - loss: 2.5324 - acc: 0.559 - ETA: 6s - loss: 2.5354 - acc: 0.559 - ETA: 6s - loss: 2.5360 - acc: 0.559 - ETA: 6s - loss: 2.5364 - acc: 0.559 - ETA: 6s - loss: 2.5369 - acc: 0.559 - ETA: 6s - loss: 2.5353 - acc: 0.559 - ETA: 5s - loss: 2.5357 - acc: 0.559 - ETA: 5s - loss: 2.5367 - acc: 0.559 - ETA: 5s - loss: 2.5349 - acc: 0.559 - ETA: 5s - loss: 2.5361 - acc: 0.559 - ETA: 5s - loss: 2.5374 - acc: 0.559 - ETA: 5s - loss: 2.5378 - acc: 0.559 - ETA: 5s - loss: 2.5389 - acc: 0.559 - ETA: 4s - loss: 2.5417 - acc: 0.558 - ETA: 4s - loss: 2.5399 - acc: 0.558 - ETA: 4s - loss: 2.5412 - acc: 0.558 - ETA: 4s - loss: 2.5389 - acc: 0.558 - ETA: 4s - loss: 2.5391 - acc: 0.558 - ETA: 4s - loss: 2.5402 - acc: 0.558 - ETA: 4s - loss: 2.5401 - acc: 0.559 - ETA: 3s - loss: 2.5407 - acc: 0.558 - ETA: 3s - loss: 2.5355 - acc: 0.560 - ETA: 3s - loss: 2.5352 - acc: 0.560 - ETA: 3s - loss: 2.5348 - acc: 0.560 - ETA: 3s - loss: 2.5351 - acc: 0.560 - ETA: 3s - loss: 2.5333 - acc: 0.560 - ETA: 3s - loss: 2.5329 - acc: 0.560 - ETA: 2s - loss: 2.5313 - acc: 0.561 - ETA: 2s - loss: 2.5323 - acc: 0.561 - ETA: 2s - loss: 2.5347 - acc: 0.561 - ETA: 2s - loss: 2.5341 - acc: 0.561 - ETA: 2s - loss: 2.5319 - acc: 0.561 - ETA: 2s - loss: 2.5339 - acc: 0.561 - ETA: 2s - loss: 2.5352 - acc: 0.561 - ETA: 1s - loss: 2.5353 - acc: 0.561 - ETA: 1s - loss: 2.5339 - acc: 0.561 - ETA: 1s - loss: 2.5341 - acc: 0.561 - ETA: 1s - loss: 2.5334 - acc: 0.561 - ETA: 1s - loss: 2.5327 - acc: 0.561 - ETA: 1s - loss: 2.5325 - acc: 0.561 - ETA: 1s - loss: 2.5333 - acc: 0.561 - ETA: 0s - loss: 2.5337 - acc: 0.561 - ETA: 0s - loss: 2.5339 - acc: 0.561 - ETA: 0s - loss: 2.5339 - acc: 0.561 - ETA: 0s - loss: 2.5339 - acc: 0.561 - ETA: 0s - loss: 2.5337 - acc: 0.561 - ETA: 0s - loss: 2.5334 - acc: 0.561 - ETA: 0s - loss: 2.5338 - acc: 0.561 - 12s 656us/step - loss: 2.5337 - acc: 0.5610 - val_loss: 3.2554 - val_acc: 0.5228\n",
      "Epoch 18/200\n",
      "18357/18357 [==============================] - ETA: 10s - loss: 2.4065 - acc: 0.57 - ETA: 10s - loss: 2.3708 - acc: 0.57 - ETA: 10s - loss: 2.3913 - acc: 0.57 - ETA: 10s - loss: 2.4087 - acc: 0.57 - ETA: 9s - loss: 2.3929 - acc: 0.5767 - ETA: 9s - loss: 2.3865 - acc: 0.576 - ETA: 9s - loss: 2.3784 - acc: 0.577 - ETA: 9s - loss: 2.3853 - acc: 0.576 - ETA: 9s - loss: 2.3909 - acc: 0.575 - ETA: 9s - loss: 2.3896 - acc: 0.576 - ETA: 8s - loss: 2.3913 - acc: 0.575 - ETA: 8s - loss: 2.3878 - acc: 0.575 - ETA: 8s - loss: 2.3814 - acc: 0.576 - ETA: 8s - loss: 2.3802 - acc: 0.576 - ETA: 8s - loss: 2.3754 - acc: 0.577 - ETA: 8s - loss: 2.3741 - acc: 0.578 - ETA: 8s - loss: 2.3753 - acc: 0.577 - ETA: 7s - loss: 2.3751 - acc: 0.577 - ETA: 7s - loss: 2.3802 - acc: 0.576 - ETA: 7s - loss: 2.3812 - acc: 0.576 - ETA: 7s - loss: 2.3840 - acc: 0.576 - ETA: 7s - loss: 2.3838 - acc: 0.576 - ETA: 7s - loss: 2.3817 - acc: 0.576 - ETA: 6s - loss: 2.3839 - acc: 0.576 - ETA: 6s - loss: 2.3822 - acc: 0.576 - ETA: 6s - loss: 2.3824 - acc: 0.576 - ETA: 6s - loss: 2.3876 - acc: 0.575 - ETA: 6s - loss: 2.3866 - acc: 0.575 - ETA: 6s - loss: 2.3877 - acc: 0.575 - ETA: 6s - loss: 2.3872 - acc: 0.575 - ETA: 5s - loss: 2.3892 - acc: 0.575 - ETA: 5s - loss: 2.3905 - acc: 0.574 - ETA: 5s - loss: 2.3890 - acc: 0.575 - ETA: 5s - loss: 2.3876 - acc: 0.575 - ETA: 5s - loss: 2.3902 - acc: 0.574 - ETA: 5s - loss: 2.3889 - acc: 0.574 - ETA: 5s - loss: 2.3879 - acc: 0.575 - ETA: 4s - loss: 2.3828 - acc: 0.576 - ETA: 4s - loss: 2.3827 - acc: 0.575 - ETA: 4s - loss: 2.3848 - acc: 0.575 - ETA: 4s - loss: 2.3864 - acc: 0.575 - ETA: 4s - loss: 2.3871 - acc: 0.575 - ETA: 4s - loss: 2.3876 - acc: 0.576 - ETA: 4s - loss: 2.3877 - acc: 0.575 - ETA: 3s - loss: 2.3884 - acc: 0.575 - ETA: 3s - loss: 2.3888 - acc: 0.575 - ETA: 3s - loss: 2.3889 - acc: 0.575 - ETA: 3s - loss: 2.3918 - acc: 0.575 - ETA: 3s - loss: 2.3912 - acc: 0.575 - ETA: 3s - loss: 2.3898 - acc: 0.575 - ETA: 3s - loss: 2.3918 - acc: 0.575 - ETA: 2s - loss: 2.3900 - acc: 0.575 - ETA: 2s - loss: 2.3879 - acc: 0.576 - ETA: 2s - loss: 2.3887 - acc: 0.576 - ETA: 2s - loss: 2.3900 - acc: 0.576 - ETA: 2s - loss: 2.3892 - acc: 0.576 - ETA: 2s - loss: 2.3874 - acc: 0.576 - ETA: 2s - loss: 2.3856 - acc: 0.577 - ETA: 1s - loss: 2.3854 - acc: 0.577 - ETA: 1s - loss: 2.3850 - acc: 0.577 - ETA: 1s - loss: 2.3848 - acc: 0.577 - ETA: 1s - loss: 2.3831 - acc: 0.577 - ETA: 1s - loss: 2.3828 - acc: 0.577 - ETA: 1s - loss: 2.3842 - acc: 0.577 - ETA: 0s - loss: 2.3839 - acc: 0.577 - ETA: 0s - loss: 2.3839 - acc: 0.577 - ETA: 0s - loss: 2.3836 - acc: 0.577 - ETA: 0s - loss: 2.3841 - acc: 0.577 - ETA: 0s - loss: 2.3836 - acc: 0.577 - ETA: 0s - loss: 2.3834 - acc: 0.577 - ETA: 0s - loss: 2.3846 - acc: 0.577 - 12s 660us/step - loss: 2.3853 - acc: 0.5775 - val_loss: 3.1769 - val_acc: 0.5312\n",
      "Epoch 19/200\n",
      "18357/18357 [==============================] - ETA: 9s - loss: 2.2280 - acc: 0.603 - ETA: 10s - loss: 2.1961 - acc: 0.60 - ETA: 10s - loss: 2.1866 - acc: 0.60 - ETA: 10s - loss: 2.2044 - acc: 0.59 - ETA: 10s - loss: 2.2175 - acc: 0.59 - ETA: 9s - loss: 2.2242 - acc: 0.5928 - ETA: 9s - loss: 2.2395 - acc: 0.590 - ETA: 9s - loss: 2.2498 - acc: 0.589 - ETA: 9s - loss: 2.2527 - acc: 0.589 - ETA: 9s - loss: 2.2490 - acc: 0.589 - ETA: 9s - loss: 2.2486 - acc: 0.589 - ETA: 8s - loss: 2.2447 - acc: 0.590 - ETA: 8s - loss: 2.2383 - acc: 0.592 - ETA: 8s - loss: 2.2312 - acc: 0.593 - ETA: 8s - loss: 2.2269 - acc: 0.594 - ETA: 8s - loss: 2.2236 - acc: 0.594 - ETA: 8s - loss: 2.2184 - acc: 0.595 - ETA: 8s - loss: 2.2201 - acc: 0.595 - ETA: 7s - loss: 2.2226 - acc: 0.594 - ETA: 7s - loss: 2.2273 - acc: 0.593 - ETA: 7s - loss: 2.2306 - acc: 0.593 - ETA: 7s - loss: 2.2293 - acc: 0.593 - ETA: 7s - loss: 2.2330 - acc: 0.591 - ETA: 7s - loss: 2.2349 - acc: 0.591 - ETA: 6s - loss: 2.2356 - acc: 0.591 - ETA: 6s - loss: 2.2360 - acc: 0.591 - ETA: 6s - loss: 2.2379 - acc: 0.591 - ETA: 6s - loss: 2.2381 - acc: 0.591 - ETA: 6s - loss: 2.2394 - acc: 0.591 - ETA: 6s - loss: 2.2387 - acc: 0.591 - ETA: 6s - loss: 2.2373 - acc: 0.592 - ETA: 5s - loss: 2.2369 - acc: 0.592 - ETA: 5s - loss: 2.2394 - acc: 0.592 - ETA: 5s - loss: 2.2407 - acc: 0.592 - ETA: 5s - loss: 2.2435 - acc: 0.591 - ETA: 5s - loss: 2.2451 - acc: 0.591 - ETA: 5s - loss: 2.2466 - acc: 0.591 - ETA: 5s - loss: 2.2446 - acc: 0.591 - ETA: 4s - loss: 2.2459 - acc: 0.591 - ETA: 4s - loss: 2.2483 - acc: 0.591 - ETA: 4s - loss: 2.2467 - acc: 0.592 - ETA: 4s - loss: 2.2492 - acc: 0.592 - ETA: 4s - loss: 2.2494 - acc: 0.591 - ETA: 4s - loss: 2.2499 - acc: 0.591 - ETA: 3s - loss: 2.2515 - acc: 0.591 - ETA: 3s - loss: 2.2512 - acc: 0.591 - ETA: 3s - loss: 2.2500 - acc: 0.591 - ETA: 3s - loss: 2.2500 - acc: 0.591 - ETA: 3s - loss: 2.2490 - acc: 0.591 - ETA: 3s - loss: 2.2493 - acc: 0.592 - ETA: 3s - loss: 2.2489 - acc: 0.592 - ETA: 2s - loss: 2.2490 - acc: 0.592 - ETA: 2s - loss: 2.2482 - acc: 0.592 - ETA: 2s - loss: 2.2482 - acc: 0.592 - ETA: 2s - loss: 2.2479 - acc: 0.592 - ETA: 2s - loss: 2.2476 - acc: 0.593 - ETA: 2s - loss: 2.2468 - acc: 0.592 - ETA: 2s - loss: 2.2469 - acc: 0.593 - ETA: 1s - loss: 2.2469 - acc: 0.593 - ETA: 1s - loss: 2.2467 - acc: 0.593 - ETA: 1s - loss: 2.2474 - acc: 0.593 - ETA: 1s - loss: 2.2468 - acc: 0.593 - ETA: 1s - loss: 2.2457 - acc: 0.593 - ETA: 1s - loss: 2.2451 - acc: 0.593 - ETA: 1s - loss: 2.2445 - acc: 0.594 - ETA: 0s - loss: 2.2450 - acc: 0.594 - ETA: 0s - loss: 2.2467 - acc: 0.594 - ETA: 0s - loss: 2.2472 - acc: 0.593 - ETA: 0s - loss: 2.2467 - acc: 0.593 - ETA: 0s - loss: 2.2449 - acc: 0.594 - ETA: 0s - loss: 2.2436 - acc: 0.594 - 12s 673us/step - loss: 2.2432 - acc: 0.5942 - val_loss: 3.1017 - val_acc: 0.5386\n",
      "Epoch 20/200\n",
      "18357/18357 [==============================] - ETA: 10s - loss: 2.1007 - acc: 0.61 - ETA: 10s - loss: 2.0783 - acc: 0.61 - ETA: 10s - loss: 2.0942 - acc: 0.61 - ETA: 10s - loss: 2.0887 - acc: 0.61 - ETA: 10s - loss: 2.1186 - acc: 0.60 - ETA: 10s - loss: 2.1093 - acc: 0.60 - ETA: 9s - loss: 2.0839 - acc: 0.6150 - ETA: 9s - loss: 2.0804 - acc: 0.614 - ETA: 9s - loss: 2.0766 - acc: 0.615 - ETA: 9s - loss: 2.0771 - acc: 0.615 - ETA: 9s - loss: 2.0805 - acc: 0.613 - ETA: 8s - loss: 2.0827 - acc: 0.612 - ETA: 8s - loss: 2.0766 - acc: 0.613 - ETA: 8s - loss: 2.0862 - acc: 0.612 - ETA: 8s - loss: 2.0881 - acc: 0.612 - ETA: 8s - loss: 2.0876 - acc: 0.611 - ETA: 8s - loss: 2.0843 - acc: 0.612 - ETA: 8s - loss: 2.0866 - acc: 0.612 - ETA: 7s - loss: 2.0849 - acc: 0.612 - ETA: 7s - loss: 2.0818 - acc: 0.612 - ETA: 7s - loss: 2.0842 - acc: 0.612 - ETA: 7s - loss: 2.0841 - acc: 0.611 - ETA: 7s - loss: 2.0849 - acc: 0.611 - ETA: 7s - loss: 2.0836 - acc: 0.611 - ETA: 7s - loss: 2.0867 - acc: 0.611 - ETA: 6s - loss: 2.0851 - acc: 0.611 - ETA: 6s - loss: 2.0859 - acc: 0.611 - ETA: 6s - loss: 2.0853 - acc: 0.611 - ETA: 6s - loss: 2.0864 - acc: 0.611 - ETA: 6s - loss: 2.0877 - acc: 0.611 - ETA: 6s - loss: 2.0897 - acc: 0.611 - ETA: 5s - loss: 2.0887 - acc: 0.611 - ETA: 5s - loss: 2.0899 - acc: 0.611 - ETA: 5s - loss: 2.0909 - acc: 0.611 - ETA: 5s - loss: 2.0919 - acc: 0.612 - ETA: 5s - loss: 2.0933 - acc: 0.611 - ETA: 5s - loss: 2.0986 - acc: 0.610 - ETA: 5s - loss: 2.1011 - acc: 0.610 - ETA: 4s - loss: 2.0992 - acc: 0.610 - ETA: 4s - loss: 2.1007 - acc: 0.610 - ETA: 4s - loss: 2.1008 - acc: 0.609 - ETA: 4s - loss: 2.1007 - acc: 0.610 - ETA: 4s - loss: 2.1014 - acc: 0.609 - ETA: 4s - loss: 2.1018 - acc: 0.609 - ETA: 4s - loss: 2.1008 - acc: 0.609 - ETA: 3s - loss: 2.1000 - acc: 0.609 - ETA: 3s - loss: 2.1001 - acc: 0.609 - ETA: 3s - loss: 2.1027 - acc: 0.609 - ETA: 3s - loss: 2.1023 - acc: 0.609 - ETA: 3s - loss: 2.1025 - acc: 0.609 - ETA: 3s - loss: 2.1051 - acc: 0.609 - ETA: 2s - loss: 2.1060 - acc: 0.609 - ETA: 2s - loss: 2.1061 - acc: 0.609 - ETA: 2s - loss: 2.1065 - acc: 0.609 - ETA: 2s - loss: 2.1050 - acc: 0.609 - ETA: 2s - loss: 2.1045 - acc: 0.609 - ETA: 2s - loss: 2.1056 - acc: 0.609 - ETA: 2s - loss: 2.1066 - acc: 0.609 - ETA: 1s - loss: 2.1075 - acc: 0.609 - ETA: 1s - loss: 2.1075 - acc: 0.609 - ETA: 1s - loss: 2.1084 - acc: 0.609 - ETA: 1s - loss: 2.1079 - acc: 0.609 - ETA: 1s - loss: 2.1087 - acc: 0.609 - ETA: 1s - loss: 2.1087 - acc: 0.609 - ETA: 1s - loss: 2.1082 - acc: 0.610 - ETA: 0s - loss: 2.1089 - acc: 0.609 - ETA: 0s - loss: 2.1078 - acc: 0.610 - ETA: 0s - loss: 2.1084 - acc: 0.609 - ETA: 0s - loss: 2.1083 - acc: 0.609 - ETA: 0s - loss: 2.1090 - acc: 0.609 - ETA: 0s - loss: 2.1094 - acc: 0.609 - 12s 681us/step - loss: 2.1099 - acc: 0.6095 - val_loss: 3.0381 - val_acc: 0.5471\n",
      "Epoch 21/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18357/18357 [==============================] - ETA: 10s - loss: 1.9584 - acc: 0.63 - ETA: 11s - loss: 1.9812 - acc: 0.62 - ETA: 10s - loss: 2.0122 - acc: 0.62 - ETA: 11s - loss: 1.9950 - acc: 0.62 - ETA: 10s - loss: 1.9691 - acc: 0.63 - ETA: 10s - loss: 1.9707 - acc: 0.63 - ETA: 10s - loss: 1.9535 - acc: 0.63 - ETA: 9s - loss: 1.9628 - acc: 0.6328 - ETA: 9s - loss: 1.9609 - acc: 0.632 - ETA: 9s - loss: 1.9696 - acc: 0.630 - ETA: 9s - loss: 1.9671 - acc: 0.631 - ETA: 9s - loss: 1.9685 - acc: 0.631 - ETA: 8s - loss: 1.9660 - acc: 0.630 - ETA: 8s - loss: 1.9672 - acc: 0.630 - ETA: 8s - loss: 1.9690 - acc: 0.629 - ETA: 8s - loss: 1.9710 - acc: 0.628 - ETA: 8s - loss: 1.9692 - acc: 0.628 - ETA: 8s - loss: 1.9700 - acc: 0.628 - ETA: 7s - loss: 1.9724 - acc: 0.627 - ETA: 7s - loss: 1.9706 - acc: 0.627 - ETA: 7s - loss: 1.9721 - acc: 0.627 - ETA: 7s - loss: 1.9736 - acc: 0.626 - ETA: 7s - loss: 1.9717 - acc: 0.626 - ETA: 7s - loss: 1.9704 - acc: 0.627 - ETA: 6s - loss: 1.9685 - acc: 0.628 - ETA: 6s - loss: 1.9665 - acc: 0.628 - ETA: 6s - loss: 1.9676 - acc: 0.627 - ETA: 6s - loss: 1.9668 - acc: 0.628 - ETA: 6s - loss: 1.9686 - acc: 0.628 - ETA: 6s - loss: 1.9692 - acc: 0.627 - ETA: 6s - loss: 1.9689 - acc: 0.628 - ETA: 5s - loss: 1.9672 - acc: 0.628 - ETA: 5s - loss: 1.9689 - acc: 0.628 - ETA: 5s - loss: 1.9684 - acc: 0.628 - ETA: 5s - loss: 1.9681 - acc: 0.628 - ETA: 5s - loss: 1.9686 - acc: 0.627 - ETA: 5s - loss: 1.9724 - acc: 0.627 - ETA: 5s - loss: 1.9718 - acc: 0.627 - ETA: 4s - loss: 1.9716 - acc: 0.627 - ETA: 4s - loss: 1.9710 - acc: 0.627 - ETA: 4s - loss: 1.9709 - acc: 0.627 - ETA: 4s - loss: 1.9704 - acc: 0.627 - ETA: 4s - loss: 1.9702 - acc: 0.627 - ETA: 4s - loss: 1.9710 - acc: 0.626 - ETA: 3s - loss: 1.9727 - acc: 0.626 - ETA: 3s - loss: 1.9730 - acc: 0.626 - ETA: 3s - loss: 1.9723 - acc: 0.626 - ETA: 3s - loss: 1.9739 - acc: 0.626 - ETA: 3s - loss: 1.9725 - acc: 0.627 - ETA: 3s - loss: 1.9733 - acc: 0.626 - ETA: 3s - loss: 1.9726 - acc: 0.627 - ETA: 2s - loss: 1.9715 - acc: 0.627 - ETA: 2s - loss: 1.9717 - acc: 0.627 - ETA: 2s - loss: 1.9726 - acc: 0.627 - ETA: 2s - loss: 1.9725 - acc: 0.627 - ETA: 2s - loss: 1.9721 - acc: 0.627 - ETA: 2s - loss: 1.9736 - acc: 0.627 - ETA: 2s - loss: 1.9744 - acc: 0.627 - ETA: 1s - loss: 1.9735 - acc: 0.627 - ETA: 1s - loss: 1.9733 - acc: 0.627 - ETA: 1s - loss: 1.9753 - acc: 0.627 - ETA: 1s - loss: 1.9761 - acc: 0.627 - ETA: 1s - loss: 1.9762 - acc: 0.627 - ETA: 1s - loss: 1.9761 - acc: 0.627 - ETA: 1s - loss: 1.9755 - acc: 0.627 - ETA: 0s - loss: 1.9773 - acc: 0.627 - ETA: 0s - loss: 1.9773 - acc: 0.626 - ETA: 0s - loss: 1.9774 - acc: 0.627 - ETA: 0s - loss: 1.9768 - acc: 0.627 - ETA: 0s - loss: 1.9773 - acc: 0.627 - ETA: 0s - loss: 1.9779 - acc: 0.626 - 12s 668us/step - loss: 1.9781 - acc: 0.6268 - val_loss: 2.9713 - val_acc: 0.5576\n",
      "Epoch 22/200\n",
      "18357/18357 [==============================] - ETA: 10s - loss: 1.8682 - acc: 0.64 - ETA: 9s - loss: 1.8247 - acc: 0.6527 - ETA: 9s - loss: 1.8405 - acc: 0.649 - ETA: 9s - loss: 1.8680 - acc: 0.642 - ETA: 9s - loss: 1.8578 - acc: 0.645 - ETA: 9s - loss: 1.8443 - acc: 0.647 - ETA: 9s - loss: 1.8452 - acc: 0.646 - ETA: 9s - loss: 1.8407 - acc: 0.646 - ETA: 9s - loss: 1.8387 - acc: 0.648 - ETA: 8s - loss: 1.8426 - acc: 0.647 - ETA: 8s - loss: 1.8481 - acc: 0.646 - ETA: 8s - loss: 1.8469 - acc: 0.647 - ETA: 8s - loss: 1.8447 - acc: 0.647 - ETA: 8s - loss: 1.8399 - acc: 0.647 - ETA: 8s - loss: 1.8383 - acc: 0.647 - ETA: 8s - loss: 1.8382 - acc: 0.647 - ETA: 8s - loss: 1.8376 - acc: 0.647 - ETA: 7s - loss: 1.8332 - acc: 0.648 - ETA: 7s - loss: 1.8338 - acc: 0.647 - ETA: 7s - loss: 1.8339 - acc: 0.647 - ETA: 7s - loss: 1.8335 - acc: 0.647 - ETA: 7s - loss: 1.8338 - acc: 0.646 - ETA: 7s - loss: 1.8320 - acc: 0.647 - ETA: 7s - loss: 1.8363 - acc: 0.647 - ETA: 6s - loss: 1.8350 - acc: 0.647 - ETA: 6s - loss: 1.8315 - acc: 0.647 - ETA: 6s - loss: 1.8304 - acc: 0.647 - ETA: 6s - loss: 1.8300 - acc: 0.647 - ETA: 6s - loss: 1.8314 - acc: 0.647 - ETA: 6s - loss: 1.8314 - acc: 0.647 - ETA: 5s - loss: 1.8317 - acc: 0.647 - ETA: 5s - loss: 1.8335 - acc: 0.646 - ETA: 5s - loss: 1.8317 - acc: 0.647 - ETA: 5s - loss: 1.8305 - acc: 0.647 - ETA: 5s - loss: 1.8280 - acc: 0.647 - ETA: 5s - loss: 1.8297 - acc: 0.647 - ETA: 5s - loss: 1.8320 - acc: 0.647 - ETA: 4s - loss: 1.8328 - acc: 0.646 - ETA: 4s - loss: 1.8313 - acc: 0.646 - ETA: 4s - loss: 1.8338 - acc: 0.646 - ETA: 4s - loss: 1.8330 - acc: 0.646 - ETA: 4s - loss: 1.8336 - acc: 0.646 - ETA: 4s - loss: 1.8337 - acc: 0.646 - ETA: 4s - loss: 1.8357 - acc: 0.646 - ETA: 3s - loss: 1.8377 - acc: 0.645 - ETA: 3s - loss: 1.8381 - acc: 0.645 - ETA: 3s - loss: 1.8376 - acc: 0.645 - ETA: 3s - loss: 1.8384 - acc: 0.645 - ETA: 3s - loss: 1.8414 - acc: 0.645 - ETA: 3s - loss: 1.8406 - acc: 0.645 - ETA: 3s - loss: 1.8400 - acc: 0.645 - ETA: 2s - loss: 1.8395 - acc: 0.645 - ETA: 2s - loss: 1.8410 - acc: 0.645 - ETA: 2s - loss: 1.8403 - acc: 0.645 - ETA: 2s - loss: 1.8400 - acc: 0.645 - ETA: 2s - loss: 1.8401 - acc: 0.645 - ETA: 2s - loss: 1.8417 - acc: 0.645 - ETA: 1s - loss: 1.8430 - acc: 0.645 - ETA: 1s - loss: 1.8419 - acc: 0.645 - ETA: 1s - loss: 1.8442 - acc: 0.644 - ETA: 1s - loss: 1.8469 - acc: 0.644 - ETA: 1s - loss: 1.8462 - acc: 0.644 - ETA: 1s - loss: 1.8466 - acc: 0.644 - ETA: 1s - loss: 1.8460 - acc: 0.644 - ETA: 0s - loss: 1.8457 - acc: 0.644 - ETA: 0s - loss: 1.8461 - acc: 0.644 - ETA: 0s - loss: 1.8469 - acc: 0.644 - ETA: 0s - loss: 1.8463 - acc: 0.644 - ETA: 0s - loss: 1.8466 - acc: 0.644 - ETA: 0s - loss: 1.8464 - acc: 0.644 - ETA: 0s - loss: 1.8465 - acc: 0.644 - 12s 651us/step - loss: 1.8477 - acc: 0.6443 - val_loss: 2.9186 - val_acc: 0.5624\n",
      "Epoch 23/200\n",
      "18357/18357 [==============================] - ETA: 9s - loss: 1.7246 - acc: 0.668 - ETA: 10s - loss: 1.7177 - acc: 0.66 - ETA: 10s - loss: 1.7228 - acc: 0.66 - ETA: 9s - loss: 1.7376 - acc: 0.6641 - ETA: 9s - loss: 1.7239 - acc: 0.663 - ETA: 9s - loss: 1.7175 - acc: 0.664 - ETA: 9s - loss: 1.7178 - acc: 0.664 - ETA: 9s - loss: 1.7120 - acc: 0.664 - ETA: 9s - loss: 1.7017 - acc: 0.667 - ETA: 8s - loss: 1.7004 - acc: 0.667 - ETA: 8s - loss: 1.7072 - acc: 0.666 - ETA: 8s - loss: 1.7090 - acc: 0.666 - ETA: 8s - loss: 1.7137 - acc: 0.665 - ETA: 8s - loss: 1.7088 - acc: 0.665 - ETA: 8s - loss: 1.7107 - acc: 0.664 - ETA: 8s - loss: 1.7078 - acc: 0.665 - ETA: 7s - loss: 1.7067 - acc: 0.666 - ETA: 7s - loss: 1.7047 - acc: 0.667 - ETA: 7s - loss: 1.7025 - acc: 0.667 - ETA: 7s - loss: 1.7083 - acc: 0.665 - ETA: 7s - loss: 1.7115 - acc: 0.665 - ETA: 7s - loss: 1.7124 - acc: 0.664 - ETA: 7s - loss: 1.7108 - acc: 0.665 - ETA: 6s - loss: 1.7137 - acc: 0.664 - ETA: 6s - loss: 1.7147 - acc: 0.663 - ETA: 6s - loss: 1.7173 - acc: 0.662 - ETA: 6s - loss: 1.7157 - acc: 0.662 - ETA: 6s - loss: 1.7160 - acc: 0.662 - ETA: 6s - loss: 1.7174 - acc: 0.663 - ETA: 6s - loss: 1.7206 - acc: 0.662 - ETA: 5s - loss: 1.7178 - acc: 0.663 - ETA: 5s - loss: 1.7178 - acc: 0.663 - ETA: 5s - loss: 1.7203 - acc: 0.662 - ETA: 5s - loss: 1.7197 - acc: 0.662 - ETA: 5s - loss: 1.7214 - acc: 0.662 - ETA: 5s - loss: 1.7221 - acc: 0.661 - ETA: 5s - loss: 1.7228 - acc: 0.661 - ETA: 4s - loss: 1.7231 - acc: 0.661 - ETA: 4s - loss: 1.7225 - acc: 0.661 - ETA: 4s - loss: 1.7222 - acc: 0.661 - ETA: 4s - loss: 1.7221 - acc: 0.661 - ETA: 4s - loss: 1.7218 - acc: 0.661 - ETA: 4s - loss: 1.7198 - acc: 0.661 - ETA: 4s - loss: 1.7217 - acc: 0.661 - ETA: 3s - loss: 1.7227 - acc: 0.661 - ETA: 3s - loss: 1.7234 - acc: 0.660 - ETA: 3s - loss: 1.7209 - acc: 0.661 - ETA: 3s - loss: 1.7217 - acc: 0.661 - ETA: 3s - loss: 1.7220 - acc: 0.661 - ETA: 3s - loss: 1.7212 - acc: 0.661 - ETA: 3s - loss: 1.7214 - acc: 0.661 - ETA: 2s - loss: 1.7199 - acc: 0.661 - ETA: 2s - loss: 1.7218 - acc: 0.661 - ETA: 2s - loss: 1.7213 - acc: 0.661 - ETA: 2s - loss: 1.7230 - acc: 0.661 - ETA: 2s - loss: 1.7240 - acc: 0.661 - ETA: 2s - loss: 1.7237 - acc: 0.661 - ETA: 1s - loss: 1.7252 - acc: 0.660 - ETA: 1s - loss: 1.7256 - acc: 0.660 - ETA: 1s - loss: 1.7231 - acc: 0.661 - ETA: 1s - loss: 1.7234 - acc: 0.661 - ETA: 1s - loss: 1.7228 - acc: 0.661 - ETA: 1s - loss: 1.7235 - acc: 0.661 - ETA: 1s - loss: 1.7235 - acc: 0.661 - ETA: 0s - loss: 1.7229 - acc: 0.661 - ETA: 0s - loss: 1.7238 - acc: 0.661 - ETA: 0s - loss: 1.7245 - acc: 0.661 - ETA: 0s - loss: 1.7244 - acc: 0.661 - ETA: 0s - loss: 1.7246 - acc: 0.661 - ETA: 0s - loss: 1.7257 - acc: 0.661 - ETA: 0s - loss: 1.7263 - acc: 0.660 - 12s 654us/step - loss: 1.7268 - acc: 0.6608 - val_loss: 2.8697 - val_acc: 0.5708\n",
      "Epoch 24/200\n",
      "18357/18357 [==============================] - ETA: 9s - loss: 1.6777 - acc: 0.654 - ETA: 9s - loss: 1.6233 - acc: 0.666 - ETA: 9s - loss: 1.6181 - acc: 0.668 - ETA: 9s - loss: 1.6172 - acc: 0.667 - ETA: 9s - loss: 1.6111 - acc: 0.669 - ETA: 9s - loss: 1.6095 - acc: 0.671 - ETA: 9s - loss: 1.5988 - acc: 0.674 - ETA: 8s - loss: 1.5946 - acc: 0.674 - ETA: 8s - loss: 1.6018 - acc: 0.673 - ETA: 8s - loss: 1.6012 - acc: 0.674 - ETA: 8s - loss: 1.6002 - acc: 0.675 - ETA: 8s - loss: 1.6060 - acc: 0.674 - ETA: 8s - loss: 1.6053 - acc: 0.674 - ETA: 8s - loss: 1.6083 - acc: 0.674 - ETA: 8s - loss: 1.6038 - acc: 0.675 - ETA: 8s - loss: 1.6022 - acc: 0.675 - ETA: 7s - loss: 1.6028 - acc: 0.676 - ETA: 7s - loss: 1.6018 - acc: 0.677 - ETA: 7s - loss: 1.6016 - acc: 0.677 - ETA: 7s - loss: 1.5981 - acc: 0.678 - ETA: 7s - loss: 1.5946 - acc: 0.678 - ETA: 7s - loss: 1.5963 - acc: 0.678 - ETA: 7s - loss: 1.6003 - acc: 0.678 - ETA: 6s - loss: 1.6035 - acc: 0.677 - ETA: 6s - loss: 1.6041 - acc: 0.677 - ETA: 6s - loss: 1.6008 - acc: 0.678 - ETA: 6s - loss: 1.6003 - acc: 0.678 - ETA: 6s - loss: 1.6018 - acc: 0.679 - ETA: 6s - loss: 1.6022 - acc: 0.678 - ETA: 6s - loss: 1.6021 - acc: 0.678 - ETA: 5s - loss: 1.6002 - acc: 0.678 - ETA: 5s - loss: 1.5989 - acc: 0.679 - ETA: 5s - loss: 1.6006 - acc: 0.679 - ETA: 5s - loss: 1.6030 - acc: 0.679 - ETA: 5s - loss: 1.6043 - acc: 0.679 - ETA: 5s - loss: 1.6054 - acc: 0.678 - ETA: 5s - loss: 1.6040 - acc: 0.679 - ETA: 4s - loss: 1.6035 - acc: 0.679 - ETA: 4s - loss: 1.6045 - acc: 0.679 - ETA: 4s - loss: 1.6048 - acc: 0.678 - ETA: 4s - loss: 1.6036 - acc: 0.679 - ETA: 4s - loss: 1.6039 - acc: 0.678 - ETA: 4s - loss: 1.6037 - acc: 0.678 - ETA: 4s - loss: 1.6029 - acc: 0.679 - ETA: 3s - loss: 1.6041 - acc: 0.679 - ETA: 3s - loss: 1.6041 - acc: 0.678 - ETA: 3s - loss: 1.6032 - acc: 0.679 - ETA: 3s - loss: 1.6037 - acc: 0.678 - ETA: 3s - loss: 1.6021 - acc: 0.679 - ETA: 3s - loss: 1.6025 - acc: 0.679 - ETA: 3s - loss: 1.6042 - acc: 0.678 - ETA: 2s - loss: 1.6044 - acc: 0.678 - ETA: 2s - loss: 1.6048 - acc: 0.678 - ETA: 2s - loss: 1.6048 - acc: 0.678 - ETA: 2s - loss: 1.6065 - acc: 0.678 - ETA: 2s - loss: 1.6063 - acc: 0.678 - ETA: 2s - loss: 1.6060 - acc: 0.678 - ETA: 1s - loss: 1.6078 - acc: 0.678 - ETA: 1s - loss: 1.6077 - acc: 0.678 - ETA: 1s - loss: 1.6086 - acc: 0.678 - ETA: 1s - loss: 1.6088 - acc: 0.678 - ETA: 1s - loss: 1.6089 - acc: 0.678 - ETA: 1s - loss: 1.6111 - acc: 0.677 - ETA: 1s - loss: 1.6100 - acc: 0.678 - ETA: 0s - loss: 1.6087 - acc: 0.678 - ETA: 0s - loss: 1.6082 - acc: 0.678 - ETA: 0s - loss: 1.6079 - acc: 0.678 - ETA: 0s - loss: 1.6081 - acc: 0.678 - ETA: 0s - loss: 1.6083 - acc: 0.678 - ETA: 0s - loss: 1.6103 - acc: 0.678 - ETA: 0s - loss: 1.6098 - acc: 0.678 - 12s 653us/step - loss: 1.6099 - acc: 0.6787 - val_loss: 2.8226 - val_acc: 0.5782\n",
      "Epoch 25/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18357/18357 [==============================] - ETA: 10s - loss: 1.5085 - acc: 0.69 - ETA: 9s - loss: 1.5046 - acc: 0.6891 - ETA: 9s - loss: 1.5093 - acc: 0.689 - ETA: 9s - loss: 1.4861 - acc: 0.695 - ETA: 9s - loss: 1.4834 - acc: 0.694 - ETA: 9s - loss: 1.4818 - acc: 0.694 - ETA: 9s - loss: 1.4804 - acc: 0.695 - ETA: 9s - loss: 1.4799 - acc: 0.696 - ETA: 8s - loss: 1.4873 - acc: 0.695 - ETA: 8s - loss: 1.4981 - acc: 0.692 - ETA: 8s - loss: 1.4920 - acc: 0.693 - ETA: 8s - loss: 1.4911 - acc: 0.694 - ETA: 8s - loss: 1.4879 - acc: 0.694 - ETA: 8s - loss: 1.4859 - acc: 0.695 - ETA: 8s - loss: 1.4816 - acc: 0.697 - ETA: 7s - loss: 1.4835 - acc: 0.697 - ETA: 7s - loss: 1.4830 - acc: 0.697 - ETA: 7s - loss: 1.4831 - acc: 0.696 - ETA: 7s - loss: 1.4868 - acc: 0.696 - ETA: 7s - loss: 1.4842 - acc: 0.697 - ETA: 7s - loss: 1.4875 - acc: 0.697 - ETA: 7s - loss: 1.4900 - acc: 0.696 - ETA: 6s - loss: 1.4880 - acc: 0.696 - ETA: 6s - loss: 1.4883 - acc: 0.696 - ETA: 6s - loss: 1.4892 - acc: 0.696 - ETA: 6s - loss: 1.4923 - acc: 0.695 - ETA: 6s - loss: 1.4946 - acc: 0.695 - ETA: 6s - loss: 1.4941 - acc: 0.696 - ETA: 6s - loss: 1.4930 - acc: 0.696 - ETA: 5s - loss: 1.4939 - acc: 0.696 - ETA: 5s - loss: 1.4946 - acc: 0.696 - ETA: 5s - loss: 1.4932 - acc: 0.696 - ETA: 5s - loss: 1.4936 - acc: 0.696 - ETA: 5s - loss: 1.4901 - acc: 0.696 - ETA: 5s - loss: 1.4897 - acc: 0.696 - ETA: 5s - loss: 1.4912 - acc: 0.696 - ETA: 4s - loss: 1.4924 - acc: 0.696 - ETA: 4s - loss: 1.4965 - acc: 0.695 - ETA: 4s - loss: 1.4977 - acc: 0.695 - ETA: 4s - loss: 1.4968 - acc: 0.695 - ETA: 4s - loss: 1.4965 - acc: 0.695 - ETA: 4s - loss: 1.4960 - acc: 0.695 - ETA: 4s - loss: 1.4956 - acc: 0.695 - ETA: 4s - loss: 1.4965 - acc: 0.695 - ETA: 3s - loss: 1.4975 - acc: 0.695 - ETA: 3s - loss: 1.4976 - acc: 0.695 - ETA: 3s - loss: 1.4974 - acc: 0.695 - ETA: 3s - loss: 1.4965 - acc: 0.695 - ETA: 3s - loss: 1.4996 - acc: 0.694 - ETA: 3s - loss: 1.5006 - acc: 0.694 - ETA: 3s - loss: 1.5010 - acc: 0.694 - ETA: 2s - loss: 1.5011 - acc: 0.694 - ETA: 2s - loss: 1.5019 - acc: 0.693 - ETA: 2s - loss: 1.5000 - acc: 0.694 - ETA: 2s - loss: 1.5011 - acc: 0.694 - ETA: 2s - loss: 1.5020 - acc: 0.694 - ETA: 2s - loss: 1.5018 - acc: 0.694 - ETA: 2s - loss: 1.5018 - acc: 0.694 - ETA: 1s - loss: 1.5013 - acc: 0.694 - ETA: 1s - loss: 1.5002 - acc: 0.694 - ETA: 1s - loss: 1.5013 - acc: 0.694 - ETA: 1s - loss: 1.5014 - acc: 0.694 - ETA: 1s - loss: 1.5023 - acc: 0.694 - ETA: 1s - loss: 1.5032 - acc: 0.694 - ETA: 1s - loss: 1.5030 - acc: 0.694 - ETA: 0s - loss: 1.5035 - acc: 0.694 - ETA: 0s - loss: 1.5039 - acc: 0.694 - ETA: 0s - loss: 1.5034 - acc: 0.694 - ETA: 0s - loss: 1.5034 - acc: 0.693 - ETA: 0s - loss: 1.5033 - acc: 0.694 - ETA: 0s - loss: 1.5026 - acc: 0.694 - 12s 670us/step - loss: 1.5019 - acc: 0.6944 - val_loss: 2.7811 - val_acc: 0.5861\n",
      "Epoch 26/200\n",
      "18357/18357 [==============================] - ETA: 9s - loss: 1.4539 - acc: 0.693 - ETA: 9s - loss: 1.3972 - acc: 0.708 - ETA: 9s - loss: 1.3997 - acc: 0.711 - ETA: 9s - loss: 1.3931 - acc: 0.715 - ETA: 9s - loss: 1.3883 - acc: 0.715 - ETA: 9s - loss: 1.3822 - acc: 0.717 - ETA: 9s - loss: 1.3844 - acc: 0.716 - ETA: 9s - loss: 1.3816 - acc: 0.716 - ETA: 8s - loss: 1.3671 - acc: 0.720 - ETA: 8s - loss: 1.3718 - acc: 0.719 - ETA: 8s - loss: 1.3725 - acc: 0.718 - ETA: 8s - loss: 1.3727 - acc: 0.718 - ETA: 8s - loss: 1.3701 - acc: 0.719 - ETA: 8s - loss: 1.3713 - acc: 0.719 - ETA: 8s - loss: 1.3716 - acc: 0.719 - ETA: 7s - loss: 1.3740 - acc: 0.718 - ETA: 7s - loss: 1.3765 - acc: 0.717 - ETA: 7s - loss: 1.3820 - acc: 0.716 - ETA: 7s - loss: 1.3819 - acc: 0.716 - ETA: 7s - loss: 1.3835 - acc: 0.716 - ETA: 7s - loss: 1.3858 - acc: 0.715 - ETA: 7s - loss: 1.3859 - acc: 0.714 - ETA: 7s - loss: 1.3854 - acc: 0.714 - ETA: 6s - loss: 1.3873 - acc: 0.714 - ETA: 6s - loss: 1.3862 - acc: 0.714 - ETA: 6s - loss: 1.3828 - acc: 0.714 - ETA: 6s - loss: 1.3824 - acc: 0.714 - ETA: 6s - loss: 1.3844 - acc: 0.714 - ETA: 6s - loss: 1.3827 - acc: 0.714 - ETA: 5s - loss: 1.3813 - acc: 0.715 - ETA: 5s - loss: 1.3803 - acc: 0.715 - ETA: 5s - loss: 1.3840 - acc: 0.714 - ETA: 5s - loss: 1.3826 - acc: 0.714 - ETA: 5s - loss: 1.3824 - acc: 0.715 - ETA: 5s - loss: 1.3863 - acc: 0.714 - ETA: 5s - loss: 1.3893 - acc: 0.714 - ETA: 4s - loss: 1.3873 - acc: 0.715 - ETA: 4s - loss: 1.3883 - acc: 0.714 - ETA: 4s - loss: 1.3858 - acc: 0.715 - ETA: 4s - loss: 1.3879 - acc: 0.715 - ETA: 4s - loss: 1.3870 - acc: 0.715 - ETA: 4s - loss: 1.3876 - acc: 0.715 - ETA: 4s - loss: 1.3885 - acc: 0.714 - ETA: 3s - loss: 1.3885 - acc: 0.714 - ETA: 3s - loss: 1.3881 - acc: 0.714 - ETA: 3s - loss: 1.3880 - acc: 0.714 - ETA: 3s - loss: 1.3882 - acc: 0.714 - ETA: 3s - loss: 1.3878 - acc: 0.714 - ETA: 3s - loss: 1.3877 - acc: 0.714 - ETA: 3s - loss: 1.3877 - acc: 0.714 - ETA: 2s - loss: 1.3887 - acc: 0.714 - ETA: 2s - loss: 1.3894 - acc: 0.714 - ETA: 2s - loss: 1.3905 - acc: 0.713 - ETA: 2s - loss: 1.3901 - acc: 0.714 - ETA: 2s - loss: 1.3912 - acc: 0.713 - ETA: 2s - loss: 1.3921 - acc: 0.713 - ETA: 2s - loss: 1.3927 - acc: 0.713 - ETA: 1s - loss: 1.3946 - acc: 0.712 - ETA: 1s - loss: 1.3938 - acc: 0.713 - ETA: 1s - loss: 1.3939 - acc: 0.713 - ETA: 1s - loss: 1.3948 - acc: 0.712 - ETA: 1s - loss: 1.3952 - acc: 0.712 - ETA: 1s - loss: 1.3946 - acc: 0.713 - ETA: 1s - loss: 1.3947 - acc: 0.713 - ETA: 0s - loss: 1.3943 - acc: 0.713 - ETA: 0s - loss: 1.3951 - acc: 0.712 - ETA: 0s - loss: 1.3968 - acc: 0.712 - ETA: 0s - loss: 1.3968 - acc: 0.712 - ETA: 0s - loss: 1.3972 - acc: 0.712 - ETA: 0s - loss: 1.3970 - acc: 0.711 - ETA: 0s - loss: 1.3961 - acc: 0.712 - 12s 652us/step - loss: 1.3962 - acc: 0.7120 - val_loss: 2.7467 - val_acc: 0.5896\n",
      "Epoch 27/200\n",
      "18357/18357 [==============================] - ETA: 10s - loss: 1.2245 - acc: 0.73 - ETA: 10s - loss: 1.2508 - acc: 0.73 - ETA: 10s - loss: 1.2538 - acc: 0.74 - ETA: 9s - loss: 1.2536 - acc: 0.7393 - ETA: 9s - loss: 1.2505 - acc: 0.738 - ETA: 9s - loss: 1.2522 - acc: 0.739 - ETA: 9s - loss: 1.2485 - acc: 0.741 - ETA: 9s - loss: 1.2455 - acc: 0.744 - ETA: 9s - loss: 1.2553 - acc: 0.741 - ETA: 8s - loss: 1.2586 - acc: 0.741 - ETA: 8s - loss: 1.2589 - acc: 0.741 - ETA: 8s - loss: 1.2591 - acc: 0.740 - ETA: 8s - loss: 1.2637 - acc: 0.738 - ETA: 8s - loss: 1.2650 - acc: 0.738 - ETA: 8s - loss: 1.2728 - acc: 0.736 - ETA: 8s - loss: 1.2771 - acc: 0.736 - ETA: 7s - loss: 1.2736 - acc: 0.736 - ETA: 7s - loss: 1.2743 - acc: 0.736 - ETA: 7s - loss: 1.2717 - acc: 0.736 - ETA: 7s - loss: 1.2713 - acc: 0.736 - ETA: 7s - loss: 1.2698 - acc: 0.736 - ETA: 7s - loss: 1.2700 - acc: 0.736 - ETA: 7s - loss: 1.2704 - acc: 0.736 - ETA: 6s - loss: 1.2711 - acc: 0.736 - ETA: 6s - loss: 1.2748 - acc: 0.735 - ETA: 6s - loss: 1.2793 - acc: 0.734 - ETA: 6s - loss: 1.2808 - acc: 0.734 - ETA: 6s - loss: 1.2794 - acc: 0.734 - ETA: 6s - loss: 1.2785 - acc: 0.734 - ETA: 6s - loss: 1.2761 - acc: 0.735 - ETA: 5s - loss: 1.2744 - acc: 0.735 - ETA: 5s - loss: 1.2765 - acc: 0.735 - ETA: 5s - loss: 1.2771 - acc: 0.735 - ETA: 5s - loss: 1.2795 - acc: 0.734 - ETA: 5s - loss: 1.2812 - acc: 0.733 - ETA: 5s - loss: 1.2816 - acc: 0.733 - ETA: 5s - loss: 1.2826 - acc: 0.733 - ETA: 4s - loss: 1.2826 - acc: 0.733 - ETA: 4s - loss: 1.2825 - acc: 0.732 - ETA: 4s - loss: 1.2822 - acc: 0.732 - ETA: 4s - loss: 1.2818 - acc: 0.733 - ETA: 4s - loss: 1.2832 - acc: 0.732 - ETA: 4s - loss: 1.2830 - acc: 0.732 - ETA: 4s - loss: 1.2824 - acc: 0.732 - ETA: 3s - loss: 1.2833 - acc: 0.732 - ETA: 3s - loss: 1.2838 - acc: 0.732 - ETA: 3s - loss: 1.2833 - acc: 0.732 - ETA: 3s - loss: 1.2846 - acc: 0.732 - ETA: 3s - loss: 1.2843 - acc: 0.732 - ETA: 3s - loss: 1.2843 - acc: 0.732 - ETA: 3s - loss: 1.2836 - acc: 0.732 - ETA: 2s - loss: 1.2852 - acc: 0.732 - ETA: 2s - loss: 1.2854 - acc: 0.732 - ETA: 2s - loss: 1.2869 - acc: 0.731 - ETA: 2s - loss: 1.2871 - acc: 0.731 - ETA: 2s - loss: 1.2870 - acc: 0.731 - ETA: 2s - loss: 1.2869 - acc: 0.731 - ETA: 2s - loss: 1.2874 - acc: 0.731 - ETA: 1s - loss: 1.2878 - acc: 0.731 - ETA: 1s - loss: 1.2888 - acc: 0.731 - ETA: 1s - loss: 1.2889 - acc: 0.731 - ETA: 1s - loss: 1.2891 - acc: 0.731 - ETA: 1s - loss: 1.2897 - acc: 0.730 - ETA: 1s - loss: 1.2890 - acc: 0.731 - ETA: 0s - loss: 1.2913 - acc: 0.730 - ETA: 0s - loss: 1.2931 - acc: 0.730 - ETA: 0s - loss: 1.2932 - acc: 0.730 - ETA: 0s - loss: 1.2934 - acc: 0.730 - ETA: 0s - loss: 1.2952 - acc: 0.729 - ETA: 0s - loss: 1.2955 - acc: 0.729 - ETA: 0s - loss: 1.2966 - acc: 0.729 - 12s 665us/step - loss: 1.2976 - acc: 0.7291 - val_loss: 2.7274 - val_acc: 0.5938\n",
      "Epoch 28/200\n",
      "18357/18357 [==============================] - ETA: 9s - loss: 1.1112 - acc: 0.774 - ETA: 9s - loss: 1.1556 - acc: 0.767 - ETA: 9s - loss: 1.1528 - acc: 0.766 - ETA: 9s - loss: 1.1388 - acc: 0.770 - ETA: 9s - loss: 1.1490 - acc: 0.768 - ETA: 9s - loss: 1.1544 - acc: 0.765 - ETA: 9s - loss: 1.1591 - acc: 0.764 - ETA: 9s - loss: 1.1576 - acc: 0.764 - ETA: 9s - loss: 1.1579 - acc: 0.763 - ETA: 8s - loss: 1.1653 - acc: 0.762 - ETA: 8s - loss: 1.1648 - acc: 0.762 - ETA: 8s - loss: 1.1665 - acc: 0.761 - ETA: 8s - loss: 1.1589 - acc: 0.763 - ETA: 8s - loss: 1.1630 - acc: 0.763 - ETA: 8s - loss: 1.1607 - acc: 0.763 - ETA: 8s - loss: 1.1659 - acc: 0.761 - ETA: 7s - loss: 1.1639 - acc: 0.761 - ETA: 7s - loss: 1.1665 - acc: 0.761 - ETA: 7s - loss: 1.1659 - acc: 0.761 - ETA: 7s - loss: 1.1674 - acc: 0.760 - ETA: 7s - loss: 1.1725 - acc: 0.758 - ETA: 7s - loss: 1.1739 - acc: 0.758 - ETA: 7s - loss: 1.1741 - acc: 0.758 - ETA: 6s - loss: 1.1766 - acc: 0.757 - ETA: 6s - loss: 1.1766 - acc: 0.757 - ETA: 6s - loss: 1.1801 - acc: 0.756 - ETA: 6s - loss: 1.1787 - acc: 0.756 - ETA: 6s - loss: 1.1802 - acc: 0.756 - ETA: 6s - loss: 1.1803 - acc: 0.756 - ETA: 6s - loss: 1.1835 - acc: 0.754 - ETA: 5s - loss: 1.1854 - acc: 0.754 - ETA: 5s - loss: 1.1863 - acc: 0.753 - ETA: 5s - loss: 1.1867 - acc: 0.753 - ETA: 5s - loss: 1.1885 - acc: 0.753 - ETA: 5s - loss: 1.1877 - acc: 0.753 - ETA: 5s - loss: 1.1880 - acc: 0.753 - ETA: 5s - loss: 1.1877 - acc: 0.753 - ETA: 4s - loss: 1.1908 - acc: 0.752 - ETA: 4s - loss: 1.1919 - acc: 0.751 - ETA: 4s - loss: 1.1938 - acc: 0.750 - ETA: 4s - loss: 1.1932 - acc: 0.750 - ETA: 4s - loss: 1.1938 - acc: 0.750 - ETA: 4s - loss: 1.1940 - acc: 0.750 - ETA: 4s - loss: 1.1937 - acc: 0.750 - ETA: 3s - loss: 1.1944 - acc: 0.750 - ETA: 3s - loss: 1.1958 - acc: 0.750 - ETA: 3s - loss: 1.1956 - acc: 0.750 - ETA: 3s - loss: 1.1945 - acc: 0.750 - ETA: 3s - loss: 1.1932 - acc: 0.750 - ETA: 3s - loss: 1.1924 - acc: 0.750 - ETA: 3s - loss: 1.1936 - acc: 0.750 - ETA: 2s - loss: 1.1943 - acc: 0.750 - ETA: 2s - loss: 1.1948 - acc: 0.750 - ETA: 2s - loss: 1.1947 - acc: 0.750 - ETA: 2s - loss: 1.1953 - acc: 0.749 - ETA: 2s - loss: 1.1973 - acc: 0.749 - ETA: 2s - loss: 1.1975 - acc: 0.749 - ETA: 2s - loss: 1.1981 - acc: 0.749 - ETA: 1s - loss: 1.1984 - acc: 0.748 - ETA: 1s - loss: 1.1998 - acc: 0.748 - ETA: 1s - loss: 1.1997 - acc: 0.748 - ETA: 1s - loss: 1.1998 - acc: 0.748 - ETA: 1s - loss: 1.1994 - acc: 0.748 - ETA: 1s - loss: 1.1987 - acc: 0.748 - ETA: 0s - loss: 1.1998 - acc: 0.748 - ETA: 0s - loss: 1.2000 - acc: 0.748 - ETA: 0s - loss: 1.2009 - acc: 0.748 - ETA: 0s - loss: 1.2009 - acc: 0.748 - ETA: 0s - loss: 1.2022 - acc: 0.748 - ETA: 0s - loss: 1.2023 - acc: 0.748 - ETA: 0s - loss: 1.2030 - acc: 0.747 - 12s 666us/step - loss: 1.2038 - acc: 0.7476 - val_loss: 2.6926 - val_acc: 0.6005\n",
      "Epoch 29/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18357/18357 [==============================] - ETA: 9s - loss: 1.0968 - acc: 0.775 - ETA: 9s - loss: 1.1061 - acc: 0.769 - ETA: 9s - loss: 1.0914 - acc: 0.774 - ETA: 9s - loss: 1.0722 - acc: 0.779 - ETA: 9s - loss: 1.0831 - acc: 0.778 - ETA: 9s - loss: 1.0747 - acc: 0.780 - ETA: 9s - loss: 1.0746 - acc: 0.779 - ETA: 9s - loss: 1.0721 - acc: 0.780 - ETA: 8s - loss: 1.0746 - acc: 0.780 - ETA: 8s - loss: 1.0716 - acc: 0.780 - ETA: 8s - loss: 1.0731 - acc: 0.781 - ETA: 8s - loss: 1.0720 - acc: 0.781 - ETA: 8s - loss: 1.0727 - acc: 0.780 - ETA: 8s - loss: 1.0770 - acc: 0.778 - ETA: 8s - loss: 1.0834 - acc: 0.777 - ETA: 7s - loss: 1.0829 - acc: 0.776 - ETA: 7s - loss: 1.0838 - acc: 0.776 - ETA: 7s - loss: 1.0816 - acc: 0.776 - ETA: 7s - loss: 1.0800 - acc: 0.777 - ETA: 7s - loss: 1.0805 - acc: 0.777 - ETA: 7s - loss: 1.0812 - acc: 0.777 - ETA: 7s - loss: 1.0828 - acc: 0.776 - ETA: 7s - loss: 1.0812 - acc: 0.776 - ETA: 6s - loss: 1.0833 - acc: 0.775 - ETA: 6s - loss: 1.0853 - acc: 0.774 - ETA: 6s - loss: 1.0841 - acc: 0.774 - ETA: 6s - loss: 1.0856 - acc: 0.774 - ETA: 6s - loss: 1.0870 - acc: 0.773 - ETA: 6s - loss: 1.0855 - acc: 0.774 - ETA: 5s - loss: 1.0874 - acc: 0.773 - ETA: 5s - loss: 1.0887 - acc: 0.773 - ETA: 5s - loss: 1.0879 - acc: 0.773 - ETA: 5s - loss: 1.0911 - acc: 0.772 - ETA: 5s - loss: 1.0922 - acc: 0.772 - ETA: 5s - loss: 1.0930 - acc: 0.771 - ETA: 5s - loss: 1.0933 - acc: 0.771 - ETA: 4s - loss: 1.0949 - acc: 0.771 - ETA: 4s - loss: 1.0969 - acc: 0.770 - ETA: 4s - loss: 1.0974 - acc: 0.770 - ETA: 4s - loss: 1.0990 - acc: 0.770 - ETA: 4s - loss: 1.0998 - acc: 0.769 - ETA: 4s - loss: 1.1019 - acc: 0.769 - ETA: 4s - loss: 1.1033 - acc: 0.768 - ETA: 3s - loss: 1.1038 - acc: 0.768 - ETA: 3s - loss: 1.1062 - acc: 0.768 - ETA: 3s - loss: 1.1056 - acc: 0.768 - ETA: 3s - loss: 1.1057 - acc: 0.768 - ETA: 3s - loss: 1.1082 - acc: 0.767 - ETA: 3s - loss: 1.1096 - acc: 0.767 - ETA: 3s - loss: 1.1105 - acc: 0.767 - ETA: 2s - loss: 1.1106 - acc: 0.767 - ETA: 2s - loss: 1.1100 - acc: 0.767 - ETA: 2s - loss: 1.1107 - acc: 0.767 - ETA: 2s - loss: 1.1116 - acc: 0.766 - ETA: 2s - loss: 1.1127 - acc: 0.766 - ETA: 2s - loss: 1.1130 - acc: 0.766 - ETA: 2s - loss: 1.1130 - acc: 0.766 - ETA: 1s - loss: 1.1126 - acc: 0.766 - ETA: 1s - loss: 1.1113 - acc: 0.766 - ETA: 1s - loss: 1.1107 - acc: 0.767 - ETA: 1s - loss: 1.1099 - acc: 0.767 - ETA: 1s - loss: 1.1085 - acc: 0.767 - ETA: 1s - loss: 1.1095 - acc: 0.767 - ETA: 1s - loss: 1.1105 - acc: 0.767 - ETA: 0s - loss: 1.1115 - acc: 0.766 - ETA: 0s - loss: 1.1127 - acc: 0.766 - ETA: 0s - loss: 1.1129 - acc: 0.766 - ETA: 0s - loss: 1.1129 - acc: 0.766 - ETA: 0s - loss: 1.1150 - acc: 0.766 - ETA: 0s - loss: 1.1157 - acc: 0.766 - ETA: 0s - loss: 1.1170 - acc: 0.765 - 12s 645us/step - loss: 1.1173 - acc: 0.7656 - val_loss: 2.6632 - val_acc: 0.6041\n",
      "Epoch 30/200\n",
      "18357/18357 [==============================] - ETA: 9s - loss: 1.0750 - acc: 0.785 - ETA: 9s - loss: 1.0192 - acc: 0.798 - ETA: 9s - loss: 1.0291 - acc: 0.795 - ETA: 9s - loss: 1.0235 - acc: 0.791 - ETA: 9s - loss: 1.0094 - acc: 0.794 - ETA: 9s - loss: 1.0071 - acc: 0.794 - ETA: 9s - loss: 0.9975 - acc: 0.795 - ETA: 9s - loss: 1.0000 - acc: 0.795 - ETA: 9s - loss: 1.0009 - acc: 0.795 - ETA: 8s - loss: 1.0033 - acc: 0.793 - ETA: 8s - loss: 1.0038 - acc: 0.793 - ETA: 8s - loss: 0.9998 - acc: 0.793 - ETA: 8s - loss: 1.0002 - acc: 0.793 - ETA: 8s - loss: 1.0045 - acc: 0.793 - ETA: 8s - loss: 1.0065 - acc: 0.792 - ETA: 8s - loss: 1.0047 - acc: 0.793 - ETA: 7s - loss: 1.0070 - acc: 0.792 - ETA: 7s - loss: 1.0072 - acc: 0.792 - ETA: 7s - loss: 1.0073 - acc: 0.792 - ETA: 7s - loss: 1.0060 - acc: 0.792 - ETA: 7s - loss: 1.0067 - acc: 0.792 - ETA: 7s - loss: 1.0034 - acc: 0.793 - ETA: 7s - loss: 1.0019 - acc: 0.793 - ETA: 6s - loss: 1.0044 - acc: 0.793 - ETA: 6s - loss: 1.0033 - acc: 0.793 - ETA: 6s - loss: 1.0019 - acc: 0.793 - ETA: 6s - loss: 1.0035 - acc: 0.793 - ETA: 6s - loss: 1.0068 - acc: 0.792 - ETA: 6s - loss: 1.0107 - acc: 0.792 - ETA: 6s - loss: 1.0113 - acc: 0.791 - ETA: 5s - loss: 1.0132 - acc: 0.791 - ETA: 5s - loss: 1.0141 - acc: 0.790 - ETA: 5s - loss: 1.0158 - acc: 0.790 - ETA: 5s - loss: 1.0175 - acc: 0.790 - ETA: 5s - loss: 1.0181 - acc: 0.789 - ETA: 5s - loss: 1.0174 - acc: 0.789 - ETA: 5s - loss: 1.0167 - acc: 0.789 - ETA: 4s - loss: 1.0182 - acc: 0.789 - ETA: 4s - loss: 1.0181 - acc: 0.788 - ETA: 4s - loss: 1.0205 - acc: 0.788 - ETA: 4s - loss: 1.0210 - acc: 0.787 - ETA: 4s - loss: 1.0236 - acc: 0.787 - ETA: 4s - loss: 1.0244 - acc: 0.787 - ETA: 4s - loss: 1.0245 - acc: 0.787 - ETA: 3s - loss: 1.0266 - acc: 0.786 - ETA: 3s - loss: 1.0273 - acc: 0.785 - ETA: 3s - loss: 1.0292 - acc: 0.785 - ETA: 3s - loss: 1.0310 - acc: 0.784 - ETA: 3s - loss: 1.0308 - acc: 0.784 - ETA: 3s - loss: 1.0295 - acc: 0.785 - ETA: 3s - loss: 1.0305 - acc: 0.784 - ETA: 2s - loss: 1.0320 - acc: 0.784 - ETA: 2s - loss: 1.0320 - acc: 0.784 - ETA: 2s - loss: 1.0335 - acc: 0.783 - ETA: 2s - loss: 1.0330 - acc: 0.783 - ETA: 2s - loss: 1.0341 - acc: 0.783 - ETA: 2s - loss: 1.0339 - acc: 0.783 - ETA: 2s - loss: 1.0343 - acc: 0.783 - ETA: 1s - loss: 1.0340 - acc: 0.783 - ETA: 1s - loss: 1.0343 - acc: 0.783 - ETA: 1s - loss: 1.0343 - acc: 0.783 - ETA: 1s - loss: 1.0355 - acc: 0.782 - ETA: 1s - loss: 1.0353 - acc: 0.782 - ETA: 1s - loss: 1.0363 - acc: 0.782 - ETA: 0s - loss: 1.0368 - acc: 0.782 - ETA: 0s - loss: 1.0368 - acc: 0.782 - ETA: 0s - loss: 1.0379 - acc: 0.781 - ETA: 0s - loss: 1.0369 - acc: 0.781 - ETA: 0s - loss: 1.0369 - acc: 0.781 - ETA: 0s - loss: 1.0382 - acc: 0.781 - ETA: 0s - loss: 1.0386 - acc: 0.781 - 12s 665us/step - loss: 1.0387 - acc: 0.7813 - val_loss: 2.6452 - val_acc: 0.6102\n",
      "Epoch 31/200\n",
      "18357/18357 [==============================] - ETA: 9s - loss: 0.9601 - acc: 0.813 - ETA: 9s - loss: 0.9386 - acc: 0.818 - ETA: 9s - loss: 0.9483 - acc: 0.812 - ETA: 9s - loss: 0.9280 - acc: 0.818 - ETA: 9s - loss: 0.9401 - acc: 0.815 - ETA: 9s - loss: 0.9256 - acc: 0.818 - ETA: 9s - loss: 0.9263 - acc: 0.817 - ETA: 9s - loss: 0.9286 - acc: 0.816 - ETA: 9s - loss: 0.9315 - acc: 0.814 - ETA: 9s - loss: 0.9320 - acc: 0.813 - ETA: 8s - loss: 0.9293 - acc: 0.814 - ETA: 8s - loss: 0.9332 - acc: 0.813 - ETA: 8s - loss: 0.9342 - acc: 0.812 - ETA: 8s - loss: 0.9398 - acc: 0.810 - ETA: 8s - loss: 0.9406 - acc: 0.811 - ETA: 8s - loss: 0.9407 - acc: 0.810 - ETA: 8s - loss: 0.9403 - acc: 0.809 - ETA: 7s - loss: 0.9374 - acc: 0.810 - ETA: 7s - loss: 0.9392 - acc: 0.809 - ETA: 7s - loss: 0.9371 - acc: 0.809 - ETA: 7s - loss: 0.9394 - acc: 0.809 - ETA: 7s - loss: 0.9400 - acc: 0.809 - ETA: 7s - loss: 0.9389 - acc: 0.809 - ETA: 7s - loss: 0.9382 - acc: 0.809 - ETA: 6s - loss: 0.9383 - acc: 0.809 - ETA: 6s - loss: 0.9399 - acc: 0.808 - ETA: 6s - loss: 0.9423 - acc: 0.808 - ETA: 6s - loss: 0.9412 - acc: 0.808 - ETA: 6s - loss: 0.9428 - acc: 0.807 - ETA: 6s - loss: 0.9416 - acc: 0.807 - ETA: 6s - loss: 0.9429 - acc: 0.806 - ETA: 5s - loss: 0.9424 - acc: 0.806 - ETA: 5s - loss: 0.9414 - acc: 0.806 - ETA: 5s - loss: 0.9424 - acc: 0.806 - ETA: 5s - loss: 0.9412 - acc: 0.806 - ETA: 5s - loss: 0.9435 - acc: 0.805 - ETA: 5s - loss: 0.9452 - acc: 0.805 - ETA: 5s - loss: 0.9460 - acc: 0.805 - ETA: 4s - loss: 0.9469 - acc: 0.804 - ETA: 4s - loss: 0.9482 - acc: 0.804 - ETA: 4s - loss: 0.9496 - acc: 0.803 - ETA: 4s - loss: 0.9501 - acc: 0.803 - ETA: 4s - loss: 0.9515 - acc: 0.803 - ETA: 4s - loss: 0.9519 - acc: 0.803 - ETA: 3s - loss: 0.9522 - acc: 0.803 - ETA: 3s - loss: 0.9528 - acc: 0.802 - ETA: 3s - loss: 0.9525 - acc: 0.802 - ETA: 3s - loss: 0.9508 - acc: 0.803 - ETA: 3s - loss: 0.9513 - acc: 0.803 - ETA: 3s - loss: 0.9516 - acc: 0.803 - ETA: 3s - loss: 0.9532 - acc: 0.802 - ETA: 2s - loss: 0.9533 - acc: 0.802 - ETA: 2s - loss: 0.9539 - acc: 0.802 - ETA: 2s - loss: 0.9545 - acc: 0.801 - ETA: 2s - loss: 0.9551 - acc: 0.801 - ETA: 2s - loss: 0.9548 - acc: 0.801 - ETA: 2s - loss: 0.9548 - acc: 0.801 - ETA: 2s - loss: 0.9569 - acc: 0.801 - ETA: 1s - loss: 0.9571 - acc: 0.801 - ETA: 1s - loss: 0.9573 - acc: 0.800 - ETA: 1s - loss: 0.9570 - acc: 0.800 - ETA: 1s - loss: 0.9570 - acc: 0.800 - ETA: 1s - loss: 0.9576 - acc: 0.800 - ETA: 1s - loss: 0.9577 - acc: 0.799 - ETA: 0s - loss: 0.9584 - acc: 0.799 - ETA: 0s - loss: 0.9586 - acc: 0.799 - ETA: 0s - loss: 0.9590 - acc: 0.799 - ETA: 0s - loss: 0.9593 - acc: 0.799 - ETA: 0s - loss: 0.9592 - acc: 0.799 - ETA: 0s - loss: 0.9604 - acc: 0.798 - ETA: 0s - loss: 0.9609 - acc: 0.798 - 12s 666us/step - loss: 0.9618 - acc: 0.7985 - val_loss: 2.6370 - val_acc: 0.6072\n",
      "Epoch 32/200\n",
      "18357/18357 [==============================] - ETA: 9s - loss: 0.9300 - acc: 0.800 - ETA: 10s - loss: 0.8982 - acc: 0.81 - ETA: 9s - loss: 0.9032 - acc: 0.8138 - ETA: 9s - loss: 0.8842 - acc: 0.817 - ETA: 9s - loss: 0.8769 - acc: 0.820 - ETA: 9s - loss: 0.8723 - acc: 0.822 - ETA: 9s - loss: 0.8719 - acc: 0.824 - ETA: 9s - loss: 0.8774 - acc: 0.822 - ETA: 9s - loss: 0.8809 - acc: 0.822 - ETA: 8s - loss: 0.8776 - acc: 0.822 - ETA: 8s - loss: 0.8790 - acc: 0.822 - ETA: 8s - loss: 0.8719 - acc: 0.824 - ETA: 8s - loss: 0.8752 - acc: 0.823 - ETA: 8s - loss: 0.8750 - acc: 0.822 - ETA: 8s - loss: 0.8712 - acc: 0.823 - ETA: 8s - loss: 0.8760 - acc: 0.822 - ETA: 7s - loss: 0.8742 - acc: 0.822 - ETA: 7s - loss: 0.8721 - acc: 0.822 - ETA: 7s - loss: 0.8702 - acc: 0.823 - ETA: 7s - loss: 0.8723 - acc: 0.822 - ETA: 7s - loss: 0.8739 - acc: 0.821 - ETA: 7s - loss: 0.8776 - acc: 0.820 - ETA: 7s - loss: 0.8790 - acc: 0.820 - ETA: 6s - loss: 0.8788 - acc: 0.819 - ETA: 6s - loss: 0.8798 - acc: 0.819 - ETA: 6s - loss: 0.8804 - acc: 0.819 - ETA: 6s - loss: 0.8803 - acc: 0.819 - ETA: 6s - loss: 0.8803 - acc: 0.818 - ETA: 6s - loss: 0.8800 - acc: 0.818 - ETA: 6s - loss: 0.8772 - acc: 0.818 - ETA: 5s - loss: 0.8763 - acc: 0.818 - ETA: 5s - loss: 0.8763 - acc: 0.819 - ETA: 5s - loss: 0.8770 - acc: 0.819 - ETA: 5s - loss: 0.8781 - acc: 0.818 - ETA: 5s - loss: 0.8782 - acc: 0.818 - ETA: 5s - loss: 0.8789 - acc: 0.818 - ETA: 5s - loss: 0.8799 - acc: 0.818 - ETA: 4s - loss: 0.8793 - acc: 0.818 - ETA: 4s - loss: 0.8797 - acc: 0.818 - ETA: 4s - loss: 0.8810 - acc: 0.818 - ETA: 4s - loss: 0.8816 - acc: 0.817 - ETA: 4s - loss: 0.8826 - acc: 0.817 - ETA: 4s - loss: 0.8823 - acc: 0.817 - ETA: 4s - loss: 0.8824 - acc: 0.817 - ETA: 3s - loss: 0.8827 - acc: 0.817 - ETA: 3s - loss: 0.8831 - acc: 0.816 - ETA: 3s - loss: 0.8834 - acc: 0.816 - ETA: 3s - loss: 0.8840 - acc: 0.816 - ETA: 3s - loss: 0.8842 - acc: 0.816 - ETA: 3s - loss: 0.8855 - acc: 0.816 - ETA: 3s - loss: 0.8863 - acc: 0.815 - ETA: 2s - loss: 0.8877 - acc: 0.815 - ETA: 2s - loss: 0.8887 - acc: 0.815 - ETA: 2s - loss: 0.8894 - acc: 0.815 - ETA: 2s - loss: 0.8881 - acc: 0.815 - ETA: 2s - loss: 0.8883 - acc: 0.815 - ETA: 2s - loss: 0.8886 - acc: 0.815 - ETA: 2s - loss: 0.8895 - acc: 0.814 - ETA: 1s - loss: 0.8899 - acc: 0.814 - ETA: 1s - loss: 0.8912 - acc: 0.814 - ETA: 1s - loss: 0.8919 - acc: 0.813 - ETA: 1s - loss: 0.8927 - acc: 0.813 - ETA: 1s - loss: 0.8919 - acc: 0.813 - ETA: 1s - loss: 0.8919 - acc: 0.813 - ETA: 0s - loss: 0.8925 - acc: 0.813 - ETA: 0s - loss: 0.8921 - acc: 0.813 - ETA: 0s - loss: 0.8923 - acc: 0.813 - ETA: 0s - loss: 0.8927 - acc: 0.813 - ETA: 0s - loss: 0.8920 - acc: 0.813 - ETA: 0s - loss: 0.8925 - acc: 0.813 - ETA: 0s - loss: 0.8934 - acc: 0.813 - 12s 665us/step - loss: 0.8939 - acc: 0.8129 - val_loss: 2.6141 - val_acc: 0.6121\n",
      "Epoch 33/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18357/18357 [==============================] - ETA: 9s - loss: 0.8038 - acc: 0.843 - ETA: 9s - loss: 0.7871 - acc: 0.842 - ETA: 9s - loss: 0.7950 - acc: 0.838 - ETA: 9s - loss: 0.7739 - acc: 0.843 - ETA: 9s - loss: 0.7742 - acc: 0.843 - ETA: 9s - loss: 0.7838 - acc: 0.839 - ETA: 9s - loss: 0.7894 - acc: 0.837 - ETA: 9s - loss: 0.7877 - acc: 0.838 - ETA: 9s - loss: 0.7965 - acc: 0.835 - ETA: 8s - loss: 0.7977 - acc: 0.837 - ETA: 8s - loss: 0.7937 - acc: 0.839 - ETA: 8s - loss: 0.7999 - acc: 0.838 - ETA: 8s - loss: 0.7997 - acc: 0.837 - ETA: 8s - loss: 0.7973 - acc: 0.838 - ETA: 8s - loss: 0.7983 - acc: 0.837 - ETA: 8s - loss: 0.7988 - acc: 0.837 - ETA: 8s - loss: 0.7955 - acc: 0.837 - ETA: 7s - loss: 0.7971 - acc: 0.836 - ETA: 7s - loss: 0.7968 - acc: 0.837 - ETA: 7s - loss: 0.7958 - acc: 0.837 - ETA: 7s - loss: 0.7975 - acc: 0.837 - ETA: 7s - loss: 0.7980 - acc: 0.837 - ETA: 7s - loss: 0.7989 - acc: 0.836 - ETA: 7s - loss: 0.7983 - acc: 0.836 - ETA: 7s - loss: 0.7991 - acc: 0.836 - ETA: 6s - loss: 0.7990 - acc: 0.836 - ETA: 6s - loss: 0.7980 - acc: 0.836 - ETA: 6s - loss: 0.7973 - acc: 0.836 - ETA: 6s - loss: 0.8002 - acc: 0.835 - ETA: 6s - loss: 0.8014 - acc: 0.834 - ETA: 6s - loss: 0.8021 - acc: 0.834 - ETA: 5s - loss: 0.8039 - acc: 0.833 - ETA: 5s - loss: 0.8038 - acc: 0.833 - ETA: 5s - loss: 0.8043 - acc: 0.833 - ETA: 5s - loss: 0.8039 - acc: 0.834 - ETA: 5s - loss: 0.8032 - acc: 0.834 - ETA: 5s - loss: 0.8051 - acc: 0.834 - ETA: 5s - loss: 0.8051 - acc: 0.834 - ETA: 5s - loss: 0.8068 - acc: 0.834 - ETA: 4s - loss: 0.8064 - acc: 0.834 - ETA: 4s - loss: 0.8068 - acc: 0.834 - ETA: 4s - loss: 0.8078 - acc: 0.833 - ETA: 4s - loss: 0.8081 - acc: 0.833 - ETA: 4s - loss: 0.8092 - acc: 0.832 - ETA: 4s - loss: 0.8094 - acc: 0.832 - ETA: 3s - loss: 0.8094 - acc: 0.832 - ETA: 3s - loss: 0.8112 - acc: 0.831 - ETA: 3s - loss: 0.8121 - acc: 0.831 - ETA: 3s - loss: 0.8122 - acc: 0.831 - ETA: 3s - loss: 0.8139 - acc: 0.831 - ETA: 3s - loss: 0.8155 - acc: 0.830 - ETA: 2s - loss: 0.8163 - acc: 0.830 - ETA: 2s - loss: 0.8165 - acc: 0.830 - ETA: 2s - loss: 0.8169 - acc: 0.830 - ETA: 2s - loss: 0.8178 - acc: 0.829 - ETA: 2s - loss: 0.8182 - acc: 0.829 - ETA: 2s - loss: 0.8190 - acc: 0.829 - ETA: 2s - loss: 0.8187 - acc: 0.829 - ETA: 1s - loss: 0.8199 - acc: 0.828 - ETA: 1s - loss: 0.8205 - acc: 0.828 - ETA: 1s - loss: 0.8205 - acc: 0.828 - ETA: 1s - loss: 0.8214 - acc: 0.828 - ETA: 1s - loss: 0.8230 - acc: 0.827 - ETA: 1s - loss: 0.8250 - acc: 0.827 - ETA: 1s - loss: 0.8271 - acc: 0.826 - ETA: 0s - loss: 0.8282 - acc: 0.826 - ETA: 0s - loss: 0.8284 - acc: 0.826 - ETA: 0s - loss: 0.8301 - acc: 0.826 - ETA: 0s - loss: 0.8312 - acc: 0.826 - ETA: 0s - loss: 0.8315 - acc: 0.826 - ETA: 0s - loss: 0.8312 - acc: 0.826 - 13s 721us/step - loss: 0.8318 - acc: 0.8258 - val_loss: 2.6125 - val_acc: 0.6134\n",
      "Epoch 34/200\n",
      "18357/18357 [==============================] - ETA: 13s - loss: 0.7717 - acc: 0.82 - ETA: 12s - loss: 0.7771 - acc: 0.82 - ETA: 12s - loss: 0.7669 - acc: 0.83 - ETA: 12s - loss: 0.7560 - acc: 0.83 - ETA: 12s - loss: 0.7611 - acc: 0.84 - ETA: 12s - loss: 0.7524 - acc: 0.84 - ETA: 11s - loss: 0.7604 - acc: 0.84 - ETA: 11s - loss: 0.7570 - acc: 0.84 - ETA: 11s - loss: 0.7562 - acc: 0.84 - ETA: 11s - loss: 0.7601 - acc: 0.84 - ETA: 11s - loss: 0.7644 - acc: 0.84 - ETA: 11s - loss: 0.7583 - acc: 0.84 - ETA: 11s - loss: 0.7578 - acc: 0.84 - ETA: 10s - loss: 0.7619 - acc: 0.84 - ETA: 10s - loss: 0.7601 - acc: 0.84 - ETA: 10s - loss: 0.7618 - acc: 0.84 - ETA: 10s - loss: 0.7592 - acc: 0.84 - ETA: 10s - loss: 0.7650 - acc: 0.84 - ETA: 10s - loss: 0.7645 - acc: 0.84 - ETA: 10s - loss: 0.7650 - acc: 0.84 - ETA: 10s - loss: 0.7643 - acc: 0.84 - ETA: 10s - loss: 0.7651 - acc: 0.84 - ETA: 9s - loss: 0.7658 - acc: 0.8428 - ETA: 9s - loss: 0.7640 - acc: 0.843 - ETA: 9s - loss: 0.7638 - acc: 0.842 - ETA: 9s - loss: 0.7644 - acc: 0.842 - ETA: 9s - loss: 0.7654 - acc: 0.842 - ETA: 9s - loss: 0.7647 - acc: 0.841 - ETA: 8s - loss: 0.7628 - acc: 0.842 - ETA: 8s - loss: 0.7634 - acc: 0.842 - ETA: 8s - loss: 0.7623 - acc: 0.842 - ETA: 8s - loss: 0.7626 - acc: 0.841 - ETA: 8s - loss: 0.7624 - acc: 0.841 - ETA: 7s - loss: 0.7627 - acc: 0.841 - ETA: 7s - loss: 0.7650 - acc: 0.840 - ETA: 7s - loss: 0.7651 - acc: 0.840 - ETA: 7s - loss: 0.7664 - acc: 0.839 - ETA: 6s - loss: 0.7646 - acc: 0.840 - ETA: 6s - loss: 0.7643 - acc: 0.840 - ETA: 6s - loss: 0.7654 - acc: 0.839 - ETA: 6s - loss: 0.7655 - acc: 0.839 - ETA: 6s - loss: 0.7646 - acc: 0.839 - ETA: 5s - loss: 0.7643 - acc: 0.839 - ETA: 5s - loss: 0.7651 - acc: 0.839 - ETA: 5s - loss: 0.7643 - acc: 0.839 - ETA: 5s - loss: 0.7633 - acc: 0.839 - ETA: 4s - loss: 0.7633 - acc: 0.839 - ETA: 4s - loss: 0.7631 - acc: 0.839 - ETA: 4s - loss: 0.7654 - acc: 0.838 - ETA: 4s - loss: 0.7656 - acc: 0.838 - ETA: 4s - loss: 0.7657 - acc: 0.838 - ETA: 3s - loss: 0.7648 - acc: 0.838 - ETA: 3s - loss: 0.7659 - acc: 0.838 - ETA: 3s - loss: 0.7664 - acc: 0.838 - ETA: 3s - loss: 0.7671 - acc: 0.838 - ETA: 3s - loss: 0.7676 - acc: 0.837 - ETA: 2s - loss: 0.7676 - acc: 0.837 - ETA: 2s - loss: 0.7679 - acc: 0.837 - ETA: 2s - loss: 0.7691 - acc: 0.837 - ETA: 2s - loss: 0.7692 - acc: 0.837 - ETA: 2s - loss: 0.7694 - acc: 0.837 - ETA: 1s - loss: 0.7694 - acc: 0.837 - ETA: 1s - loss: 0.7704 - acc: 0.836 - ETA: 1s - loss: 0.7705 - acc: 0.836 - ETA: 1s - loss: 0.7700 - acc: 0.836 - ETA: 1s - loss: 0.7706 - acc: 0.836 - ETA: 0s - loss: 0.7703 - acc: 0.836 - ETA: 0s - loss: 0.7709 - acc: 0.836 - ETA: 0s - loss: 0.7729 - acc: 0.835 - ETA: 0s - loss: 0.7736 - acc: 0.835 - ETA: 0s - loss: 0.7736 - acc: 0.835 - 14s 788us/step - loss: 0.7750 - acc: 0.8349 - val_loss: 2.5969 - val_acc: 0.6167\n",
      "Epoch 35/200\n",
      "18357/18357 [==============================] - ETA: 9s - loss: 0.6831 - acc: 0.854 - ETA: 9s - loss: 0.6769 - acc: 0.865 - ETA: 9s - loss: 0.6819 - acc: 0.864 - ETA: 9s - loss: 0.6913 - acc: 0.860 - ETA: 9s - loss: 0.6955 - acc: 0.860 - ETA: 9s - loss: 0.7016 - acc: 0.857 - ETA: 9s - loss: 0.7111 - acc: 0.855 - ETA: 8s - loss: 0.7159 - acc: 0.854 - ETA: 8s - loss: 0.7111 - acc: 0.855 - ETA: 8s - loss: 0.7087 - acc: 0.853 - ETA: 8s - loss: 0.7132 - acc: 0.852 - ETA: 8s - loss: 0.7111 - acc: 0.852 - ETA: 8s - loss: 0.7096 - acc: 0.852 - ETA: 8s - loss: 0.7075 - acc: 0.852 - ETA: 7s - loss: 0.7037 - acc: 0.853 - ETA: 7s - loss: 0.6999 - acc: 0.854 - ETA: 7s - loss: 0.7017 - acc: 0.853 - ETA: 7s - loss: 0.7025 - acc: 0.852 - ETA: 7s - loss: 0.7055 - acc: 0.852 - ETA: 7s - loss: 0.7062 - acc: 0.851 - ETA: 7s - loss: 0.7044 - acc: 0.852 - ETA: 7s - loss: 0.7036 - acc: 0.852 - ETA: 6s - loss: 0.7036 - acc: 0.852 - ETA: 6s - loss: 0.7043 - acc: 0.851 - ETA: 6s - loss: 0.7028 - acc: 0.851 - ETA: 6s - loss: 0.7006 - acc: 0.852 - ETA: 6s - loss: 0.7012 - acc: 0.852 - ETA: 6s - loss: 0.7026 - acc: 0.852 - ETA: 6s - loss: 0.7031 - acc: 0.852 - ETA: 6s - loss: 0.7038 - acc: 0.852 - ETA: 5s - loss: 0.7041 - acc: 0.852 - ETA: 5s - loss: 0.7031 - acc: 0.853 - ETA: 5s - loss: 0.7034 - acc: 0.852 - ETA: 5s - loss: 0.7036 - acc: 0.852 - ETA: 5s - loss: 0.7060 - acc: 0.851 - ETA: 5s - loss: 0.7060 - acc: 0.851 - ETA: 5s - loss: 0.7054 - acc: 0.851 - ETA: 5s - loss: 0.7066 - acc: 0.850 - ETA: 5s - loss: 0.7064 - acc: 0.850 - ETA: 4s - loss: 0.7071 - acc: 0.850 - ETA: 4s - loss: 0.7062 - acc: 0.850 - ETA: 4s - loss: 0.7077 - acc: 0.850 - ETA: 4s - loss: 0.7086 - acc: 0.850 - ETA: 4s - loss: 0.7069 - acc: 0.850 - ETA: 4s - loss: 0.7073 - acc: 0.850 - ETA: 4s - loss: 0.7092 - acc: 0.849 - ETA: 3s - loss: 0.7081 - acc: 0.850 - ETA: 3s - loss: 0.7086 - acc: 0.849 - ETA: 3s - loss: 0.7087 - acc: 0.849 - ETA: 3s - loss: 0.7090 - acc: 0.849 - ETA: 3s - loss: 0.7098 - acc: 0.849 - ETA: 3s - loss: 0.7106 - acc: 0.848 - ETA: 3s - loss: 0.7114 - acc: 0.848 - ETA: 2s - loss: 0.7115 - acc: 0.848 - ETA: 2s - loss: 0.7122 - acc: 0.847 - ETA: 2s - loss: 0.7130 - acc: 0.847 - ETA: 2s - loss: 0.7132 - acc: 0.847 - ETA: 2s - loss: 0.7137 - acc: 0.847 - ETA: 2s - loss: 0.7142 - acc: 0.847 - ETA: 1s - loss: 0.7147 - acc: 0.847 - ETA: 1s - loss: 0.7150 - acc: 0.847 - ETA: 1s - loss: 0.7157 - acc: 0.846 - ETA: 1s - loss: 0.7155 - acc: 0.846 - ETA: 1s - loss: 0.7169 - acc: 0.846 - ETA: 1s - loss: 0.7181 - acc: 0.845 - ETA: 0s - loss: 0.7180 - acc: 0.845 - ETA: 0s - loss: 0.7181 - acc: 0.845 - ETA: 0s - loss: 0.7183 - acc: 0.845 - ETA: 0s - loss: 0.7191 - acc: 0.845 - ETA: 0s - loss: 0.7192 - acc: 0.845 - ETA: 0s - loss: 0.7204 - acc: 0.844 - 14s 748us/step - loss: 0.7205 - acc: 0.8446 - val_loss: 2.5912 - val_acc: 0.6164\n",
      "Epoch 36/200\n",
      "18357/18357 [==============================] - ETA: 9s - loss: 0.6631 - acc: 0.851 - ETA: 9s - loss: 0.6445 - acc: 0.862 - ETA: 9s - loss: 0.6416 - acc: 0.867 - ETA: 10s - loss: 0.6492 - acc: 0.86 - ETA: 10s - loss: 0.6401 - acc: 0.86 - ETA: 10s - loss: 0.6347 - acc: 0.86 - ETA: 10s - loss: 0.6314 - acc: 0.86 - ETA: 10s - loss: 0.6357 - acc: 0.86 - ETA: 10s - loss: 0.6393 - acc: 0.86 - ETA: 9s - loss: 0.6456 - acc: 0.8625 - ETA: 9s - loss: 0.6455 - acc: 0.863 - ETA: 9s - loss: 0.6487 - acc: 0.863 - ETA: 9s - loss: 0.6511 - acc: 0.862 - ETA: 9s - loss: 0.6487 - acc: 0.863 - ETA: 8s - loss: 0.6452 - acc: 0.863 - ETA: 8s - loss: 0.6475 - acc: 0.861 - ETA: 8s - loss: 0.6435 - acc: 0.863 - ETA: 8s - loss: 0.6439 - acc: 0.863 - ETA: 8s - loss: 0.6423 - acc: 0.864 - ETA: 7s - loss: 0.6429 - acc: 0.864 - ETA: 7s - loss: 0.6463 - acc: 0.863 - ETA: 7s - loss: 0.6472 - acc: 0.863 - ETA: 7s - loss: 0.6485 - acc: 0.862 - ETA: 7s - loss: 0.6470 - acc: 0.862 - ETA: 7s - loss: 0.6467 - acc: 0.862 - ETA: 6s - loss: 0.6510 - acc: 0.861 - ETA: 6s - loss: 0.6526 - acc: 0.861 - ETA: 6s - loss: 0.6550 - acc: 0.860 - ETA: 6s - loss: 0.6571 - acc: 0.860 - ETA: 6s - loss: 0.6582 - acc: 0.860 - ETA: 6s - loss: 0.6572 - acc: 0.860 - ETA: 5s - loss: 0.6548 - acc: 0.861 - ETA: 5s - loss: 0.6554 - acc: 0.861 - ETA: 5s - loss: 0.6559 - acc: 0.861 - ETA: 5s - loss: 0.6552 - acc: 0.861 - ETA: 5s - loss: 0.6560 - acc: 0.860 - ETA: 5s - loss: 0.6577 - acc: 0.860 - ETA: 4s - loss: 0.6567 - acc: 0.860 - ETA: 4s - loss: 0.6570 - acc: 0.860 - ETA: 4s - loss: 0.6569 - acc: 0.860 - ETA: 4s - loss: 0.6586 - acc: 0.860 - ETA: 4s - loss: 0.6586 - acc: 0.859 - ETA: 4s - loss: 0.6594 - acc: 0.859 - ETA: 4s - loss: 0.6595 - acc: 0.859 - ETA: 3s - loss: 0.6602 - acc: 0.859 - ETA: 3s - loss: 0.6602 - acc: 0.859 - ETA: 3s - loss: 0.6602 - acc: 0.859 - ETA: 3s - loss: 0.6608 - acc: 0.859 - ETA: 3s - loss: 0.6626 - acc: 0.858 - ETA: 3s - loss: 0.6630 - acc: 0.858 - ETA: 3s - loss: 0.6629 - acc: 0.858 - ETA: 2s - loss: 0.6623 - acc: 0.858 - ETA: 2s - loss: 0.6627 - acc: 0.857 - ETA: 2s - loss: 0.6630 - acc: 0.857 - ETA: 2s - loss: 0.6635 - acc: 0.857 - ETA: 2s - loss: 0.6638 - acc: 0.857 - ETA: 2s - loss: 0.6643 - acc: 0.857 - ETA: 2s - loss: 0.6642 - acc: 0.857 - ETA: 1s - loss: 0.6650 - acc: 0.857 - ETA: 1s - loss: 0.6655 - acc: 0.856 - ETA: 1s - loss: 0.6667 - acc: 0.856 - ETA: 1s - loss: 0.6672 - acc: 0.856 - ETA: 1s - loss: 0.6677 - acc: 0.855 - ETA: 1s - loss: 0.6690 - acc: 0.855 - ETA: 0s - loss: 0.6694 - acc: 0.855 - ETA: 0s - loss: 0.6704 - acc: 0.854 - ETA: 0s - loss: 0.6718 - acc: 0.854 - ETA: 0s - loss: 0.6720 - acc: 0.854 - ETA: 0s - loss: 0.6723 - acc: 0.853 - ETA: 0s - loss: 0.6728 - acc: 0.853 - ETA: 0s - loss: 0.6727 - acc: 0.853 - 12s 660us/step - loss: 0.6726 - acc: 0.8537 - val_loss: 2.5898 - val_acc: 0.6202\n",
      "Epoch 37/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18357/18357 [==============================] - ETA: 9s - loss: 0.6420 - acc: 0.868 - ETA: 9s - loss: 0.6095 - acc: 0.871 - ETA: 9s - loss: 0.6047 - acc: 0.872 - ETA: 9s - loss: 0.6004 - acc: 0.874 - ETA: 9s - loss: 0.5951 - acc: 0.878 - ETA: 9s - loss: 0.5980 - acc: 0.877 - ETA: 9s - loss: 0.6014 - acc: 0.875 - ETA: 8s - loss: 0.5913 - acc: 0.877 - ETA: 8s - loss: 0.5910 - acc: 0.878 - ETA: 8s - loss: 0.5937 - acc: 0.877 - ETA: 8s - loss: 0.5943 - acc: 0.876 - ETA: 8s - loss: 0.5933 - acc: 0.876 - ETA: 8s - loss: 0.5939 - acc: 0.875 - ETA: 8s - loss: 0.5952 - acc: 0.874 - ETA: 8s - loss: 0.5986 - acc: 0.873 - ETA: 8s - loss: 0.5968 - acc: 0.873 - ETA: 8s - loss: 0.5958 - acc: 0.873 - ETA: 8s - loss: 0.6016 - acc: 0.872 - ETA: 8s - loss: 0.6019 - acc: 0.871 - ETA: 7s - loss: 0.6041 - acc: 0.871 - ETA: 7s - loss: 0.6063 - acc: 0.870 - ETA: 7s - loss: 0.6088 - acc: 0.869 - ETA: 7s - loss: 0.6085 - acc: 0.870 - ETA: 7s - loss: 0.6088 - acc: 0.870 - ETA: 7s - loss: 0.6104 - acc: 0.870 - ETA: 7s - loss: 0.6106 - acc: 0.869 - ETA: 6s - loss: 0.6099 - acc: 0.869 - ETA: 6s - loss: 0.6113 - acc: 0.869 - ETA: 6s - loss: 0.6120 - acc: 0.869 - ETA: 6s - loss: 0.6094 - acc: 0.870 - ETA: 6s - loss: 0.6083 - acc: 0.870 - ETA: 6s - loss: 0.6095 - acc: 0.870 - ETA: 5s - loss: 0.6109 - acc: 0.869 - ETA: 5s - loss: 0.6113 - acc: 0.869 - ETA: 5s - loss: 0.6116 - acc: 0.868 - ETA: 5s - loss: 0.6117 - acc: 0.868 - ETA: 5s - loss: 0.6150 - acc: 0.867 - ETA: 5s - loss: 0.6152 - acc: 0.866 - ETA: 4s - loss: 0.6159 - acc: 0.866 - ETA: 4s - loss: 0.6158 - acc: 0.866 - ETA: 4s - loss: 0.6150 - acc: 0.866 - ETA: 4s - loss: 0.6147 - acc: 0.867 - ETA: 4s - loss: 0.6143 - acc: 0.867 - ETA: 4s - loss: 0.6146 - acc: 0.866 - ETA: 4s - loss: 0.6150 - acc: 0.866 - ETA: 3s - loss: 0.6147 - acc: 0.866 - ETA: 3s - loss: 0.6159 - acc: 0.865 - ETA: 3s - loss: 0.6158 - acc: 0.866 - ETA: 3s - loss: 0.6166 - acc: 0.865 - ETA: 3s - loss: 0.6165 - acc: 0.865 - ETA: 3s - loss: 0.6160 - acc: 0.866 - ETA: 2s - loss: 0.6170 - acc: 0.866 - ETA: 2s - loss: 0.6187 - acc: 0.865 - ETA: 2s - loss: 0.6190 - acc: 0.865 - ETA: 2s - loss: 0.6199 - acc: 0.865 - ETA: 2s - loss: 0.6206 - acc: 0.865 - ETA: 2s - loss: 0.6212 - acc: 0.864 - ETA: 2s - loss: 0.6215 - acc: 0.864 - ETA: 1s - loss: 0.6220 - acc: 0.864 - ETA: 1s - loss: 0.6232 - acc: 0.864 - ETA: 1s - loss: 0.6230 - acc: 0.864 - ETA: 1s - loss: 0.6235 - acc: 0.863 - ETA: 1s - loss: 0.6235 - acc: 0.863 - ETA: 1s - loss: 0.6240 - acc: 0.863 - ETA: 0s - loss: 0.6249 - acc: 0.863 - ETA: 0s - loss: 0.6255 - acc: 0.862 - ETA: 0s - loss: 0.6257 - acc: 0.862 - ETA: 0s - loss: 0.6262 - acc: 0.862 - ETA: 0s - loss: 0.6261 - acc: 0.862 - ETA: 0s - loss: 0.6271 - acc: 0.862 - ETA: 0s - loss: 0.6271 - acc: 0.861 - 12s 667us/step - loss: 0.6277 - acc: 0.8619 - val_loss: 2.5874 - val_acc: 0.6194\n",
      "Epoch 38/200\n",
      "18357/18357 [==============================] - ETA: 10s - loss: 0.6518 - acc: 0.85 - ETA: 9s - loss: 0.5946 - acc: 0.8711 - ETA: 9s - loss: 0.5967 - acc: 0.873 - ETA: 9s - loss: 0.5727 - acc: 0.880 - ETA: 9s - loss: 0.5603 - acc: 0.882 - ETA: 9s - loss: 0.5569 - acc: 0.883 - ETA: 9s - loss: 0.5636 - acc: 0.883 - ETA: 9s - loss: 0.5657 - acc: 0.882 - ETA: 9s - loss: 0.5595 - acc: 0.883 - ETA: 8s - loss: 0.5659 - acc: 0.882 - ETA: 8s - loss: 0.5671 - acc: 0.881 - ETA: 8s - loss: 0.5680 - acc: 0.880 - ETA: 8s - loss: 0.5649 - acc: 0.880 - ETA: 8s - loss: 0.5661 - acc: 0.879 - ETA: 8s - loss: 0.5638 - acc: 0.879 - ETA: 8s - loss: 0.5675 - acc: 0.878 - ETA: 8s - loss: 0.5675 - acc: 0.878 - ETA: 8s - loss: 0.5677 - acc: 0.878 - ETA: 8s - loss: 0.5687 - acc: 0.878 - ETA: 8s - loss: 0.5706 - acc: 0.877 - ETA: 8s - loss: 0.5711 - acc: 0.877 - ETA: 7s - loss: 0.5728 - acc: 0.877 - ETA: 7s - loss: 0.5733 - acc: 0.877 - ETA: 7s - loss: 0.5745 - acc: 0.876 - ETA: 7s - loss: 0.5758 - acc: 0.876 - ETA: 7s - loss: 0.5752 - acc: 0.876 - ETA: 7s - loss: 0.5745 - acc: 0.876 - ETA: 6s - loss: 0.5719 - acc: 0.876 - ETA: 6s - loss: 0.5729 - acc: 0.876 - ETA: 6s - loss: 0.5738 - acc: 0.875 - ETA: 6s - loss: 0.5724 - acc: 0.875 - ETA: 6s - loss: 0.5712 - acc: 0.876 - ETA: 5s - loss: 0.5710 - acc: 0.876 - ETA: 5s - loss: 0.5717 - acc: 0.875 - ETA: 5s - loss: 0.5721 - acc: 0.875 - ETA: 5s - loss: 0.5736 - acc: 0.874 - ETA: 5s - loss: 0.5745 - acc: 0.874 - ETA: 5s - loss: 0.5743 - acc: 0.874 - ETA: 5s - loss: 0.5747 - acc: 0.874 - ETA: 4s - loss: 0.5749 - acc: 0.874 - ETA: 4s - loss: 0.5759 - acc: 0.873 - ETA: 4s - loss: 0.5761 - acc: 0.873 - ETA: 4s - loss: 0.5780 - acc: 0.872 - ETA: 4s - loss: 0.5781 - acc: 0.872 - ETA: 4s - loss: 0.5791 - acc: 0.872 - ETA: 3s - loss: 0.5806 - acc: 0.871 - ETA: 3s - loss: 0.5809 - acc: 0.871 - ETA: 3s - loss: 0.5808 - acc: 0.871 - ETA: 3s - loss: 0.5814 - acc: 0.871 - ETA: 3s - loss: 0.5815 - acc: 0.871 - ETA: 3s - loss: 0.5825 - acc: 0.871 - ETA: 3s - loss: 0.5823 - acc: 0.871 - ETA: 2s - loss: 0.5843 - acc: 0.870 - ETA: 2s - loss: 0.5847 - acc: 0.870 - ETA: 2s - loss: 0.5842 - acc: 0.870 - ETA: 2s - loss: 0.5857 - acc: 0.870 - ETA: 2s - loss: 0.5861 - acc: 0.870 - ETA: 2s - loss: 0.5870 - acc: 0.869 - ETA: 1s - loss: 0.5869 - acc: 0.869 - ETA: 1s - loss: 0.5876 - acc: 0.869 - ETA: 1s - loss: 0.5885 - acc: 0.869 - ETA: 1s - loss: 0.5886 - acc: 0.869 - ETA: 1s - loss: 0.5881 - acc: 0.869 - ETA: 1s - loss: 0.5883 - acc: 0.869 - ETA: 1s - loss: 0.5892 - acc: 0.868 - ETA: 0s - loss: 0.5889 - acc: 0.868 - ETA: 0s - loss: 0.5899 - acc: 0.867 - ETA: 0s - loss: 0.5900 - acc: 0.867 - ETA: 0s - loss: 0.5897 - acc: 0.867 - ETA: 0s - loss: 0.5895 - acc: 0.867 - ETA: 0s - loss: 0.5893 - acc: 0.867 - 12s 677us/step - loss: 0.5894 - acc: 0.8674 - val_loss: 2.5859 - val_acc: 0.6195\n",
      "Epoch 39/200\n",
      "18357/18357 [==============================] - ETA: 12s - loss: 0.5206 - acc: 0.88 - ETA: 11s - loss: 0.5119 - acc: 0.88 - ETA: 10s - loss: 0.5138 - acc: 0.89 - ETA: 10s - loss: 0.5121 - acc: 0.88 - ETA: 10s - loss: 0.5075 - acc: 0.88 - ETA: 9s - loss: 0.5103 - acc: 0.8889 - ETA: 9s - loss: 0.5159 - acc: 0.889 - ETA: 9s - loss: 0.5226 - acc: 0.887 - ETA: 9s - loss: 0.5235 - acc: 0.887 - ETA: 9s - loss: 0.5241 - acc: 0.887 - ETA: 9s - loss: 0.5236 - acc: 0.888 - ETA: 8s - loss: 0.5215 - acc: 0.888 - ETA: 8s - loss: 0.5242 - acc: 0.887 - ETA: 8s - loss: 0.5228 - acc: 0.887 - ETA: 8s - loss: 0.5240 - acc: 0.887 - ETA: 8s - loss: 0.5247 - acc: 0.886 - ETA: 8s - loss: 0.5273 - acc: 0.886 - ETA: 7s - loss: 0.5279 - acc: 0.886 - ETA: 7s - loss: 0.5310 - acc: 0.885 - ETA: 7s - loss: 0.5324 - acc: 0.884 - ETA: 7s - loss: 0.5350 - acc: 0.883 - ETA: 7s - loss: 0.5364 - acc: 0.881 - ETA: 7s - loss: 0.5386 - acc: 0.881 - ETA: 6s - loss: 0.5376 - acc: 0.881 - ETA: 6s - loss: 0.5363 - acc: 0.881 - ETA: 6s - loss: 0.5353 - acc: 0.882 - ETA: 6s - loss: 0.5345 - acc: 0.882 - ETA: 6s - loss: 0.5355 - acc: 0.881 - ETA: 6s - loss: 0.5362 - acc: 0.881 - ETA: 6s - loss: 0.5385 - acc: 0.880 - ETA: 5s - loss: 0.5392 - acc: 0.880 - ETA: 5s - loss: 0.5380 - acc: 0.880 - ETA: 5s - loss: 0.5371 - acc: 0.880 - ETA: 5s - loss: 0.5370 - acc: 0.880 - ETA: 5s - loss: 0.5370 - acc: 0.880 - ETA: 5s - loss: 0.5376 - acc: 0.880 - ETA: 5s - loss: 0.5381 - acc: 0.880 - ETA: 4s - loss: 0.5388 - acc: 0.880 - ETA: 4s - loss: 0.5395 - acc: 0.879 - ETA: 4s - loss: 0.5392 - acc: 0.879 - ETA: 4s - loss: 0.5405 - acc: 0.879 - ETA: 4s - loss: 0.5415 - acc: 0.878 - ETA: 4s - loss: 0.5415 - acc: 0.878 - ETA: 4s - loss: 0.5415 - acc: 0.878 - ETA: 3s - loss: 0.5434 - acc: 0.877 - ETA: 3s - loss: 0.5448 - acc: 0.877 - ETA: 3s - loss: 0.5445 - acc: 0.877 - ETA: 3s - loss: 0.5450 - acc: 0.877 - ETA: 3s - loss: 0.5455 - acc: 0.877 - ETA: 3s - loss: 0.5454 - acc: 0.877 - ETA: 3s - loss: 0.5457 - acc: 0.877 - ETA: 2s - loss: 0.5468 - acc: 0.876 - ETA: 2s - loss: 0.5475 - acc: 0.876 - ETA: 2s - loss: 0.5489 - acc: 0.876 - ETA: 2s - loss: 0.5497 - acc: 0.875 - ETA: 2s - loss: 0.5494 - acc: 0.875 - ETA: 2s - loss: 0.5503 - acc: 0.875 - ETA: 1s - loss: 0.5504 - acc: 0.875 - ETA: 1s - loss: 0.5516 - acc: 0.874 - ETA: 1s - loss: 0.5510 - acc: 0.875 - ETA: 1s - loss: 0.5513 - acc: 0.874 - ETA: 1s - loss: 0.5515 - acc: 0.875 - ETA: 1s - loss: 0.5518 - acc: 0.875 - ETA: 1s - loss: 0.5524 - acc: 0.874 - ETA: 0s - loss: 0.5528 - acc: 0.874 - ETA: 0s - loss: 0.5535 - acc: 0.874 - ETA: 0s - loss: 0.5523 - acc: 0.875 - ETA: 0s - loss: 0.5521 - acc: 0.874 - ETA: 0s - loss: 0.5532 - acc: 0.874 - ETA: 0s - loss: 0.5539 - acc: 0.874 - ETA: 0s - loss: 0.5541 - acc: 0.874 - 111s 6ms/step - loss: 0.5546 - acc: 0.8739 - val_loss: 2.5853 - val_acc: 0.6203\n",
      "Epoch 40/200\n",
      "18357/18357 [==============================] - ETA: 13s - loss: 0.4884 - acc: 0.90 - ETA: 13s - loss: 0.4847 - acc: 0.90 - ETA: 13s - loss: 0.4868 - acc: 0.89 - ETA: 13s - loss: 0.4902 - acc: 0.89 - ETA: 13s - loss: 0.4866 - acc: 0.89 - ETA: 13s - loss: 0.4896 - acc: 0.89 - ETA: 13s - loss: 0.4852 - acc: 0.89 - ETA: 13s - loss: 0.4834 - acc: 0.89 - ETA: 12s - loss: 0.4837 - acc: 0.89 - ETA: 12s - loss: 0.4809 - acc: 0.89 - ETA: 11s - loss: 0.4859 - acc: 0.89 - ETA: 11s - loss: 0.4886 - acc: 0.89 - ETA: 10s - loss: 0.4875 - acc: 0.89 - ETA: 10s - loss: 0.4851 - acc: 0.89 - ETA: 10s - loss: 0.4887 - acc: 0.89 - ETA: 9s - loss: 0.4893 - acc: 0.8927 - ETA: 9s - loss: 0.4894 - acc: 0.892 - ETA: 9s - loss: 0.4922 - acc: 0.892 - ETA: 9s - loss: 0.4934 - acc: 0.892 - ETA: 8s - loss: 0.4926 - acc: 0.891 - ETA: 8s - loss: 0.4956 - acc: 0.891 - ETA: 8s - loss: 0.4973 - acc: 0.890 - ETA: 8s - loss: 0.4970 - acc: 0.890 - ETA: 7s - loss: 0.4964 - acc: 0.890 - ETA: 7s - loss: 0.4957 - acc: 0.890 - ETA: 7s - loss: 0.4973 - acc: 0.889 - ETA: 7s - loss: 0.4978 - acc: 0.889 - ETA: 7s - loss: 0.4990 - acc: 0.889 - ETA: 6s - loss: 0.4987 - acc: 0.889 - ETA: 6s - loss: 0.5013 - acc: 0.888 - ETA: 6s - loss: 0.5023 - acc: 0.888 - ETA: 6s - loss: 0.5019 - acc: 0.888 - ETA: 6s - loss: 0.5024 - acc: 0.888 - ETA: 5s - loss: 0.5021 - acc: 0.887 - ETA: 5s - loss: 0.5021 - acc: 0.888 - ETA: 5s - loss: 0.5059 - acc: 0.886 - ETA: 5s - loss: 0.5055 - acc: 0.886 - ETA: 5s - loss: 0.5068 - acc: 0.886 - ETA: 5s - loss: 0.5076 - acc: 0.886 - ETA: 5s - loss: 0.5082 - acc: 0.885 - ETA: 4s - loss: 0.5081 - acc: 0.885 - ETA: 4s - loss: 0.5094 - acc: 0.885 - ETA: 4s - loss: 0.5099 - acc: 0.884 - ETA: 4s - loss: 0.5102 - acc: 0.884 - ETA: 4s - loss: 0.5111 - acc: 0.884 - ETA: 4s - loss: 0.5120 - acc: 0.884 - ETA: 3s - loss: 0.5122 - acc: 0.884 - ETA: 3s - loss: 0.5112 - acc: 0.883 - ETA: 3s - loss: 0.5125 - acc: 0.883 - ETA: 3s - loss: 0.5130 - acc: 0.883 - ETA: 3s - loss: 0.5135 - acc: 0.883 - ETA: 3s - loss: 0.5138 - acc: 0.882 - ETA: 2s - loss: 0.5143 - acc: 0.882 - ETA: 2s - loss: 0.5147 - acc: 0.882 - ETA: 2s - loss: 0.5141 - acc: 0.882 - ETA: 2s - loss: 0.5140 - acc: 0.882 - ETA: 2s - loss: 0.5146 - acc: 0.881 - ETA: 2s - loss: 0.5157 - acc: 0.881 - ETA: 1s - loss: 0.5164 - acc: 0.880 - ETA: 1s - loss: 0.5165 - acc: 0.880 - ETA: 1s - loss: 0.5168 - acc: 0.880 - ETA: 1s - loss: 0.5174 - acc: 0.880 - ETA: 1s - loss: 0.5182 - acc: 0.880 - ETA: 1s - loss: 0.5196 - acc: 0.880 - ETA: 1s - loss: 0.5206 - acc: 0.880 - ETA: 0s - loss: 0.5208 - acc: 0.879 - ETA: 0s - loss: 0.5210 - acc: 0.879 - ETA: 0s - loss: 0.5213 - acc: 0.879 - ETA: 0s - loss: 0.5215 - acc: 0.879 - ETA: 0s - loss: 0.5225 - acc: 0.879 - ETA: 0s - loss: 0.5228 - acc: 0.878 - 13s 713us/step - loss: 0.5222 - acc: 0.8790 - val_loss: 2.5883 - val_acc: 0.6197\n",
      "Epoch 41/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18357/18357 [==============================] - ETA: 11s - loss: 0.4660 - acc: 0.89 - ETA: 11s - loss: 0.4637 - acc: 0.90 - ETA: 11s - loss: 0.4511 - acc: 0.90 - ETA: 11s - loss: 0.4485 - acc: 0.90 - ETA: 10s - loss: 0.4487 - acc: 0.90 - ETA: 10s - loss: 0.4551 - acc: 0.90 - ETA: 10s - loss: 0.4543 - acc: 0.90 - ETA: 10s - loss: 0.4553 - acc: 0.89 - ETA: 10s - loss: 0.4570 - acc: 0.89 - ETA: 10s - loss: 0.4530 - acc: 0.90 - ETA: 10s - loss: 0.4544 - acc: 0.90 - ETA: 10s - loss: 0.4534 - acc: 0.90 - ETA: 9s - loss: 0.4565 - acc: 0.9000 - ETA: 9s - loss: 0.4587 - acc: 0.899 - ETA: 9s - loss: 0.4597 - acc: 0.899 - ETA: 9s - loss: 0.4598 - acc: 0.898 - ETA: 9s - loss: 0.4622 - acc: 0.897 - ETA: 8s - loss: 0.4610 - acc: 0.898 - ETA: 8s - loss: 0.4604 - acc: 0.898 - ETA: 8s - loss: 0.4634 - acc: 0.896 - ETA: 8s - loss: 0.4649 - acc: 0.896 - ETA: 8s - loss: 0.4655 - acc: 0.896 - ETA: 8s - loss: 0.4647 - acc: 0.896 - ETA: 8s - loss: 0.4659 - acc: 0.895 - ETA: 8s - loss: 0.4656 - acc: 0.895 - ETA: 7s - loss: 0.4679 - acc: 0.894 - ETA: 7s - loss: 0.4688 - acc: 0.894 - ETA: 7s - loss: 0.4699 - acc: 0.894 - ETA: 7s - loss: 0.4699 - acc: 0.893 - ETA: 7s - loss: 0.4689 - acc: 0.893 - ETA: 6s - loss: 0.4703 - acc: 0.892 - ETA: 6s - loss: 0.4697 - acc: 0.893 - ETA: 6s - loss: 0.4700 - acc: 0.892 - ETA: 6s - loss: 0.4700 - acc: 0.892 - ETA: 6s - loss: 0.4702 - acc: 0.892 - ETA: 6s - loss: 0.4712 - acc: 0.892 - ETA: 5s - loss: 0.4712 - acc: 0.891 - ETA: 5s - loss: 0.4725 - acc: 0.891 - ETA: 5s - loss: 0.4731 - acc: 0.891 - ETA: 5s - loss: 0.4728 - acc: 0.891 - ETA: 5s - loss: 0.4727 - acc: 0.891 - ETA: 5s - loss: 0.4730 - acc: 0.891 - ETA: 4s - loss: 0.4746 - acc: 0.890 - ETA: 4s - loss: 0.4751 - acc: 0.890 - ETA: 4s - loss: 0.4780 - acc: 0.889 - ETA: 4s - loss: 0.4788 - acc: 0.889 - ETA: 4s - loss: 0.4790 - acc: 0.889 - ETA: 3s - loss: 0.4798 - acc: 0.888 - ETA: 3s - loss: 0.4805 - acc: 0.888 - ETA: 3s - loss: 0.4814 - acc: 0.888 - ETA: 3s - loss: 0.4832 - acc: 0.887 - ETA: 3s - loss: 0.4842 - acc: 0.887 - ETA: 3s - loss: 0.4849 - acc: 0.886 - ETA: 2s - loss: 0.4854 - acc: 0.886 - ETA: 2s - loss: 0.4858 - acc: 0.886 - ETA: 2s - loss: 0.4864 - acc: 0.886 - ETA: 2s - loss: 0.4873 - acc: 0.885 - ETA: 2s - loss: 0.4873 - acc: 0.886 - ETA: 2s - loss: 0.4871 - acc: 0.886 - ETA: 1s - loss: 0.4882 - acc: 0.885 - ETA: 1s - loss: 0.4884 - acc: 0.885 - ETA: 1s - loss: 0.4884 - acc: 0.885 - ETA: 1s - loss: 0.4880 - acc: 0.885 - ETA: 1s - loss: 0.4880 - acc: 0.885 - ETA: 1s - loss: 0.4892 - acc: 0.885 - ETA: 0s - loss: 0.4893 - acc: 0.885 - ETA: 0s - loss: 0.4900 - acc: 0.884 - ETA: 0s - loss: 0.4897 - acc: 0.884 - ETA: 0s - loss: 0.4899 - acc: 0.884 - ETA: 0s - loss: 0.4905 - acc: 0.884 - ETA: 0s - loss: 0.4908 - acc: 0.884 - 13s 731us/step - loss: 0.4910 - acc: 0.8839 - val_loss: 2.5843 - val_acc: 0.6207\n",
      "Epoch 42/200\n",
      "18357/18357 [==============================] - ETA: 12s - loss: 0.4515 - acc: 0.88 - ETA: 12s - loss: 0.4525 - acc: 0.88 - ETA: 11s - loss: 0.4356 - acc: 0.89 - ETA: 11s - loss: 0.4285 - acc: 0.89 - ETA: 10s - loss: 0.4267 - acc: 0.90 - ETA: 10s - loss: 0.4311 - acc: 0.89 - ETA: 10s - loss: 0.4308 - acc: 0.89 - ETA: 10s - loss: 0.4338 - acc: 0.89 - ETA: 9s - loss: 0.4318 - acc: 0.8977 - ETA: 9s - loss: 0.4343 - acc: 0.897 - ETA: 9s - loss: 0.4350 - acc: 0.896 - ETA: 9s - loss: 0.4357 - acc: 0.897 - ETA: 8s - loss: 0.4385 - acc: 0.896 - ETA: 8s - loss: 0.4375 - acc: 0.897 - ETA: 8s - loss: 0.4370 - acc: 0.897 - ETA: 8s - loss: 0.4335 - acc: 0.899 - ETA: 8s - loss: 0.4341 - acc: 0.898 - ETA: 8s - loss: 0.4358 - acc: 0.898 - ETA: 8s - loss: 0.4369 - acc: 0.897 - ETA: 7s - loss: 0.4363 - acc: 0.898 - ETA: 7s - loss: 0.4387 - acc: 0.897 - ETA: 7s - loss: 0.4391 - acc: 0.897 - ETA: 7s - loss: 0.4398 - acc: 0.897 - ETA: 7s - loss: 0.4396 - acc: 0.897 - ETA: 7s - loss: 0.4390 - acc: 0.897 - ETA: 6s - loss: 0.4400 - acc: 0.897 - ETA: 6s - loss: 0.4422 - acc: 0.897 - ETA: 6s - loss: 0.4420 - acc: 0.897 - ETA: 6s - loss: 0.4443 - acc: 0.896 - ETA: 6s - loss: 0.4438 - acc: 0.896 - ETA: 6s - loss: 0.4449 - acc: 0.896 - ETA: 6s - loss: 0.4446 - acc: 0.896 - ETA: 5s - loss: 0.4465 - acc: 0.895 - ETA: 5s - loss: 0.4478 - acc: 0.895 - ETA: 5s - loss: 0.4471 - acc: 0.895 - ETA: 5s - loss: 0.4471 - acc: 0.895 - ETA: 5s - loss: 0.4494 - acc: 0.894 - ETA: 5s - loss: 0.4502 - acc: 0.894 - ETA: 4s - loss: 0.4491 - acc: 0.894 - ETA: 4s - loss: 0.4490 - acc: 0.894 - ETA: 4s - loss: 0.4498 - acc: 0.894 - ETA: 4s - loss: 0.4509 - acc: 0.893 - ETA: 4s - loss: 0.4509 - acc: 0.893 - ETA: 4s - loss: 0.4519 - acc: 0.892 - ETA: 4s - loss: 0.4526 - acc: 0.892 - ETA: 3s - loss: 0.4520 - acc: 0.892 - ETA: 3s - loss: 0.4532 - acc: 0.892 - ETA: 3s - loss: 0.4551 - acc: 0.891 - ETA: 3s - loss: 0.4560 - acc: 0.891 - ETA: 3s - loss: 0.4567 - acc: 0.891 - ETA: 3s - loss: 0.4567 - acc: 0.891 - ETA: 3s - loss: 0.4567 - acc: 0.891 - ETA: 2s - loss: 0.4573 - acc: 0.891 - ETA: 2s - loss: 0.4575 - acc: 0.891 - ETA: 2s - loss: 0.4577 - acc: 0.890 - ETA: 2s - loss: 0.4578 - acc: 0.890 - ETA: 2s - loss: 0.4581 - acc: 0.890 - ETA: 2s - loss: 0.4587 - acc: 0.890 - ETA: 1s - loss: 0.4582 - acc: 0.890 - ETA: 1s - loss: 0.4591 - acc: 0.890 - ETA: 1s - loss: 0.4595 - acc: 0.890 - ETA: 1s - loss: 0.4608 - acc: 0.889 - ETA: 1s - loss: 0.4606 - acc: 0.889 - ETA: 1s - loss: 0.4610 - acc: 0.889 - ETA: 1s - loss: 0.4616 - acc: 0.889 - ETA: 0s - loss: 0.4619 - acc: 0.889 - ETA: 0s - loss: 0.4623 - acc: 0.889 - ETA: 0s - loss: 0.4632 - acc: 0.888 - ETA: 0s - loss: 0.4636 - acc: 0.888 - ETA: 0s - loss: 0.4640 - acc: 0.888 - ETA: 0s - loss: 0.4649 - acc: 0.888 - 13s 698us/step - loss: 0.4652 - acc: 0.8884 - val_loss: 2.5898 - val_acc: 0.6236\n",
      "Epoch 43/200\n",
      "18357/18357 [==============================] - ETA: 10s - loss: 0.4365 - acc: 0.90 - ETA: 11s - loss: 0.4382 - acc: 0.89 - ETA: 10s - loss: 0.4307 - acc: 0.89 - ETA: 11s - loss: 0.4208 - acc: 0.90 - ETA: 11s - loss: 0.4247 - acc: 0.90 - ETA: 11s - loss: 0.4233 - acc: 0.90 - ETA: 11s - loss: 0.4213 - acc: 0.90 - ETA: 11s - loss: 0.4178 - acc: 0.90 - ETA: 11s - loss: 0.4147 - acc: 0.90 - ETA: 10s - loss: 0.4148 - acc: 0.90 - ETA: 10s - loss: 0.4171 - acc: 0.90 - ETA: 10s - loss: 0.4134 - acc: 0.90 - ETA: 9s - loss: 0.4122 - acc: 0.9055 - ETA: 9s - loss: 0.4101 - acc: 0.905 - ETA: 9s - loss: 0.4095 - acc: 0.906 - ETA: 9s - loss: 0.4104 - acc: 0.906 - ETA: 8s - loss: 0.4101 - acc: 0.905 - ETA: 8s - loss: 0.4100 - acc: 0.906 - ETA: 8s - loss: 0.4093 - acc: 0.906 - ETA: 8s - loss: 0.4110 - acc: 0.906 - ETA: 8s - loss: 0.4110 - acc: 0.906 - ETA: 7s - loss: 0.4131 - acc: 0.906 - ETA: 7s - loss: 0.4143 - acc: 0.905 - ETA: 7s - loss: 0.4148 - acc: 0.905 - ETA: 7s - loss: 0.4158 - acc: 0.904 - ETA: 7s - loss: 0.4158 - acc: 0.904 - ETA: 7s - loss: 0.4161 - acc: 0.904 - ETA: 6s - loss: 0.4160 - acc: 0.904 - ETA: 6s - loss: 0.4170 - acc: 0.903 - ETA: 6s - loss: 0.4160 - acc: 0.903 - ETA: 6s - loss: 0.4156 - acc: 0.903 - ETA: 6s - loss: 0.4163 - acc: 0.902 - ETA: 6s - loss: 0.4170 - acc: 0.902 - ETA: 5s - loss: 0.4194 - acc: 0.902 - ETA: 5s - loss: 0.4200 - acc: 0.901 - ETA: 5s - loss: 0.4192 - acc: 0.901 - ETA: 5s - loss: 0.4205 - acc: 0.901 - ETA: 5s - loss: 0.4221 - acc: 0.900 - ETA: 5s - loss: 0.4227 - acc: 0.900 - ETA: 4s - loss: 0.4248 - acc: 0.899 - ETA: 4s - loss: 0.4260 - acc: 0.899 - ETA: 4s - loss: 0.4260 - acc: 0.899 - ETA: 4s - loss: 0.4265 - acc: 0.898 - ETA: 4s - loss: 0.4271 - acc: 0.898 - ETA: 4s - loss: 0.4292 - acc: 0.897 - ETA: 3s - loss: 0.4305 - acc: 0.897 - ETA: 3s - loss: 0.4309 - acc: 0.897 - ETA: 3s - loss: 0.4308 - acc: 0.897 - ETA: 3s - loss: 0.4303 - acc: 0.896 - ETA: 3s - loss: 0.4307 - acc: 0.896 - ETA: 3s - loss: 0.4315 - acc: 0.896 - ETA: 3s - loss: 0.4323 - acc: 0.896 - ETA: 2s - loss: 0.4326 - acc: 0.896 - ETA: 2s - loss: 0.4329 - acc: 0.895 - ETA: 2s - loss: 0.4331 - acc: 0.895 - ETA: 2s - loss: 0.4333 - acc: 0.895 - ETA: 2s - loss: 0.4327 - acc: 0.895 - ETA: 2s - loss: 0.4341 - acc: 0.895 - ETA: 1s - loss: 0.4337 - acc: 0.895 - ETA: 1s - loss: 0.4351 - acc: 0.895 - ETA: 1s - loss: 0.4358 - acc: 0.895 - ETA: 1s - loss: 0.4374 - acc: 0.894 - ETA: 1s - loss: 0.4372 - acc: 0.894 - ETA: 1s - loss: 0.4372 - acc: 0.894 - ETA: 1s - loss: 0.4380 - acc: 0.894 - ETA: 0s - loss: 0.4387 - acc: 0.893 - ETA: 0s - loss: 0.4385 - acc: 0.893 - ETA: 0s - loss: 0.4394 - acc: 0.893 - ETA: 0s - loss: 0.4396 - acc: 0.893 - ETA: 0s - loss: 0.4403 - acc: 0.893 - ETA: 0s - loss: 0.4404 - acc: 0.893 - 12s 681us/step - loss: 0.4402 - acc: 0.8930 - val_loss: 2.5921 - val_acc: 0.6228\n",
      "Epoch 44/200\n",
      "18357/18357 [==============================] - ETA: 9s - loss: 0.3381 - acc: 0.928 - ETA: 10s - loss: 0.3440 - acc: 0.92 - ETA: 9s - loss: 0.3613 - acc: 0.9190 - ETA: 9s - loss: 0.3755 - acc: 0.913 - ETA: 9s - loss: 0.3726 - acc: 0.914 - ETA: 9s - loss: 0.3786 - acc: 0.912 - ETA: 9s - loss: 0.3849 - acc: 0.910 - ETA: 9s - loss: 0.3893 - acc: 0.910 - ETA: 9s - loss: 0.3938 - acc: 0.908 - ETA: 9s - loss: 0.3920 - acc: 0.908 - ETA: 8s - loss: 0.3904 - acc: 0.909 - ETA: 8s - loss: 0.3887 - acc: 0.910 - ETA: 8s - loss: 0.3870 - acc: 0.909 - ETA: 8s - loss: 0.3921 - acc: 0.908 - ETA: 8s - loss: 0.3950 - acc: 0.907 - ETA: 8s - loss: 0.3958 - acc: 0.906 - ETA: 8s - loss: 0.3965 - acc: 0.906 - ETA: 8s - loss: 0.3976 - acc: 0.905 - ETA: 7s - loss: 0.3989 - acc: 0.905 - ETA: 7s - loss: 0.3990 - acc: 0.904 - ETA: 7s - loss: 0.3982 - acc: 0.904 - ETA: 7s - loss: 0.3976 - acc: 0.904 - ETA: 7s - loss: 0.3977 - acc: 0.904 - ETA: 7s - loss: 0.3983 - acc: 0.904 - ETA: 7s - loss: 0.3990 - acc: 0.904 - ETA: 6s - loss: 0.3998 - acc: 0.903 - ETA: 6s - loss: 0.4001 - acc: 0.904 - ETA: 6s - loss: 0.4007 - acc: 0.904 - ETA: 6s - loss: 0.4004 - acc: 0.904 - ETA: 6s - loss: 0.4012 - acc: 0.903 - ETA: 6s - loss: 0.4015 - acc: 0.903 - ETA: 6s - loss: 0.4014 - acc: 0.903 - ETA: 6s - loss: 0.4015 - acc: 0.903 - ETA: 5s - loss: 0.4024 - acc: 0.903 - ETA: 5s - loss: 0.4039 - acc: 0.902 - ETA: 5s - loss: 0.4036 - acc: 0.902 - ETA: 5s - loss: 0.4034 - acc: 0.902 - ETA: 5s - loss: 0.4032 - acc: 0.902 - ETA: 5s - loss: 0.4032 - acc: 0.902 - ETA: 4s - loss: 0.4039 - acc: 0.902 - ETA: 4s - loss: 0.4053 - acc: 0.901 - ETA: 4s - loss: 0.4052 - acc: 0.901 - ETA: 4s - loss: 0.4071 - acc: 0.900 - ETA: 4s - loss: 0.4071 - acc: 0.900 - ETA: 4s - loss: 0.4074 - acc: 0.900 - ETA: 4s - loss: 0.4068 - acc: 0.900 - ETA: 3s - loss: 0.4072 - acc: 0.900 - ETA: 3s - loss: 0.4083 - acc: 0.900 - ETA: 3s - loss: 0.4086 - acc: 0.900 - ETA: 3s - loss: 0.4086 - acc: 0.900 - ETA: 3s - loss: 0.4097 - acc: 0.899 - ETA: 3s - loss: 0.4098 - acc: 0.899 - ETA: 3s - loss: 0.4109 - acc: 0.899 - ETA: 2s - loss: 0.4117 - acc: 0.898 - ETA: 2s - loss: 0.4120 - acc: 0.898 - ETA: 2s - loss: 0.4129 - acc: 0.898 - ETA: 2s - loss: 0.4133 - acc: 0.898 - ETA: 2s - loss: 0.4129 - acc: 0.898 - ETA: 2s - loss: 0.4135 - acc: 0.897 - ETA: 1s - loss: 0.4134 - acc: 0.897 - ETA: 1s - loss: 0.4133 - acc: 0.897 - ETA: 1s - loss: 0.4146 - acc: 0.897 - ETA: 1s - loss: 0.4145 - acc: 0.897 - ETA: 1s - loss: 0.4146 - acc: 0.897 - ETA: 1s - loss: 0.4153 - acc: 0.896 - ETA: 0s - loss: 0.4154 - acc: 0.896 - ETA: 0s - loss: 0.4158 - acc: 0.896 - ETA: 0s - loss: 0.4164 - acc: 0.896 - ETA: 0s - loss: 0.4163 - acc: 0.896 - ETA: 0s - loss: 0.4161 - acc: 0.896 - ETA: 0s - loss: 0.4168 - acc: 0.896 - 13s 705us/step - loss: 0.4170 - acc: 0.8961 - val_loss: 2.6059 - val_acc: 0.6229\n",
      "Epoch 45/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18357/18357 [==============================] - ETA: 11s - loss: 0.3758 - acc: 0.90 - ETA: 11s - loss: 0.3611 - acc: 0.91 - ETA: 11s - loss: 0.3698 - acc: 0.91 - ETA: 11s - loss: 0.3635 - acc: 0.91 - ETA: 11s - loss: 0.3686 - acc: 0.91 - ETA: 10s - loss: 0.3602 - acc: 0.91 - ETA: 10s - loss: 0.3629 - acc: 0.91 - ETA: 10s - loss: 0.3605 - acc: 0.91 - ETA: 10s - loss: 0.3588 - acc: 0.91 - ETA: 9s - loss: 0.3586 - acc: 0.9182 - ETA: 9s - loss: 0.3601 - acc: 0.918 - ETA: 9s - loss: 0.3626 - acc: 0.916 - ETA: 9s - loss: 0.3655 - acc: 0.915 - ETA: 8s - loss: 0.3692 - acc: 0.914 - ETA: 8s - loss: 0.3678 - acc: 0.914 - ETA: 8s - loss: 0.3700 - acc: 0.914 - ETA: 8s - loss: 0.3690 - acc: 0.914 - ETA: 8s - loss: 0.3687 - acc: 0.914 - ETA: 8s - loss: 0.3682 - acc: 0.913 - ETA: 7s - loss: 0.3707 - acc: 0.912 - ETA: 7s - loss: 0.3692 - acc: 0.912 - ETA: 7s - loss: 0.3730 - acc: 0.911 - ETA: 7s - loss: 0.3716 - acc: 0.911 - ETA: 7s - loss: 0.3711 - acc: 0.911 - ETA: 7s - loss: 0.3719 - acc: 0.911 - ETA: 6s - loss: 0.3729 - acc: 0.910 - ETA: 6s - loss: 0.3719 - acc: 0.911 - ETA: 6s - loss: 0.3719 - acc: 0.910 - ETA: 6s - loss: 0.3715 - acc: 0.910 - ETA: 6s - loss: 0.3719 - acc: 0.910 - ETA: 6s - loss: 0.3724 - acc: 0.909 - ETA: 6s - loss: 0.3728 - acc: 0.909 - ETA: 5s - loss: 0.3756 - acc: 0.908 - ETA: 5s - loss: 0.3767 - acc: 0.908 - ETA: 5s - loss: 0.3769 - acc: 0.907 - ETA: 5s - loss: 0.3790 - acc: 0.906 - ETA: 5s - loss: 0.3798 - acc: 0.906 - ETA: 5s - loss: 0.3811 - acc: 0.906 - ETA: 4s - loss: 0.3809 - acc: 0.906 - ETA: 4s - loss: 0.3830 - acc: 0.905 - ETA: 4s - loss: 0.3840 - acc: 0.905 - ETA: 4s - loss: 0.3835 - acc: 0.905 - ETA: 4s - loss: 0.3834 - acc: 0.905 - ETA: 4s - loss: 0.3836 - acc: 0.905 - ETA: 4s - loss: 0.3847 - acc: 0.904 - ETA: 3s - loss: 0.3850 - acc: 0.904 - ETA: 3s - loss: 0.3852 - acc: 0.904 - ETA: 3s - loss: 0.3855 - acc: 0.904 - ETA: 3s - loss: 0.3863 - acc: 0.903 - ETA: 3s - loss: 0.3870 - acc: 0.903 - ETA: 3s - loss: 0.3879 - acc: 0.902 - ETA: 3s - loss: 0.3883 - acc: 0.902 - ETA: 2s - loss: 0.3891 - acc: 0.902 - ETA: 2s - loss: 0.3896 - acc: 0.902 - ETA: 2s - loss: 0.3910 - acc: 0.901 - ETA: 2s - loss: 0.3908 - acc: 0.901 - ETA: 2s - loss: 0.3914 - acc: 0.901 - ETA: 2s - loss: 0.3919 - acc: 0.901 - ETA: 2s - loss: 0.3919 - acc: 0.901 - ETA: 1s - loss: 0.3917 - acc: 0.901 - ETA: 1s - loss: 0.3918 - acc: 0.901 - ETA: 1s - loss: 0.3919 - acc: 0.901 - ETA: 1s - loss: 0.3931 - acc: 0.900 - ETA: 1s - loss: 0.3931 - acc: 0.900 - ETA: 1s - loss: 0.3936 - acc: 0.900 - ETA: 0s - loss: 0.3942 - acc: 0.900 - ETA: 0s - loss: 0.3948 - acc: 0.900 - ETA: 0s - loss: 0.3949 - acc: 0.899 - ETA: 0s - loss: 0.3951 - acc: 0.899 - ETA: 0s - loss: 0.3955 - acc: 0.899 - ETA: 0s - loss: 0.3960 - acc: 0.899 - 14s 747us/step - loss: 0.3966 - acc: 0.8994 - val_loss: 2.6016 - val_acc: 0.6230\n"
     ]
    }
   ],
   "source": [
    "# Computing inputs for `create_model`\n",
    "input_n = len(eng_word_to_ix)\n",
    "output_n = len(spa_word_to_ix)\n",
    "X_seq_len = len(X[0])\n",
    "Y_seq_len = Y_onehot.shape[1]\n",
    "\n",
    "# Create the model\n",
    "# YOUR CODE HERE\n",
    "model = create_model(input_n, X_seq_len, output_n, Y_seq_len, 256, 512)\n",
    "\n",
    "# Train the model (note that you should feed both the input sentence and a zero array with the correct shape to\n",
    "# the `fit` method).\n",
    "# YOUR CODE HERE\n",
    "from keras.callbacks import EarlyStopping\n",
    "estop = EarlyStopping(monitor='val_acc', patience=3)\n",
    "model.fit(x=[X, np.zeros(X.shape[0])], y=Y_onehot, epochs=200, batch_size=256, validation_split=0.2, callbacks=[estop])\n",
    "\n",
    "model.save_weights('seq2seq_model_correct.hdf5')\n",
    "\n",
    "# If you need, you can use the following line for loading a saved model weights\n",
    "#model.load_weights('seq2seq_model.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "60f558a2b0a63756c5ac087989c5350e",
     "grade": false,
     "grade_id": "cell-4411359602a0cb69",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### 2.2.3 Evaluation <a class=\"anchor\" id=\"2.2.3\"></a>\n",
    "You are free to evaluate and compare results of your model(s) in any way you have learned from the deep learning course.  \n",
    "\n",
    "Now motivate your choice of architecture and hyperparameters by answering the questions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2c0c7acfbf28dcfb876fb8259a44e89b",
     "grade": false,
     "grade_id": "cell-5440fa7a5a785784",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Question:** What loss function, metrics and optimizer did you use and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "423b493471e8b0d54142e1bab890c791",
     "grade": true,
     "grade_id": "cell-369f4073de21366f",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** The problem of machine translation is similar to the problem of multi-class classification, where the number of words in the vocabulary is the number of categories that we have. So at each timestep, we want to predict the same class as the ground truth. For this reason, we want to use categorical cross entropy as the loss function that we want to optimize. We set the hidden_dim to 256, the embedding_dim to 512, and the batch_size to 256. After many times of re-running the network with different hyper-parameters' values, we found this setup to be quite good. It's intuitive to set the hidden_dim to be as high as 256, because this will give us more parameters to optimize, thus will potentially lead to a more complex model that is able to capture more non-linearity. As for the optimizer, we use Adam because it seems to be the most stable and fast optimizer that we have learned so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bff435770aa8062226c9b3895a1f6a96",
     "grade": false,
     "grade_id": "cell-b3a5072f283d1dbd",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Question:** Did you use any Keras callbacks? If so, how did they help you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "de7542f0702ad21a0adf46d99618ca84",
     "grade": true,
     "grade_id": "cell-707212a45931de41",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** We only used Early Stopping here. This stops the training when the validation accuracy drops, thus preventing our final model fromb overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "210021597f4c858f6a2509a7397351f4",
     "grade": false,
     "grade_id": "cell-a563bc3066bdc5ad",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Question:** How did you evaluate that the model was good enough?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c7db7d78e789f3703545dc6ab75f9728",
     "grade": true,
     "grade_id": "cell-2de00c740a7a8a5f",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** We use 20% of the entire dataset for validation. Since the learning algorithm only looks at the train dataset, the validation can act as \"unseen\" data that can be used to validate the model returned by the learning algorithm. We then run the algorithm through many epochs until the validation accuracy is around 62%, which for us, seems to be good enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6d1c1c8df33ecb29a306280ddc562384",
     "grade": false,
     "grade_id": "cell-c70fa415812abf8b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### 2.2.4 Testing <a class=\"anchor\" id=\"2.2.4\"></a>\n",
    "Test your neural machine translator on a few sentences by running the test case below. The similarity metric is calculated comparing word embeddings.  \n",
    "\n",
    "You can also use the function `translate` to try your own sentences, just remember that every word needs to be in the vocabulary!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "552a75386c2d6110b5331dada6f9c689",
     "grade": true,
     "grade_id": "cell-e96abd2e0fbbae6d",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    \"\"\" Translate a sentence using `model`. `model` is assumed to be a global variable.\n",
    "    \n",
    "    Arguments:\n",
    "    sentence - a string to translate\n",
    "    \n",
    "    Returns:\n",
    "    the translated sentence as a string\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check that each of the words in the input sentence exist in the input dictionary\n",
    "    try:\n",
    "        x = [eng_word_to_ix[word] for word in sentence.split(' ')]\n",
    "    except KeyError as e: \n",
    "        print('{0} doesn\\'t exist in the vocabulary!'.format(e))\n",
    "        \n",
    "    # Pad the input sentence\n",
    "    x = pad_sequences([x], maxlen=SEQ_MAX_LEN, dtype='int32')\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    ans = model.predict([x, np.zeros(x.shape[0])])\n",
    "    ans = ans.reshape(ans.shape[1], ans.shape[2])\n",
    "    ans = np.argmax(ans, axis=1)\n",
    "    y_pred_words = ' '.join(spa_ix_to_word[ind] for ind in ans if ind != 0)\n",
    "    \n",
    "    return y_pred_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "96ed353059f01b613aae6682d7b936c8",
     "grade": false,
     "grade_id": "cell-6301fda0cbde1746",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \"she is a singer .\" \n",
      "Prediction: \"ella es un viejo .\" \n",
      "Actual \"ella es cantante .\"\n",
      "Similarity: 0.5824283019494632\n",
      "-----------------\n",
      "Input: \"my opinion is irrelevant .\" \n",
      "Prediction: \"mi opinion es irrelevante .\" \n",
      "Actual \"mi opinion es irrelevante .\"\n",
      "Similarity: 1.0\n",
      "-----------------\n",
      "Input: \"look out for pickpockets .\" \n",
      "Prediction: \"ojo con los lanzas .\" \n",
      "Actual \"ojo con los lanzas .\"\n",
      "Similarity: 1.0\n",
      "-----------------\n",
      "Input: \"this might just work .\" \n",
      "Prediction: \"esto simplemente podria funcionar .\" \n",
      "Actual \"esto simplemente podria funcionar .\"\n",
      "Similarity: 1.0\n",
      "-----------------\n",
      "Input: \"these books are mine .\" \n",
      "Prediction: \"estos libros son mios .\" \n",
      "Actual \"estos libros son mios .\"\n",
      "Similarity: 1.0\n",
      "-----------------\n",
      "Input: \"the police are there .\" \n",
      "Prediction: \"la policia esta alli .\" \n",
      "Actual \"la policia esta alli.\"\n",
      "Similarity: 1.0\n",
      "-----------------\n",
      "Input: \"the car is very fast .\" \n",
      "Prediction: \"muy muy rapido .\" \n",
      "Actual \"el auto es muy rapido .\"\n",
      "Similarity: 0.7767206986522339\n",
      "-----------------\n",
      "Input: \"thats a stupid idea .\" \n",
      "Prediction: \"es una idea idea .\" \n",
      "Actual \"esa es una idea estupida .\"\n",
      "Similarity: 0.8535344215857666\n",
      "-----------------\n",
      "Input: \"she opens the window .\" \n",
      "Prediction: \"ella jalo la ventana .\" \n",
      "Actual \"ella abre la ventana .\"\n",
      "Similarity: 0.8047258487459226\n",
      "-----------------\n",
      "Input: \"nothing is happening .\" \n",
      "Prediction: \"no hay pasando nada .\" \n",
      "Actual \"no pasa nada .\"\n",
      "Similarity: 0.6406514975271986\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "# test case. be sure the variable model has a reference to your trained model\n",
    "test_predictions(translate, eng_word_to_ix, spa_ix_to_word, SEQ_MAX_LEN, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esto es un perro .\n"
     ]
    }
   ],
   "source": [
    "# test for yourself\n",
    "print(translate('this is a dog .'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ella gustan las manzanas .\n"
     ]
    }
   ],
   "source": [
    "print(translate('she love apples .'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el abrio la ventana .\n"
     ]
    }
   ],
   "source": [
    "print(translate('he opened the window .'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esto esta bien bien .\n"
     ]
    }
   ],
   "source": [
    "print(translate('this is working well .'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soy un buen bonita .\n"
     ]
    }
   ],
   "source": [
    "print(translate('i am a great student .'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a6ca7937953dd9f99a480ffe2a74e426",
     "grade": false,
     "grade_id": "cell-874b0d3a8d72aaa6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Congratulations!\n",
    "You have successfully implemented a recurrent neural network from scratch using only NumPy and also implemented a neural machine translator using Keras!\n",
    "\n",
    "When choosing the architecture for your neural machine translator you are partly restricted from the capabilities of Keras and partly restricted from your available computing power.  \n",
    "\n",
    "**Question:** Give 3 suggestions for different techniques that can be used to improve your neural machine translator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "0945de0b3c8d25a9bca06c96f6717ce9",
     "grade": true,
     "grade_id": "cell-1b51b4c3f80a3c02",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** Our 3 suggestions are: stacking LSTMs (with more layers), adding regularization (to reduce the variance, and thus overfitting), or using word2vec (instead of training our own Embedding layer).\n",
    "\n",
    "Stacking LSTMs means that we add more LSTMs layers on top of each other. This gives us the ability to capture non-linearity of machine translation.\n",
    "\n",
    "The reason why regularization might help is because our results seem a bit overfit. We could use something like L2 regularizations, to make sure that the values of the parameters are small.\n",
    "\n",
    "We can use word2vec as the embedding layer. This can improve the results because word2vec is a pre-trained model that has been trained on lots of data. Since we don't have a lot of data, training from scratch with an Embedding layer potentially gives worse results compared to word2vec."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
